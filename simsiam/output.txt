 
Running with excep_size=200 
Use GPU: 0 for training
=> creating model 'resnet50'
SimSiam(
  (encoder): ResNet(
    (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=512, bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=512, out_features=2048, bias=True)
      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (predictor): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=2048, bias=True)
  )
)
Epoch: [0][0/2]	Time  1.462 ( 1.462)	Data  0.086 ( 0.086)	Loss 0.0021 (0.0021)
Epoch: [1][0/2]	Time  0.121 ( 0.121)	Data  0.061 ( 0.061)	Loss -0.0839 (-0.0839)
Epoch: [2][0/2]	Time  0.120 ( 0.120)	Data  0.058 ( 0.058)	Loss -0.1707 (-0.1707)
Epoch: [3][0/2]	Time  0.121 ( 0.121)	Data  0.058 ( 0.058)	Loss -0.2477 (-0.2477)
Epoch: [4][0/2]	Time  0.120 ( 0.120)	Data  0.057 ( 0.057)	Loss -0.3597 (-0.3597)
Epoch: [5][0/2]	Time  0.118 ( 0.118)	Data  0.056 ( 0.056)	Loss -0.4416 (-0.4416)
Epoch: [6][0/2]	Time  0.117 ( 0.117)	Data  0.057 ( 0.057)	Loss -0.5251 (-0.5251)
Epoch: [7][0/2]	Time  0.122 ( 0.122)	Data  0.057 ( 0.057)	Loss -0.5938 (-0.5938)
Epoch: [8][0/2]	Time  0.123 ( 0.123)	Data  0.059 ( 0.059)	Loss -0.6478 (-0.6478)
Epoch: [9][0/2]	Time  0.119 ( 0.119)	Data  0.057 ( 0.057)	Loss -0.6851 (-0.6851)
Epoch: [10][0/2]	Time  0.127 ( 0.127)	Data  0.056 ( 0.056)	Loss -0.7198 (-0.7198)
Epoch: [11][0/2]	Time  0.119 ( 0.119)	Data  0.057 ( 0.057)	Loss -0.7377 (-0.7377)
Epoch: [12][0/2]	Time  0.128 ( 0.128)	Data  0.065 ( 0.065)	Loss -0.7515 (-0.7515)
Epoch: [13][0/2]	Time  0.121 ( 0.121)	Data  0.061 ( 0.061)	Loss -0.7617 (-0.7617)
Epoch: [14][0/2]	Time  0.137 ( 0.137)	Data  0.074 ( 0.074)	Loss -0.7333 (-0.7333)
Epoch: [15][0/2]	Time  0.126 ( 0.126)	Data  0.065 ( 0.065)	Loss -0.7739 (-0.7739)
Epoch: [16][0/2]	Time  0.123 ( 0.123)	Data  0.056 ( 0.056)	Loss -0.7710 (-0.7710)
Epoch: [17][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.7680 (-0.7680)
Epoch: [18][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.7609 (-0.7609)
Epoch: [19][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.7818 (-0.7818)
Epoch: [20][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.7959 (-0.7959)
Epoch: [21][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.7857 (-0.7857)
Epoch: [22][0/2]	Time  0.215 ( 0.215)	Data  0.062 ( 0.062)	Loss -0.8185 (-0.8185)
Epoch: [23][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.8225 (-0.8225)
Epoch: [24][0/2]	Time  0.197 ( 0.197)	Data  0.056 ( 0.056)	Loss -0.8271 (-0.8271)
Epoch: [25][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.8174 (-0.8174)
Epoch: [26][0/2]	Time  0.207 ( 0.207)	Data  0.060 ( 0.060)	Loss -0.8233 (-0.8233)
Epoch: [27][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.8216 (-0.8216)
Epoch: [28][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.8175 (-0.8175)
Epoch: [29][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.8163 (-0.8163)
Epoch: [30][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.8122 (-0.8122)
Epoch: [31][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.7973 (-0.7973)
Epoch: [32][0/2]	Time  0.208 ( 0.208)	Data  0.063 ( 0.063)	Loss -0.7739 (-0.7739)
Epoch: [33][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.7643 (-0.7643)
Epoch: [34][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.7447 (-0.7447)
Epoch: [35][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.7711 (-0.7711)
Epoch: [36][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.7665 (-0.7665)
Epoch: [37][0/2]	Time  0.197 ( 0.197)	Data  0.059 ( 0.059)	Loss -0.7842 (-0.7842)
Epoch: [38][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.7829 (-0.7829)
Epoch: [39][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.8034 (-0.8034)
Epoch: [40][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.8053 (-0.8053)
Epoch: [41][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.8334 (-0.8334)
Epoch: [42][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.8230 (-0.8230)
Epoch: [43][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.8304 (-0.8304)
Epoch: [44][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.8374 (-0.8374)
Epoch: [45][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8446 (-0.8446)
Epoch: [46][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.8448 (-0.8448)
Epoch: [47][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.8478 (-0.8478)
Epoch: [48][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8548 (-0.8548)
Epoch: [49][0/2]	Time  0.213 ( 0.213)	Data  0.059 ( 0.059)	Loss -0.8393 (-0.8393)
Epoch: [50][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.8395 (-0.8395)
Epoch: [51][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.8093 (-0.8093)
Epoch: [52][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.8056 (-0.8056)
Epoch: [53][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.7879 (-0.7879)
Epoch: [54][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.8019 (-0.8019)
Epoch: [55][0/2]	Time  0.216 ( 0.216)	Data  0.057 ( 0.057)	Loss -0.7799 (-0.7799)
Epoch: [56][0/2]	Time  0.195 ( 0.195)	Data  0.057 ( 0.057)	Loss -0.8072 (-0.8072)
Epoch: [57][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.8158 (-0.8158)
Epoch: [58][0/2]	Time  0.213 ( 0.213)	Data  0.062 ( 0.062)	Loss -0.8277 (-0.8277)
Epoch: [59][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.8380 (-0.8380)
Epoch: [60][0/2]	Time  0.214 ( 0.214)	Data  0.057 ( 0.057)	Loss -0.8362 (-0.8362)
Epoch: [61][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.8460 (-0.8460)
Epoch: [62][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.8678 (-0.8678)
Epoch: [63][0/2]	Time  0.207 ( 0.207)	Data  0.056 ( 0.056)	Loss -0.8657 (-0.8657)
Epoch: [64][0/2]	Time  0.194 ( 0.194)	Data  0.056 ( 0.056)	Loss -0.8709 (-0.8709)
Epoch: [65][0/2]	Time  0.212 ( 0.212)	Data  0.060 ( 0.060)	Loss -0.8714 (-0.8714)
Epoch: [66][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.8767 (-0.8767)
Epoch: [67][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.8779 (-0.8779)
Epoch: [68][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.8806 (-0.8806)
Epoch: [69][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.8900 (-0.8900)
Epoch: [70][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.8879 (-0.8879)
Epoch: [71][0/2]	Time  0.209 ( 0.209)	Data  0.062 ( 0.062)	Loss -0.8872 (-0.8872)
Epoch: [72][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8907 (-0.8907)
Epoch: [73][0/2]	Time  0.216 ( 0.216)	Data  0.061 ( 0.061)	Loss -0.8895 (-0.8895)
Epoch: [74][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.8811 (-0.8811)
Epoch: [75][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.8869 (-0.8869)
Epoch: [76][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.8776 (-0.8776)
Epoch: [77][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8927 (-0.8927)
Epoch: [78][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9021 (-0.9021)
Epoch: [79][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9006 (-0.9006)
Epoch: [80][0/2]	Time  0.214 ( 0.214)	Data  0.057 ( 0.057)	Loss -0.8921 (-0.8921)
Epoch: [81][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9034 (-0.9034)
Epoch: [82][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.9014 (-0.9014)
Epoch: [83][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.8885 (-0.8885)
Epoch: [84][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9037 (-0.9037)
Epoch: [85][0/2]	Time  0.216 ( 0.216)	Data  0.060 ( 0.060)	Loss -0.8990 (-0.8990)
Epoch: [86][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9028 (-0.9028)
Epoch: [87][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9033 (-0.9033)
Epoch: [88][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9008 (-0.9008)
Epoch: [89][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.8988 (-0.8988)
Epoch: [90][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.9032 (-0.9032)
Epoch: [91][0/2]	Time  0.211 ( 0.211)	Data  0.056 ( 0.056)	Loss -0.8999 (-0.8999)
Epoch: [92][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9047 (-0.9047)
Epoch: [93][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.8996 (-0.8996)
Epoch: [94][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9127 (-0.9127)
Epoch: [95][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9116 (-0.9116)
Epoch: [96][0/2]	Time  0.205 ( 0.205)	Data  0.057 ( 0.057)	Loss -0.9131 (-0.9131)
Epoch: [97][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9070 (-0.9070)
Epoch: [98][0/2]	Time  0.195 ( 0.195)	Data  0.057 ( 0.057)	Loss -0.9053 (-0.9053)
Epoch: [99][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9009 (-0.9009)
Epoch: [100][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9042 (-0.9042)
Epoch: [101][0/2]	Time  0.179 ( 0.179)	Data  0.057 ( 0.057)	Loss -0.9093 (-0.9093)
Epoch: [102][0/2]	Time  0.208 ( 0.208)	Data  0.056 ( 0.056)	Loss -0.9131 (-0.9131)
Epoch: [103][0/2]	Time  0.200 ( 0.200)	Data  0.061 ( 0.061)	Loss -0.9138 (-0.9138)
Epoch: [104][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9105 (-0.9105)
Epoch: [105][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9075 (-0.9075)
Epoch: [106][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9126 (-0.9126)
Epoch: [107][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9116 (-0.9116)
Epoch: [108][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9054 (-0.9054)
Epoch: [109][0/2]	Time  0.190 ( 0.190)	Data  0.066 ( 0.066)	Loss -0.9024 (-0.9024)
Epoch: [110][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9086 (-0.9086)
Epoch: [111][0/2]	Time  0.199 ( 0.199)	Data  0.057 ( 0.057)	Loss -0.9099 (-0.9099)
Epoch: [112][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9123 (-0.9123)
Epoch: [113][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9105 (-0.9105)
Epoch: [114][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9116 (-0.9116)
Epoch: [115][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9150 (-0.9150)
Epoch: [116][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9125 (-0.9125)
Epoch: [117][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9059 (-0.9059)
Epoch: [118][0/2]	Time  0.211 ( 0.211)	Data  0.059 ( 0.059)	Loss -0.9129 (-0.9129)
Epoch: [119][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8996 (-0.8996)
Epoch: [120][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9112 (-0.9112)
Epoch: [121][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9171 (-0.9171)
Epoch: [122][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9163 (-0.9163)
Epoch: [123][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9092 (-0.9092)
Epoch: [124][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9200 (-0.9200)
Epoch: [125][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9086 (-0.9086)
Epoch: [126][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9169 (-0.9169)
Epoch: [127][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9130 (-0.9130)
.\main_simsiam.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
Epoch: [128][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9123 (-0.9123)
Epoch: [129][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9109 (-0.9109)
Epoch: [130][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9112 (-0.9112)
Epoch: [131][0/2]	Time  0.193 ( 0.193)	Data  0.057 ( 0.057)	Loss -0.9079 (-0.9079)
Epoch: [132][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9197 (-0.9197)
Epoch: [133][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9132 (-0.9132)
Epoch: [134][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9136 (-0.9136)
Epoch: [135][0/2]	Time  0.205 ( 0.205)	Data  0.056 ( 0.056)	Loss -0.9151 (-0.9151)
Epoch: [136][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9170 (-0.9170)
Epoch: [137][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9148 (-0.9148)
Epoch: [138][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9177 (-0.9177)
Epoch: [139][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9245 (-0.9245)
Epoch: [140][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9073 (-0.9073)
Epoch: [141][0/2]	Time  0.211 ( 0.211)	Data  0.060 ( 0.060)	Loss -0.9169 (-0.9169)
Epoch: [142][0/2]	Time  0.180 ( 0.180)	Data  0.057 ( 0.057)	Loss -0.9095 (-0.9095)
Epoch: [143][0/2]	Time  0.214 ( 0.214)	Data  0.061 ( 0.061)	Loss -0.9217 (-0.9217)
Epoch: [144][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9113 (-0.9113)
Epoch: [145][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9197 (-0.9197)
Epoch: [146][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9205 (-0.9205)
Epoch: [147][0/2]	Time  0.205 ( 0.205)	Data  0.061 ( 0.061)	Loss -0.9169 (-0.9169)
Epoch: [148][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9072 (-0.9072)
Epoch: [149][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9158 (-0.9158)
Epoch: [150][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.9181 (-0.9181)
Epoch: [151][0/2]	Time  0.215 ( 0.215)	Data  0.063 ( 0.063)	Loss -0.9136 (-0.9136)
Epoch: [152][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9131 (-0.9131)
Epoch: [153][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.9263 (-0.9263)
Epoch: [154][0/2]	Time  0.205 ( 0.205)	Data  0.057 ( 0.057)	Loss -0.9115 (-0.9115)
Epoch: [155][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.9171 (-0.9171)
Epoch: [156][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9214 (-0.9214)
Epoch: [157][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9171 (-0.9171)
Epoch: [158][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9227 (-0.9227)
Epoch: [159][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9158 (-0.9158)
Epoch: [160][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9188 (-0.9188)
Epoch: [161][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9104 (-0.9104)
Epoch: [162][0/2]	Time  0.199 ( 0.199)	Data  0.056 ( 0.056)	Loss -0.9186 (-0.9186)
Epoch: [163][0/2]	Time  0.215 ( 0.215)	Data  0.059 ( 0.059)	Loss -0.9288 (-0.9288)
Epoch: [164][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9059 (-0.9059)
Epoch: [165][0/2]	Time  0.200 ( 0.200)	Data  0.062 ( 0.062)	Loss -0.9264 (-0.9264)
Epoch: [166][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9205 (-0.9205)
Epoch: [167][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.9158 (-0.9158)
Epoch: [168][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9156 (-0.9156)
Epoch: [169][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9138 (-0.9138)
Epoch: [170][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9254 (-0.9254)
Epoch: [171][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9149 (-0.9149)
Epoch: [172][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9154 (-0.9154)
Epoch: [173][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9189 (-0.9189)
Epoch: [174][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9184 (-0.9184)
Epoch: [175][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9227 (-0.9227)
Epoch: [176][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9211 (-0.9211)
Epoch: [177][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9217 (-0.9217)
Epoch: [178][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9177 (-0.9177)
Epoch: [179][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9129 (-0.9129)
Epoch: [180][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9248 (-0.9248)
Epoch: [181][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9140 (-0.9140)
Epoch: [182][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9170 (-0.9170)
Epoch: [183][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9155 (-0.9155)
Epoch: [184][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9241 (-0.9241)
Epoch: [185][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9145 (-0.9145)
Epoch: [186][0/2]	Time  0.198 ( 0.198)	Data  0.056 ( 0.056)	Loss -0.9176 (-0.9176)
Epoch: [187][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9140 (-0.9140)
Epoch: [188][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9194 (-0.9194)
Epoch: [189][0/2]	Time  0.199 ( 0.199)	Data  0.057 ( 0.057)	Loss -0.9238 (-0.9238)
Epoch: [190][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9186 (-0.9186)
Epoch: [191][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9167 (-0.9167)
Epoch: [192][0/2]	Time  0.199 ( 0.199)	Data  0.060 ( 0.060)	Loss -0.9228 (-0.9228)
Epoch: [193][0/2]	Time  0.214 ( 0.214)	Data  0.057 ( 0.057)	Loss -0.9148 (-0.9148)
Epoch: [194][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9176 (-0.9176)
Epoch: [195][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.9101 (-0.9101)
Epoch: [196][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9192 (-0.9192)
Epoch: [197][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9275 (-0.9275)
Epoch: [198][0/2]	Time  0.207 ( 0.207)	Data  0.061 ( 0.061)	Loss -0.9241 (-0.9241)
Epoch: [199][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9213 (-0.9213)
Epoch: [200][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9249 (-0.9249)
Epoch: [201][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9232 (-0.9232)
Epoch: [202][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9154 (-0.9154)
Epoch: [203][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9235 (-0.9235)
Epoch: [204][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9148 (-0.9148)
Epoch: [205][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9156 (-0.9156)
Epoch: [206][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9257 (-0.9257)
Epoch: [207][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9135 (-0.9135)
Epoch: [208][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9176 (-0.9176)
Epoch: [209][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9213 (-0.9213)
Epoch: [210][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9230 (-0.9230)
Epoch: [211][0/2]	Time  0.210 ( 0.210)	Data  0.060 ( 0.060)	Loss -0.9252 (-0.9252)
Epoch: [212][0/2]	Time  0.182 ( 0.182)	Data  0.059 ( 0.059)	Loss -0.9236 (-0.9236)
Epoch: [213][0/2]	Time  0.211 ( 0.211)	Data  0.059 ( 0.059)	Loss -0.9233 (-0.9233)
Epoch: [214][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9188 (-0.9188)
Epoch: [215][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.9265 (-0.9265)
Epoch: [216][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9223 (-0.9223)
Epoch: [217][0/2]	Time  0.190 ( 0.190)	Data  0.062 ( 0.062)	Loss -0.9250 (-0.9250)
Epoch: [218][0/2]	Time  0.215 ( 0.215)	Data  0.061 ( 0.061)	Loss -0.9225 (-0.9225)
Epoch: [219][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9223 (-0.9223)
Epoch: [220][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9227 (-0.9227)
Epoch: [221][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9234 (-0.9234)
Epoch: [222][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9187 (-0.9187)
Epoch: [223][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.9174 (-0.9174)
Epoch: [224][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9209 (-0.9209)
Epoch: [225][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9176 (-0.9176)
Epoch: [226][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9186 (-0.9186)
Epoch: [227][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9283 (-0.9283)
Epoch: [228][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9208 (-0.9208)
Epoch: [229][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9250 (-0.9250)
Epoch: [230][0/2]	Time  0.214 ( 0.214)	Data  0.061 ( 0.061)	Loss -0.9261 (-0.9261)
Epoch: [231][0/2]	Time  0.206 ( 0.206)	Data  0.061 ( 0.061)	Loss -0.9194 (-0.9194)
Epoch: [232][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9237 (-0.9237)
Epoch: [233][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9223 (-0.9223)
Epoch: [234][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9205 (-0.9205)
Epoch: [235][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9211 (-0.9211)
Epoch: [236][0/2]	Time  0.215 ( 0.215)	Data  0.059 ( 0.059)	Loss -0.9187 (-0.9187)
Epoch: [237][0/2]	Time  0.207 ( 0.207)	Data  0.060 ( 0.060)	Loss -0.9192 (-0.9192)
Epoch: [238][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9250 (-0.9250)
Epoch: [239][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9250 (-0.9250)
Epoch: [240][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9202 (-0.9202)
Epoch: [241][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9184 (-0.9184)
Epoch: [242][0/2]	Time  0.214 ( 0.214)	Data  0.057 ( 0.057)	Loss -0.9257 (-0.9257)
Epoch: [243][0/2]	Time  0.202 ( 0.202)	Data  0.060 ( 0.060)	Loss -0.9163 (-0.9163)
Epoch: [244][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9211 (-0.9211)
Epoch: [245][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9195 (-0.9195)
Epoch: [246][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9166 (-0.9166)
Epoch: [247][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9188 (-0.9188)
Epoch: [248][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.9191 (-0.9191)
Epoch: [249][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9245 (-0.9245)
Epoch: [250][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9241 (-0.9241)
Epoch: [251][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9212 (-0.9212)
Epoch: [252][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9304 (-0.9304)
Epoch: [253][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9226 (-0.9226)
Epoch: [254][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9227 (-0.9227)
Epoch: [255][0/2]	Time  0.199 ( 0.199)	Data  0.059 ( 0.059)	Loss -0.9173 (-0.9173)
Epoch: [256][0/2]	Time  0.212 ( 0.212)	Data  0.060 ( 0.060)	Loss -0.9278 (-0.9278)
Epoch: [257][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9207 (-0.9207)
Epoch: [258][0/2]	Time  0.206 ( 0.206)	Data  0.061 ( 0.061)	Loss -0.9235 (-0.9235)
Epoch: [259][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9168 (-0.9168)
Epoch: [260][0/2]	Time  0.219 ( 0.219)	Data  0.059 ( 0.059)	Loss -0.9199 (-0.9199)
Epoch: [261][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9221 (-0.9221)
Epoch: [262][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9219 (-0.9219)
Epoch: [263][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9149 (-0.9149)
Epoch: [264][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9235 (-0.9235)
Epoch: [265][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9226 (-0.9226)
Epoch: [266][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9176 (-0.9176)
Epoch: [267][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9284 (-0.9284)
Epoch: [268][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.9212 (-0.9212)
Epoch: [269][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9205 (-0.9205)
Epoch: [270][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9202 (-0.9202)
Epoch: [271][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.9180 (-0.9180)
Epoch: [272][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9144 (-0.9144)
Epoch: [273][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9220 (-0.9220)
Epoch: [274][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9143 (-0.9143)
Epoch: [275][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9195 (-0.9195)
Epoch: [276][0/2]	Time  0.202 ( 0.202)	Data  0.062 ( 0.062)	Loss -0.9252 (-0.9252)
Epoch: [277][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9230 (-0.9230)
Epoch: [278][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9248 (-0.9248)
Epoch: [279][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9218 (-0.9218)
Epoch: [280][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9243 (-0.9243)
Epoch: [281][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9197 (-0.9197)
Epoch: [282][0/2]	Time  0.179 ( 0.179)	Data  0.057 ( 0.057)	Loss -0.9166 (-0.9166)
Epoch: [283][0/2]	Time  0.214 ( 0.214)	Data  0.057 ( 0.057)	Loss -0.9109 (-0.9109)
Epoch: [284][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9180 (-0.9180)
Epoch: [285][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9195 (-0.9195)
Epoch: [286][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9111 (-0.9111)
Epoch: [287][0/2]	Time  0.198 ( 0.198)	Data  0.056 ( 0.056)	Loss -0.9292 (-0.9292)
Epoch: [288][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.9274 (-0.9274)
Epoch: [289][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9186 (-0.9186)
Epoch: [290][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9188 (-0.9188)
Epoch: [291][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9306 (-0.9306)
Epoch: [292][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9226 (-0.9226)
Epoch: [293][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.9224 (-0.9224)
Epoch: [294][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9237 (-0.9237)
Epoch: [295][0/2]	Time  0.215 ( 0.215)	Data  0.060 ( 0.060)	Loss -0.9209 (-0.9209)
Epoch: [296][0/2]	Time  0.181 ( 0.181)	Data  0.057 ( 0.057)	Loss -0.9104 (-0.9104)
Epoch: [297][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9285 (-0.9285)
Epoch: [298][0/2]	Time  0.205 ( 0.205)	Data  0.061 ( 0.061)	Loss -0.9211 (-0.9211)
Epoch: [299][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9305 (-0.9305)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Use GPU: 0 for training
=> creating model 'resnet50'
=> loading checkpoint 'checkpoints/checkpoint_0099.pth.tar'
=> loaded pre-trained model 'checkpoints/checkpoint_0099.pth.tar'
Epoch: [0][0/1]	Time  1.005 ( 1.005)	Data  0.094 ( 0.094)	Loss 1.7972e+00 (1.7972e+00)	Acc@1  18.42 ( 18.42)	Acc@5  92.17 ( 92.17)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.199 ( 0.199)	Loss 1.5516e+00 (1.5516e+00)	Acc@1  25.78 ( 25.78)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.073 ( 0.053)	Loss 1.7253e+00 (1.5814e+00)	Acc@1   0.00 ( 26.95)	Acc@5 100.00 (100.00)
 * Acc@1 26.947 Acc@5 100.000
=> loading 'checkpoints/checkpoint_0099.pth.tar' for sanity check
=> sanity check passed.
Epoch: [1][0/1]	Time  0.180 ( 0.180)	Data  0.091 ( 0.091)	Loss 1.6882e+00 (1.6882e+00)	Acc@1  25.50 ( 25.50)	Acc@5  98.33 ( 98.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 1.8140e-01 (1.8140e-01)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 9.5311e-01 (5.2278e+00)	Acc@1  37.50 ( 36.53)	Acc@5 100.00 ( 93.26)
 * Acc@1 36.526 Acc@5 93.263
Epoch: [2][0/1]	Time  0.160 ( 0.160)	Data  0.086 ( 0.086)	Loss 4.1121e+00 (4.1121e+00)	Acc@1  34.50 ( 34.50)	Acc@5  89.83 ( 89.83)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 6.3721e-01 (6.3721e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.033)	Loss 2.4340e+00 (1.1088e+01)	Acc@1   0.00 ( 38.98)	Acc@5 100.00 ( 84.81)
 * Acc@1 38.980 Acc@5 84.813
Epoch: [3][0/1]	Time  0.165 ( 0.165)	Data  0.093 ( 0.093)	Loss 9.0299e+00 (9.0299e+00)	Acc@1  34.67 ( 34.67)	Acc@5  85.42 ( 85.42)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.036 ( 0.036)	Loss 1.7725e+00 (1.7725e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 5.7693e-01 (1.4343e+01)	Acc@1  75.00 ( 32.28)	Acc@5 100.00 ( 83.33)
 * Acc@1 32.282 Acc@5 83.333
Epoch: [4][0/1]	Time  0.162 ( 0.162)	Data  0.089 ( 0.089)	Loss 1.2681e+01 (1.2681e+01)	Acc@1  26.50 ( 26.50)	Acc@5  83.33 ( 83.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 2.1671e-01 (2.1671e-01)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.032)	Loss 4.6491e+00 (1.8017e+01)	Acc@1   0.00 ( 36.45)	Acc@5 100.00 ( 87.42)
 * Acc@1 36.449 Acc@5 87.422
Epoch: [5][0/1]	Time  0.166 ( 0.166)	Data  0.089 ( 0.089)	Loss 1.6621e+01 (1.6621e+01)	Acc@1  33.58 ( 33.58)	Acc@5  85.25 ( 85.25)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.036 ( 0.036)	Loss 2.8472e-04 (2.8472e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.032)	Loss 6.1426e-02 (1.8987e+01)	Acc@1 100.00 ( 35.94)	Acc@5 100.00 ( 90.58)
 * Acc@1 35.942 Acc@5 90.576
Epoch: [6][0/1]	Time  0.244 ( 0.244)	Data  0.089 ( 0.089)	Loss 1.7802e+01 (1.7802e+01)	Acc@1  32.17 ( 32.17)	Acc@5  90.33 ( 90.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.5110e-06 (5.5110e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.0334e+01 (1.3067e+01)	Acc@1   0.00 ( 23.91)	Acc@5 100.00 ( 93.50)
 * Acc@1 23.910 Acc@5 93.497
Epoch: [7][0/1]	Time  0.248 ( 0.248)	Data  0.086 ( 0.086)	Loss 1.1698e+01 (1.1698e+01)	Acc@1  25.00 ( 25.00)	Acc@5  89.50 ( 89.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 5.2322e+00 (5.2322e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 3.0980e+01 (2.5797e+01)	Acc@1   0.00 ( 16.67)	Acc@5 100.00 ( 95.44)
 * Acc@1 16.667 Acc@5 95.444
Epoch: [8][0/1]	Time  0.246 ( 0.246)	Data  0.090 ( 0.090)	Loss 2.3004e+01 (2.3004e+01)	Acc@1  16.67 ( 16.67)	Acc@5  91.75 ( 91.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.9046e+00 (4.9046e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 3.7437e+01 (3.6472e+01)	Acc@1   0.00 ( 16.67)	Acc@5 100.00 ( 95.87)
 * Acc@1 16.667 Acc@5 95.872
Epoch: [9][0/1]	Time  0.248 ( 0.248)	Data  0.089 ( 0.089)	Loss 3.1353e+01 (3.1353e+01)	Acc@1  16.67 ( 16.67)	Acc@5  93.17 ( 93.17)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 3.9263e-05 (3.9263e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 2.3648e+01 (1.8015e+01)	Acc@1   0.00 ( 33.33)	Acc@5 100.00 ( 94.55)
 * Acc@1 33.333 Acc@5 94.548
Epoch: [10][0/1]	Time  0.249 ( 0.249)	Data  0.089 ( 0.089)	Loss 1.7191e+01 (1.7191e+01)	Acc@1  32.17 ( 32.17)	Acc@5  94.92 ( 94.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 6.2763e-02 (6.2763e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 1.1642e+01 (2.3570e+01)	Acc@1   0.00 ( 30.76)	Acc@5 100.00 ( 98.05)
 * Acc@1 30.763 Acc@5 98.053
Epoch: [11][0/1]	Time  0.246 ( 0.246)	Data  0.088 ( 0.088)	Loss 1.8851e+01 (1.8851e+01)	Acc@1  31.58 ( 31.58)	Acc@5  95.92 ( 95.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 6.6952e+00 (6.6952e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 1.8107e+00 (1.6369e+01)	Acc@1  75.00 ( 27.49)	Acc@5 100.00 ( 92.56)
 * Acc@1 27.492 Acc@5 92.562
Epoch: [12][0/1]	Time  0.249 ( 0.249)	Data  0.090 ( 0.090)	Loss 1.8464e+01 (1.8464e+01)	Acc@1  24.75 ( 24.75)	Acc@5  91.83 ( 91.83)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.9852e+00 (8.9852e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 2.2091e+00 (2.4822e+01)	Acc@1  37.50 ( 29.24)	Acc@5 100.00 ( 87.77)
 * Acc@1 29.245 Acc@5 87.773
Epoch: [13][0/1]	Time  0.236 ( 0.236)	Data  0.082 ( 0.082)	Loss 2.7068e+01 (2.7068e+01)	Acc@1  26.33 ( 26.33)	Acc@5  88.67 ( 88.67)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 5.0571e-01 (5.0571e-01)	Acc@1  80.47 ( 80.47)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.008 ( 0.049)	Loss 1.2777e+00 (2.4472e+01)	Acc@1  75.00 ( 36.92)	Acc@5 100.00 ( 86.21)
 * Acc@1 36.916 Acc@5 86.215
Epoch: [14][0/1]	Time  0.243 ( 0.243)	Data  0.089 ( 0.089)	Loss 2.7076e+01 (2.7076e+01)	Acc@1  34.17 ( 34.17)	Acc@5  87.83 ( 87.83)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 1.7797e-06 (1.7797e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 2.1314e-02 (2.0482e+01)	Acc@1 100.00 ( 48.52)	Acc@5 100.00 ( 86.84)
 * Acc@1 48.520 Acc@5 86.838
Epoch: [15][0/1]	Time  0.256 ( 0.256)	Data  0.090 ( 0.090)	Loss 2.1412e+01 (2.1412e+01)	Acc@1  39.75 ( 39.75)	Acc@5  88.25 ( 88.25)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 4.1910e-09 (4.1910e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 5.3337e+00 (1.8911e+01)	Acc@1  37.50 ( 41.00)	Acc@5 100.00 ( 89.45)
 * Acc@1 41.005 Acc@5 89.447
Epoch: [16][0/1]	Time  0.241 ( 0.241)	Data  0.087 ( 0.087)	Loss 1.9748e+01 (1.9748e+01)	Acc@1  34.33 ( 34.33)	Acc@5  89.75 ( 89.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 3.2596e-09 (3.2596e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 7.0609e+00 (1.3613e+01)	Acc@1  12.50 ( 39.49)	Acc@5 100.00 ( 93.61)
 * Acc@1 39.486 Acc@5 93.614
Epoch: [17][0/1]	Time  0.242 ( 0.242)	Data  0.089 ( 0.089)	Loss 1.6101e+01 (1.6101e+01)	Acc@1  36.58 ( 36.58)	Acc@5  92.33 ( 92.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.5180e-07 (1.5180e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 5.5275e+00 (1.1866e+01)	Acc@1  25.00 ( 41.86)	Acc@5 100.00 ( 97.35)
 * Acc@1 41.861 Acc@5 97.352
Epoch: [18][0/1]	Time  0.240 ( 0.240)	Data  0.086 ( 0.086)	Loss 1.4724e+01 (1.4724e+01)	Acc@1  39.92 ( 39.92)	Acc@5  95.75 ( 95.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 6.7756e-04 (6.7756e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 5.7694e+00 (1.4006e+01)	Acc@1  25.00 ( 45.13)	Acc@5 100.00 ( 99.57)
 * Acc@1 45.132 Acc@5 99.572
Epoch: [19][0/1]	Time  0.242 ( 0.242)	Data  0.085 ( 0.085)	Loss 1.6507e+01 (1.6507e+01)	Acc@1  43.83 ( 43.83)	Acc@5  96.33 ( 96.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 8.2337e+00 (8.2337e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 4.9423e+00 (1.7066e+01)	Acc@1   0.00 ( 26.32)	Acc@5 100.00 ( 99.96)
 * Acc@1 26.324 Acc@5 99.961
Epoch: [20][0/1]	Time  0.251 ( 0.251)	Data  0.090 ( 0.090)	Loss 1.7084e+01 (1.7084e+01)	Acc@1  25.08 ( 25.08)	Acc@5  97.42 ( 97.42)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.1658e+01 (1.1658e+01)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.6947e+00 (1.4689e+01)	Acc@1  50.00 ( 36.41)	Acc@5 100.00 (100.00)
 * Acc@1 36.410 Acc@5 100.000
Epoch: [21][0/1]	Time  0.247 ( 0.247)	Data  0.090 ( 0.090)	Loss 1.4166e+01 (1.4166e+01)	Acc@1  30.00 ( 30.00)	Acc@5  97.50 ( 97.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.0915e+01 (1.0915e+01)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 2.5431e-01 (1.0767e+01)	Acc@1  87.50 ( 38.28)	Acc@5 100.00 (100.00)
 * Acc@1 38.279 Acc@5 100.000
Epoch: [22][0/1]	Time  0.253 ( 0.253)	Data  0.094 ( 0.094)	Loss 1.1221e+01 (1.1221e+01)	Acc@1  34.00 ( 34.00)	Acc@5  97.25 ( 97.25)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 9.6620e+00 (9.6620e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 2.5740e+00 (9.7615e+00)	Acc@1  12.50 ( 34.85)	Acc@5 100.00 (100.00)
 * Acc@1 34.852 Acc@5 100.000
Epoch: [23][0/1]	Time  0.245 ( 0.245)	Data  0.089 ( 0.089)	Loss 9.6927e+00 (9.6927e+00)	Acc@1  32.83 ( 32.83)	Acc@5  98.00 ( 98.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 8.5136e-01 (8.5136e-01)	Acc@1  71.09 ( 71.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.1027e+00 (6.7471e+00)	Acc@1  50.00 ( 45.79)	Acc@5 100.00 (100.00)
 * Acc@1 45.794 Acc@5 100.000
Epoch: [24][0/1]	Time  0.242 ( 0.242)	Data  0.088 ( 0.088)	Loss 7.1575e+00 (7.1575e+00)	Acc@1  46.92 ( 46.92)	Acc@5  97.92 ( 97.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.1070e-04 (1.1070e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 3.0693e-01 (5.2156e+00)	Acc@1  75.00 ( 54.01)	Acc@5 100.00 (100.00)
 * Acc@1 54.011 Acc@5 100.000
Epoch: [25][0/1]	Time  0.248 ( 0.248)	Data  0.090 ( 0.090)	Loss 5.4053e+00 (5.4053e+00)	Acc@1  50.08 ( 50.08)	Acc@5  98.08 ( 98.08)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.2596e-09 (3.2596e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 1.0793e+00 (6.4373e+00)	Acc@1  50.00 ( 55.69)	Acc@5 100.00 ( 98.13)
 * Acc@1 55.685 Acc@5 98.131
Epoch: [26][0/1]	Time  0.242 ( 0.242)	Data  0.089 ( 0.089)	Loss 6.7322e+00 (6.7322e+00)	Acc@1  46.50 ( 46.50)	Acc@5  98.50 ( 98.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 7.0494e+00 (8.3047e+00)	Acc@1  12.50 ( 51.71)	Acc@5 100.00 ( 97.00)
 * Acc@1 51.713 Acc@5 97.002
Epoch: [27][0/1]	Time  0.241 ( 0.241)	Data  0.087 ( 0.087)	Loss 8.1390e+00 (8.1390e+00)	Acc@1  44.00 ( 44.00)	Acc@5  97.17 ( 97.17)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 9.3132e-10 (9.3132e-10)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 4.1307e+00 (7.9135e+00)	Acc@1  25.00 ( 51.40)	Acc@5 100.00 ( 97.12)
 * Acc@1 51.402 Acc@5 97.118
Epoch: [28][0/1]	Time  0.240 ( 0.240)	Data  0.085 ( 0.085)	Loss 8.0819e+00 (8.0819e+00)	Acc@1  41.00 ( 41.00)	Acc@5  98.17 ( 98.17)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.1370e-06 (1.1370e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 4.1628e-01 (7.9111e+00)	Acc@1  75.00 ( 57.13)	Acc@5 100.00 ( 97.74)
 * Acc@1 57.126 Acc@5 97.741
Epoch: [29][0/1]	Time  0.243 ( 0.243)	Data  0.085 ( 0.085)	Loss 7.4789e+00 (7.4789e+00)	Acc@1  45.83 ( 45.83)	Acc@5  97.92 ( 97.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 5.8342e-03 (5.8342e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 4.7309e-02 (8.9078e+00)	Acc@1 100.00 ( 58.92)	Acc@5 100.00 ( 98.13)
 * Acc@1 58.917 Acc@5 98.131
Epoch: [30][0/1]	Time  0.248 ( 0.248)	Data  0.090 ( 0.090)	Loss 7.2960e+00 (7.2960e+00)	Acc@1  50.83 ( 50.83)	Acc@5  99.08 ( 99.08)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.7892e-03 (5.7892e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.8467e+00 (8.3722e+00)	Acc@1  50.00 ( 57.59)	Acc@5 100.00 ( 98.71)
 * Acc@1 57.593 Acc@5 98.715
Epoch: [31][0/1]	Time  0.252 ( 0.252)	Data  0.092 ( 0.092)	Loss 6.9240e+00 (6.9240e+00)	Acc@1  51.33 ( 51.33)	Acc@5  98.75 ( 98.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 4.9146e-01 (4.9146e-01)	Acc@1  84.77 ( 84.77)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 6.0500e+00 (6.9822e+00)	Acc@1  12.50 ( 55.06)	Acc@5 100.00 ( 99.57)
 * Acc@1 55.062 Acc@5 99.572
Epoch: [32][0/1]	Time  0.246 ( 0.246)	Data  0.089 ( 0.089)	Loss 5.8600e+00 (5.8600e+00)	Acc@1  50.50 ( 50.50)	Acc@5  99.67 ( 99.67)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.4442e+00 (1.4442e+00)	Acc@1  60.55 ( 60.55)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 6.2348e+00 (5.6279e+00)	Acc@1   0.00 ( 45.76)	Acc@5 100.00 ( 99.96)
 * Acc@1 45.755 Acc@5 99.961
Epoch: [33][0/1]	Time  0.251 ( 0.251)	Data  0.094 ( 0.094)	Loss 5.9671e+00 (5.9671e+00)	Acc@1  43.25 ( 43.25)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 8.5845e-01 (8.5845e-01)	Acc@1  78.52 ( 78.52)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 2.3660e+00 (3.9261e+00)	Acc@1  25.00 ( 55.65)	Acc@5 100.00 (100.00)
 * Acc@1 55.646 Acc@5 100.000
Epoch: [34][0/1]	Time  0.243 ( 0.243)	Data  0.089 ( 0.089)	Loss 4.2988e+00 (4.2988e+00)	Acc@1  49.83 ( 49.83)	Acc@5  99.83 ( 99.83)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.9396e+00 (1.9396e+00)	Acc@1  46.88 ( 46.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 7.6847e-03 (4.4568e+00)	Acc@1 100.00 ( 53.97)	Acc@5 100.00 (100.00)
 * Acc@1 53.972 Acc@5 100.000
Epoch: [35][0/1]	Time  0.249 ( 0.249)	Data  0.091 ( 0.091)	Loss 5.2863e+00 (5.2863e+00)	Acc@1  42.58 ( 42.58)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 3.4068e-01 (3.4068e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 3.1646e+00 (3.3398e+00)	Acc@1  25.00 ( 53.00)	Acc@5 100.00 (100.00)
 * Acc@1 52.998 Acc@5 100.000
Epoch: [36][0/1]	Time  0.241 ( 0.241)	Data  0.085 ( 0.085)	Loss 3.8895e+00 (3.8895e+00)	Acc@1  49.00 ( 49.00)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.0848e-01 (2.0848e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 8.5335e+00 (4.0037e+00)	Acc@1   0.00 ( 53.19)	Acc@5 100.00 (100.00)
 * Acc@1 53.193 Acc@5 100.000
Epoch: [37][0/1]	Time  0.243 ( 0.243)	Data  0.086 ( 0.086)	Loss 4.3944e+00 (4.3944e+00)	Acc@1  47.42 ( 47.42)	Acc@5  99.50 ( 99.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 2.3938e-01 (2.3938e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 5.9702e+00 (3.7339e+00)	Acc@1   0.00 ( 53.39)	Acc@5 100.00 (100.00)
 * Acc@1 53.388 Acc@5 100.000
Epoch: [38][0/1]	Time  0.247 ( 0.247)	Data  0.089 ( 0.089)	Loss 4.1300e+00 (4.1300e+00)	Acc@1  47.33 ( 47.33)	Acc@5  99.33 ( 99.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.2538e-01 (3.2538e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 8.7663e-01 (2.6451e+00)	Acc@1  87.50 ( 60.32)	Acc@5 100.00 (100.00)
 * Acc@1 60.319 Acc@5 100.000
Epoch: [39][0/1]	Time  0.248 ( 0.248)	Data  0.091 ( 0.091)	Loss 3.4147e+00 (3.4147e+00)	Acc@1  55.50 ( 55.50)	Acc@5  99.50 ( 99.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.2055e-01 (2.2055e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 3.3856e-01 (2.8002e+00)	Acc@1  75.00 ( 58.96)	Acc@5 100.00 (100.00)
 * Acc@1 58.956 Acc@5 100.000
Epoch: [40][0/1]	Time  0.251 ( 0.251)	Data  0.089 ( 0.089)	Loss 4.2126e+00 (4.2126e+00)	Acc@1  50.83 ( 50.83)	Acc@5  99.25 ( 99.25)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 8.2283e-02 (8.2283e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 4.4764e-01 (2.2736e+00)	Acc@1  75.00 ( 63.12)	Acc@5 100.00 ( 99.77)
 * Acc@1 63.123 Acc@5 99.766
Epoch: [41][0/1]	Time  0.241 ( 0.241)	Data  0.089 ( 0.089)	Loss 2.9974e+00 (2.9974e+00)	Acc@1  58.08 ( 58.08)	Acc@5  99.75 ( 99.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 1.2088e-02 (1.2088e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 2.0991e+00 (2.5900e+00)	Acc@1  12.50 ( 56.00)	Acc@5 100.00 ( 99.61)
 * Acc@1 55.997 Acc@5 99.611
Epoch: [42][0/1]	Time  0.252 ( 0.252)	Data  0.093 ( 0.093)	Loss 2.8834e+00 (2.8834e+00)	Acc@1  52.42 ( 52.42)	Acc@5  99.58 ( 99.58)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 4.7447e-03 (4.7447e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 5.2316e+00 (3.0104e+00)	Acc@1   0.00 ( 52.22)	Acc@5 100.00 ( 99.77)
 * Acc@1 52.220 Acc@5 99.766
Epoch: [43][0/1]	Time  0.242 ( 0.242)	Data  0.085 ( 0.085)	Loss 2.9947e+00 (2.9947e+00)	Acc@1  50.25 ( 50.25)	Acc@5  99.50 ( 99.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 1.6803e-02 (1.6803e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 2.3903e+00 (2.6406e+00)	Acc@1  62.50 ( 52.14)	Acc@5 100.00 ( 99.77)
 * Acc@1 52.142 Acc@5 99.766
Epoch: [44][0/1]	Time  0.246 ( 0.246)	Data  0.087 ( 0.087)	Loss 2.7053e+00 (2.7053e+00)	Acc@1  49.67 ( 49.67)	Acc@5  99.83 ( 99.83)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 5.1369e-04 (5.1369e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 3.1478e-01 (2.8741e+00)	Acc@1  87.50 ( 63.47)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.474 Acc@5 99.961
Epoch: [45][0/1]	Time  0.247 ( 0.247)	Data  0.086 ( 0.086)	Loss 3.0434e+00 (3.0434e+00)	Acc@1  54.25 ( 54.25)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 2.3819e-04 (2.3819e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 2.5066e-01 (2.1821e+00)	Acc@1  87.50 ( 64.14)	Acc@5 100.00 (100.00)
 * Acc@1 64.136 Acc@5 100.000
Epoch: [46][0/1]	Time  0.242 ( 0.242)	Data  0.089 ( 0.089)	Loss 2.4781e+00 (2.4781e+00)	Acc@1  57.25 ( 57.25)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 2.3244e-03 (2.3244e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 2.0991e+00 (1.8369e+00)	Acc@1  25.00 ( 55.41)	Acc@5 100.00 (100.00)
 * Acc@1 55.413 Acc@5 100.000
Epoch: [47][0/1]	Time  0.240 ( 0.240)	Data  0.087 ( 0.087)	Loss 2.2163e+00 (2.2163e+00)	Acc@1  53.75 ( 53.75)	Acc@5  99.67 ( 99.67)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.9907e-02 (1.9907e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 4.1141e+00 (2.0467e+00)	Acc@1  12.50 ( 57.13)	Acc@5 100.00 (100.00)
 * Acc@1 57.126 Acc@5 100.000
Epoch: [48][0/1]	Time  0.245 ( 0.245)	Data  0.086 ( 0.086)	Loss 2.3377e+00 (2.3377e+00)	Acc@1  54.75 ( 54.75)	Acc@5  99.75 ( 99.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.9859e-02 (3.9859e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 2.4891e+00 (1.8908e+00)	Acc@1  37.50 ( 61.41)	Acc@5 100.00 (100.00)
 * Acc@1 61.410 Acc@5 100.000
Epoch: [49][0/1]	Time  0.242 ( 0.242)	Data  0.085 ( 0.085)	Loss 2.3963e+00 (2.3963e+00)	Acc@1  57.25 ( 57.25)	Acc@5  99.58 ( 99.58)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 8.0010e-03 (8.0010e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.5903e+00 (1.6204e+00)	Acc@1  62.50 ( 63.28)	Acc@5 100.00 (100.00)
 * Acc@1 63.279 Acc@5 100.000
Epoch: [50][0/1]	Time  0.330 ( 0.330)	Data  0.172 ( 0.172)	Loss 2.1671e+00 (2.1671e+00)	Acc@1  59.50 ( 59.50)	Acc@5  99.50 ( 99.50)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.6825e-02 (1.6825e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 5.3040e-01 (1.6766e+00)	Acc@1  75.00 ( 63.94)	Acc@5 100.00 (100.00)
 * Acc@1 63.941 Acc@5 100.000
Epoch: [51][0/1]	Time  0.246 ( 0.246)	Data  0.091 ( 0.091)	Loss 2.0242e+00 (2.0242e+00)	Acc@1  56.75 ( 56.75)	Acc@5  99.33 ( 99.33)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 6.1158e-03 (6.1158e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 7.3311e-01 (1.8171e+00)	Acc@1  62.50 ( 62.27)	Acc@5 100.00 (100.00)
 * Acc@1 62.266 Acc@5 100.000
Epoch: [52][0/1]	Time  0.247 ( 0.247)	Data  0.091 ( 0.091)	Loss 2.2251e+00 (2.2251e+00)	Acc@1  56.25 ( 56.25)	Acc@5  99.75 ( 99.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.9271e-02 (1.9271e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.9125e+00 (1.8273e+00)	Acc@1  12.50 ( 59.89)	Acc@5 100.00 (100.00)
 * Acc@1 59.891 Acc@5 100.000
Epoch: [53][0/1]	Time  0.241 ( 0.241)	Data  0.085 ( 0.085)	Loss 1.9921e+00 (1.9921e+00)	Acc@1  57.42 ( 57.42)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.6551e-02 (1.6551e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.5499e+00 (1.6242e+00)	Acc@1  37.50 ( 62.81)	Acc@5 100.00 (100.00)
 * Acc@1 62.812 Acc@5 100.000
Epoch: [54][0/1]	Time  0.251 ( 0.251)	Data  0.093 ( 0.093)	Loss 1.6745e+00 (1.6745e+00)	Acc@1  59.42 ( 59.42)	Acc@5  99.75 ( 99.75)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 7.5160e-02 (7.5160e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.6896e+00 (1.6134e+00)	Acc@1  37.50 ( 63.20)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.201 Acc@5 99.961
Epoch: [55][0/1]	Time  0.240 ( 0.240)	Data  0.087 ( 0.087)	Loss 1.7213e+00 (1.7213e+00)	Acc@1  61.08 ( 61.08)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 9.3797e-02 (9.3797e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.6051e+00 (1.5938e+00)	Acc@1  50.00 ( 63.32)	Acc@5 100.00 (100.00)
 * Acc@1 63.318 Acc@5 100.000
Epoch: [56][0/1]	Time  0.249 ( 0.249)	Data  0.093 ( 0.093)	Loss 1.7838e+00 (1.7838e+00)	Acc@1  59.67 ( 59.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 5.0089e-02 (5.0089e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 2.0235e+00 (1.5686e+00)	Acc@1  25.00 ( 62.34)	Acc@5 100.00 (100.00)
 * Acc@1 62.344 Acc@5 100.000
Epoch: [57][0/1]	Time  0.246 ( 0.246)	Data  0.088 ( 0.088)	Loss 1.6794e+00 (1.6794e+00)	Acc@1  60.67 ( 60.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 6.9875e-02 (6.9875e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 1.1900e+00 (1.5503e+00)	Acc@1  37.50 ( 62.54)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.539 Acc@5 99.961
Epoch: [58][0/1]	Time  0.246 ( 0.246)	Data  0.088 ( 0.088)	Loss 1.6178e+00 (1.6178e+00)	Acc@1  62.17 ( 62.17)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 5.0681e-02 (5.0681e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 6.1720e-01 (1.4728e+00)	Acc@1  75.00 ( 63.94)	Acc@5 100.00 (100.00)
 * Acc@1 63.941 Acc@5 100.000
Epoch: [59][0/1]	Time  0.250 ( 0.250)	Data  0.098 ( 0.098)	Loss 1.5397e+00 (1.5397e+00)	Acc@1  61.42 ( 61.42)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.6068e-02 (8.6068e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 6.4228e-01 (1.5414e+00)	Acc@1  75.00 ( 62.38)	Acc@5 100.00 ( 99.92)
 * Acc@1 62.383 Acc@5 99.922
Epoch: [60][0/1]	Time  0.251 ( 0.251)	Data  0.091 ( 0.091)	Loss 1.6600e+00 (1.6600e+00)	Acc@1  60.50 ( 60.50)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 6.2417e-02 (6.2417e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 9.4294e-01 (1.5041e+00)	Acc@1  37.50 ( 63.36)	Acc@5 100.00 ( 99.92)
 * Acc@1 63.357 Acc@5 99.922
Epoch: [61][0/1]	Time  0.247 ( 0.247)	Data  0.090 ( 0.090)	Loss 1.5042e+00 (1.5042e+00)	Acc@1  62.08 ( 62.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 3.3226e-02 (3.3226e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.1422e+00 (1.4309e+00)	Acc@1  50.00 ( 64.45)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.447 Acc@5 99.961
Epoch: [62][0/1]	Time  0.243 ( 0.243)	Data  0.087 ( 0.087)	Loss 1.5024e+00 (1.5024e+00)	Acc@1  61.33 ( 61.33)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 8.8403e-02 (8.8403e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 5.8125e-01 (1.4746e+00)	Acc@1  75.00 ( 63.43)	Acc@5 100.00 (100.00)
 * Acc@1 63.435 Acc@5 100.000
Epoch: [63][0/1]	Time  0.247 ( 0.247)	Data  0.090 ( 0.090)	Loss 1.5804e+00 (1.5804e+00)	Acc@1  60.58 ( 60.58)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 5.5133e-02 (5.5133e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 1.1862e+00 (1.3937e+00)	Acc@1  62.50 ( 64.91)	Acc@5 100.00 (100.00)
 * Acc@1 64.914 Acc@5 100.000
Epoch: [64][0/1]	Time  0.246 ( 0.246)	Data  0.089 ( 0.089)	Loss 1.5485e+00 (1.5485e+00)	Acc@1  62.67 ( 62.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.1954e-02 (8.1954e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 9.2220e-01 (1.3623e+00)	Acc@1  50.00 ( 65.46)	Acc@5 100.00 (100.00)
 * Acc@1 65.460 Acc@5 100.000
Epoch: [65][0/1]	Time  0.244 ( 0.244)	Data  0.087 ( 0.087)	Loss 1.5387e+00 (1.5387e+00)	Acc@1  61.67 ( 61.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.2280e-01 (2.2280e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 9.5048e-01 (1.3284e+00)	Acc@1  50.00 ( 64.56)	Acc@5 100.00 (100.00)
 * Acc@1 64.564 Acc@5 100.000
Epoch: [66][0/1]	Time  0.240 ( 0.240)	Data  0.083 ( 0.083)	Loss 1.5286e+00 (1.5286e+00)	Acc@1  62.00 ( 62.00)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.1956e-01 (1.1956e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.3377e+00 (1.3232e+00)	Acc@1  25.00 ( 64.68)	Acc@5 100.00 (100.00)
 * Acc@1 64.681 Acc@5 100.000
Epoch: [67][0/1]	Time  0.249 ( 0.249)	Data  0.091 ( 0.091)	Loss 1.3949e+00 (1.3949e+00)	Acc@1  62.67 ( 62.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.6939e-01 (1.6939e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 5.7097e-01 (1.3187e+00)	Acc@1  87.50 ( 64.68)	Acc@5 100.00 (100.00)
 * Acc@1 64.681 Acc@5 100.000
Epoch: [68][0/1]	Time  0.248 ( 0.248)	Data  0.092 ( 0.092)	Loss 1.3440e+00 (1.3440e+00)	Acc@1  61.08 ( 61.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 2.1417e-01 (2.1417e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 9.8446e-01 (1.3361e+00)	Acc@1  50.00 ( 63.32)	Acc@5 100.00 (100.00)
 * Acc@1 63.318 Acc@5 100.000
Epoch: [69][0/1]	Time  0.250 ( 0.250)	Data  0.090 ( 0.090)	Loss 1.4044e+00 (1.4044e+00)	Acc@1  61.25 ( 61.25)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.1903e-01 (2.1903e-01)	Acc@1  93.36 ( 93.36)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 1.2388e+00 (1.3466e+00)	Acc@1  62.50 ( 62.93)	Acc@5 100.00 (100.00)
 * Acc@1 62.928 Acc@5 100.000
Epoch: [70][0/1]	Time  0.246 ( 0.246)	Data  0.089 ( 0.089)	Loss 1.3907e+00 (1.3907e+00)	Acc@1  63.67 ( 63.67)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 1.8449e-01 (1.8449e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 1.2000e+00 (1.3203e+00)	Acc@1  50.00 ( 62.54)	Acc@5 100.00 (100.00)
 * Acc@1 62.539 Acc@5 100.000
Epoch: [71][0/1]	Time  0.247 ( 0.247)	Data  0.092 ( 0.092)	Loss 1.4062e+00 (1.4062e+00)	Acc@1  60.17 ( 60.17)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
.\main_lincls.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
.\main_lincls.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained, map_location="cpu")
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
.\main_lincls.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_weights, map_location="cpu")
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.4418e-01 (1.4418e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 1.1590e+00 (1.3218e+00)	Acc@1  62.50 ( 62.81)	Acc@5 100.00 (100.00)
 * Acc@1 62.812 Acc@5 100.000
Epoch: [72][0/1]	Time  0.246 ( 0.246)	Data  0.086 ( 0.086)	Loss 1.3859e+00 (1.3859e+00)	Acc@1  62.75 ( 62.75)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 1.1181e-01 (1.1181e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 1.2749e+00 (1.2470e+00)	Acc@1  50.00 ( 63.67)	Acc@5 100.00 (100.00)
 * Acc@1 63.668 Acc@5 100.000
Epoch: [73][0/1]	Time  0.249 ( 0.249)	Data  0.091 ( 0.091)	Loss 1.3397e+00 (1.3397e+00)	Acc@1  62.50 ( 62.50)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.0314e-01 (2.0314e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 5.9595e-01 (1.3212e+00)	Acc@1  87.50 ( 64.41)	Acc@5 100.00 (100.00)
 * Acc@1 64.408 Acc@5 100.000
Epoch: [74][0/1]	Time  0.247 ( 0.247)	Data  0.085 ( 0.085)	Loss 1.3389e+00 (1.3389e+00)	Acc@1  64.25 ( 64.25)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.058 ( 0.058)	Loss 1.9241e-01 (1.9241e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 9.6135e-01 (1.2742e+00)	Acc@1  75.00 ( 64.49)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.486 Acc@5 99.961
Epoch: [75][0/1]	Time  0.245 ( 0.245)	Data  0.092 ( 0.092)	Loss 1.3987e+00 (1.3987e+00)	Acc@1  61.42 ( 61.42)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.7367e-01 (1.7367e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.3686e+00 (1.3120e+00)	Acc@1  37.50 ( 63.98)	Acc@5 100.00 (100.00)
 * Acc@1 63.980 Acc@5 100.000
Epoch: [76][0/1]	Time  0.246 ( 0.246)	Data  0.089 ( 0.089)	Loss 1.3904e+00 (1.3904e+00)	Acc@1  63.58 ( 63.58)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 9.8858e-02 (9.8858e-02)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.0038e+00 (1.2863e+00)	Acc@1  37.50 ( 63.98)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.980 Acc@5 99.961
Epoch: [77][0/1]	Time  0.243 ( 0.243)	Data  0.085 ( 0.085)	Loss 1.3778e+00 (1.3778e+00)	Acc@1  63.92 ( 63.92)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 9.8788e-02 (9.8788e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 6.3052e-01 (1.3169e+00)	Acc@1  75.00 ( 63.90)	Acc@5 100.00 (100.00)
 * Acc@1 63.902 Acc@5 100.000
Epoch: [78][0/1]	Time  0.241 ( 0.241)	Data  0.089 ( 0.089)	Loss 1.4168e+00 (1.4168e+00)	Acc@1  63.08 ( 63.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.4656e-01 (1.4656e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.1830e+00 (1.2547e+00)	Acc@1  37.50 ( 63.40)	Acc@5 100.00 (100.00)
 * Acc@1 63.396 Acc@5 100.000
Epoch: [79][0/1]	Time  0.241 ( 0.241)	Data  0.087 ( 0.087)	Loss 1.3821e+00 (1.3821e+00)	Acc@1  62.67 ( 62.67)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.4812e-01 (1.4812e-01)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 8.6493e-01 (1.2695e+00)	Acc@1  37.50 ( 63.82)	Acc@5 100.00 (100.00)
 * Acc@1 63.824 Acc@5 100.000
Epoch: [80][0/1]	Time  0.245 ( 0.245)	Data  0.087 ( 0.087)	Loss 1.2998e+00 (1.2998e+00)	Acc@1  64.17 ( 64.17)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.7959e-01 (1.7959e-01)	Acc@1  95.70 ( 95.70)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 1.0030e+00 (1.3179e+00)	Acc@1  50.00 ( 64.84)	Acc@5 100.00 (100.00)
 * Acc@1 64.836 Acc@5 100.000
Epoch: [81][0/1]	Time  0.242 ( 0.242)	Data  0.086 ( 0.086)	Loss 1.3497e+00 (1.3497e+00)	Acc@1  62.17 ( 62.17)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.1878e-01 (1.1878e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 7.3660e-01 (1.2574e+00)	Acc@1  62.50 ( 62.93)	Acc@5 100.00 (100.00)
 * Acc@1 62.928 Acc@5 100.000
Epoch: [82][0/1]	Time  0.256 ( 0.256)	Data  0.101 ( 0.101)	Loss 1.3311e+00 (1.3311e+00)	Acc@1  64.08 ( 64.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.2597e-01 (1.2597e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.8778e+00 (1.2696e+00)	Acc@1  25.00 ( 64.45)	Acc@5 100.00 (100.00)
 * Acc@1 64.447 Acc@5 100.000
Epoch: [83][0/1]	Time  0.247 ( 0.247)	Data  0.091 ( 0.091)	Loss 1.3933e+00 (1.3933e+00)	Acc@1  60.08 ( 60.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.5826e-01 (1.5826e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.2320e+00 (1.3065e+00)	Acc@1  25.00 ( 63.40)	Acc@5 100.00 (100.00)
 * Acc@1 63.396 Acc@5 100.000
Epoch: [84][0/1]	Time  0.247 ( 0.247)	Data  0.089 ( 0.089)	Loss 1.3727e+00 (1.3727e+00)	Acc@1  62.00 ( 62.00)	Acc@5  99.92 ( 99.92)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.6336e-01 (1.6336e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.2390e+00 (1.2642e+00)	Acc@1  25.00 ( 62.89)	Acc@5 100.00 (100.00)
 * Acc@1 62.889 Acc@5 100.000
Epoch: [85][0/1]	Time  0.242 ( 0.242)	Data  0.088 ( 0.088)	Loss 1.3860e+00 (1.3860e+00)	Acc@1  61.42 ( 61.42)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.3364e-01 (1.3364e-01)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.4587e+00 (1.2692e+00)	Acc@1  50.00 ( 63.47)	Acc@5 100.00 (100.00)
 * Acc@1 63.474 Acc@5 100.000
Epoch: [86][0/1]	Time  0.243 ( 0.243)	Data  0.089 ( 0.089)	Loss 1.3860e+00 (1.3860e+00)	Acc@1  60.25 ( 60.25)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 1.9902e-01 (1.9902e-01)	Acc@1  94.92 ( 94.92)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.1319e+00 (1.3239e+00)	Acc@1  62.50 ( 62.97)	Acc@5 100.00 (100.00)
 * Acc@1 62.967 Acc@5 100.000
Epoch: [87][0/1]	Time  0.245 ( 0.245)	Data  0.087 ( 0.087)	Loss 1.3267e+00 (1.3267e+00)	Acc@1  62.00 ( 62.00)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.1208e-01 (1.1208e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 7.9929e-01 (1.3024e+00)	Acc@1  75.00 ( 63.20)	Acc@5 100.00 (100.00)
 * Acc@1 63.201 Acc@5 100.000
Epoch: [88][0/1]	Time  0.247 ( 0.247)	Data  0.089 ( 0.089)	Loss 1.3236e+00 (1.3236e+00)	Acc@1  63.00 ( 63.00)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.2137e-01 (1.2137e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 1.1911e+00 (1.2790e+00)	Acc@1  50.00 ( 63.24)	Acc@5 100.00 (100.00)
 * Acc@1 63.240 Acc@5 100.000
Epoch: [89][0/1]	Time  0.240 ( 0.240)	Data  0.087 ( 0.087)	Loss 1.3244e+00 (1.3244e+00)	Acc@1  63.08 ( 63.08)	Acc@5 100.00 (100.00)
simsiam program ends here, while ssv:200, normal:200, excep_size:200
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.5904e-01 (1.5904e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.0840e+00 (1.2938e+00)	Acc@1  12.50 ( 64.45)	Acc@5 100.00 (100.00)
 * Acc@1 64.447 Acc@5 100.000
Running with excep_size=100 
Use GPU: 0 for training
=> creating model 'resnet50'
SimSiam(
  (encoder): ResNet(
    (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=512, bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=512, out_features=2048, bias=True)
      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (predictor): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=2048, bias=True)
  )
)
Epoch: [0][0/2]	Time  1.619 ( 1.619)	Data  0.083 ( 0.083)	Loss 0.0048 (0.0048)
Epoch: [1][0/2]	Time  0.140 ( 0.140)	Data  0.061 ( 0.061)	Loss -0.0838 (-0.0838)
Epoch: [2][0/2]	Time  0.135 ( 0.135)	Data  0.058 ( 0.058)	Loss -0.1882 (-0.1882)
Epoch: [3][0/2]	Time  0.133 ( 0.133)	Data  0.058 ( 0.058)	Loss -0.2718 (-0.2718)
Epoch: [4][0/2]	Time  0.134 ( 0.134)	Data  0.057 ( 0.057)	Loss -0.3807 (-0.3807)
Epoch: [5][0/2]	Time  0.133 ( 0.133)	Data  0.058 ( 0.058)	Loss -0.4719 (-0.4719)
Epoch: [6][0/2]	Time  0.138 ( 0.138)	Data  0.058 ( 0.058)	Loss -0.5752 (-0.5752)
Epoch: [7][0/2]	Time  0.174 ( 0.174)	Data  0.065 ( 0.065)	Loss -0.6351 (-0.6351)
Epoch: [8][0/2]	Time  0.176 ( 0.176)	Data  0.061 ( 0.061)	Loss -0.6607 (-0.6607)
Epoch: [9][0/2]	Time  0.203 ( 0.203)	Data  0.060 ( 0.060)	Loss -0.7087 (-0.7087)
Epoch: [10][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.7326 (-0.7326)
Epoch: [11][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.7575 (-0.7575)
Epoch: [12][0/2]	Time  0.193 ( 0.193)	Data  0.057 ( 0.057)	Loss -0.7570 (-0.7570)
Epoch: [13][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.7757 (-0.7757)
Epoch: [14][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.7700 (-0.7700)
Epoch: [15][0/2]	Time  0.179 ( 0.179)	Data  0.057 ( 0.057)	Loss -0.7888 (-0.7888)
Epoch: [16][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.7637 (-0.7637)
Epoch: [17][0/2]	Time  0.200 ( 0.200)	Data  0.060 ( 0.060)	Loss -0.7729 (-0.7729)
Epoch: [18][0/2]	Time  0.207 ( 0.207)	Data  0.059 ( 0.059)	Loss -0.7670 (-0.7670)
Epoch: [19][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.7633 (-0.7633)
Epoch: [20][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.7483 (-0.7483)
Epoch: [21][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.7588 (-0.7588)
Epoch: [22][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.7578 (-0.7578)
Epoch: [23][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.7826 (-0.7826)
Epoch: [24][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.7580 (-0.7580)
Epoch: [25][0/2]	Time  0.194 ( 0.194)	Data  0.057 ( 0.057)	Loss -0.7752 (-0.7752)
Epoch: [26][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.7835 (-0.7835)
Epoch: [27][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.7976 (-0.7976)
Epoch: [28][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.8110 (-0.8110)
Epoch: [29][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.7990 (-0.7990)
Epoch: [30][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.8159 (-0.8159)
Epoch: [31][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.8117 (-0.8117)
Epoch: [32][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.8242 (-0.8242)
Epoch: [33][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8238 (-0.8238)
Epoch: [34][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.8419 (-0.8419)
Epoch: [35][0/2]	Time  0.218 ( 0.218)	Data  0.062 ( 0.062)	Loss -0.8422 (-0.8422)
Epoch: [36][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8440 (-0.8440)
Epoch: [37][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.8369 (-0.8369)
Epoch: [38][0/2]	Time  0.216 ( 0.216)	Data  0.060 ( 0.060)	Loss -0.8468 (-0.8468)
Epoch: [39][0/2]	Time  0.192 ( 0.192)	Data  0.059 ( 0.059)	Loss -0.8535 (-0.8535)
Epoch: [40][0/2]	Time  0.211 ( 0.211)	Data  0.060 ( 0.060)	Loss -0.8448 (-0.8448)
Epoch: [41][0/2]	Time  0.208 ( 0.208)	Data  0.061 ( 0.061)	Loss -0.8557 (-0.8557)
Epoch: [42][0/2]	Time  0.207 ( 0.207)	Data  0.061 ( 0.061)	Loss -0.8444 (-0.8444)
Epoch: [43][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.8302 (-0.8302)
Epoch: [44][0/2]	Time  0.202 ( 0.202)	Data  0.061 ( 0.061)	Loss -0.8268 (-0.8268)
Epoch: [45][0/2]	Time  0.211 ( 0.211)	Data  0.062 ( 0.062)	Loss -0.8049 (-0.8049)
Epoch: [46][0/2]	Time  0.213 ( 0.213)	Data  0.059 ( 0.059)	Loss -0.7817 (-0.7817)
Epoch: [47][0/2]	Time  0.201 ( 0.201)	Data  0.062 ( 0.062)	Loss -0.7807 (-0.7807)
Epoch: [48][0/2]	Time  0.215 ( 0.215)	Data  0.059 ( 0.059)	Loss -0.7750 (-0.7750)
Epoch: [49][0/2]	Time  0.210 ( 0.210)	Data  0.061 ( 0.061)	Loss -0.7925 (-0.7925)
Epoch: [50][0/2]	Time  0.220 ( 0.220)	Data  0.077 ( 0.077)	Loss -0.8011 (-0.8011)
Epoch: [51][0/2]	Time  0.209 ( 0.209)	Data  0.064 ( 0.064)	Loss -0.8020 (-0.8020)
Epoch: [52][0/2]	Time  0.213 ( 0.213)	Data  0.059 ( 0.059)	Loss -0.8072 (-0.8072)
Epoch: [53][0/2]	Time  0.201 ( 0.201)	Data  0.060 ( 0.060)	Loss -0.8167 (-0.8167)
Epoch: [54][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.8248 (-0.8248)
Epoch: [55][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.8308 (-0.8308)
Epoch: [56][0/2]	Time  0.199 ( 0.199)	Data  0.062 ( 0.062)	Loss -0.8273 (-0.8273)
Epoch: [57][0/2]	Time  0.232 ( 0.232)	Data  0.078 ( 0.078)	Loss -0.8463 (-0.8463)
Epoch: [58][0/2]	Time  0.216 ( 0.216)	Data  0.060 ( 0.060)	Loss -0.8429 (-0.8429)
Epoch: [59][0/2]	Time  0.199 ( 0.199)	Data  0.062 ( 0.062)	Loss -0.8512 (-0.8512)
Epoch: [60][0/2]	Time  0.221 ( 0.221)	Data  0.071 ( 0.071)	Loss -0.8615 (-0.8615)
Epoch: [61][0/2]	Time  0.220 ( 0.220)	Data  0.069 ( 0.069)	Loss -0.8642 (-0.8642)
Epoch: [62][0/2]	Time  0.219 ( 0.219)	Data  0.063 ( 0.063)	Loss -0.8677 (-0.8677)
Epoch: [63][0/2]	Time  0.197 ( 0.197)	Data  0.060 ( 0.060)	Loss -0.8486 (-0.8486)
Epoch: [64][0/2]	Time  0.217 ( 0.217)	Data  0.060 ( 0.060)	Loss -0.8570 (-0.8570)
Epoch: [65][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.8590 (-0.8590)
Epoch: [66][0/2]	Time  0.199 ( 0.199)	Data  0.060 ( 0.060)	Loss -0.8591 (-0.8591)
Epoch: [67][0/2]	Time  0.223 ( 0.223)	Data  0.069 ( 0.069)	Loss -0.8535 (-0.8535)
Epoch: [68][0/2]	Time  0.215 ( 0.215)	Data  0.062 ( 0.062)	Loss -0.8645 (-0.8645)
Epoch: [69][0/2]	Time  0.229 ( 0.229)	Data  0.078 ( 0.078)	Loss -0.8603 (-0.8603)
Epoch: [70][0/2]	Time  0.184 ( 0.184)	Data  0.060 ( 0.060)	Loss -0.8629 (-0.8629)
Epoch: [71][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.8618 (-0.8618)
Epoch: [72][0/2]	Time  0.200 ( 0.200)	Data  0.060 ( 0.060)	Loss -0.8594 (-0.8594)
Epoch: [73][0/2]	Time  0.212 ( 0.212)	Data  0.060 ( 0.060)	Loss -0.8550 (-0.8550)
Epoch: [74][0/2]	Time  0.220 ( 0.220)	Data  0.066 ( 0.066)	Loss -0.8603 (-0.8603)
Epoch: [75][0/2]	Time  0.205 ( 0.205)	Data  0.061 ( 0.061)	Loss -0.8704 (-0.8704)
Epoch: [76][0/2]	Time  0.211 ( 0.211)	Data  0.059 ( 0.059)	Loss -0.8739 (-0.8739)
Epoch: [77][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.8732 (-0.8732)
Epoch: [78][0/2]	Time  0.196 ( 0.196)	Data  0.060 ( 0.060)	Loss -0.8682 (-0.8682)
Epoch: [79][0/2]	Time  0.213 ( 0.213)	Data  0.059 ( 0.059)	Loss -0.8727 (-0.8727)
Epoch: [80][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.8755 (-0.8755)
Epoch: [81][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.8791 (-0.8791)
Epoch: [82][0/2]	Time  0.216 ( 0.216)	Data  0.063 ( 0.063)	Loss -0.8817 (-0.8817)
Epoch: [83][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.8904 (-0.8904)
Epoch: [84][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.8893 (-0.8893)
Epoch: [85][0/2]	Time  0.215 ( 0.215)	Data  0.058 ( 0.058)	Loss -0.8865 (-0.8865)
Epoch: [86][0/2]	Time  0.198 ( 0.198)	Data  0.062 ( 0.062)	Loss -0.9007 (-0.9007)
Epoch: [87][0/2]	Time  0.208 ( 0.208)	Data  0.060 ( 0.060)	Loss -0.9031 (-0.9031)
Epoch: [88][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9013 (-0.9013)
Epoch: [89][0/2]	Time  0.166 ( 0.166)	Data  0.058 ( 0.058)	Loss -0.8995 (-0.8995)
Epoch: [90][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.8927 (-0.8927)
Epoch: [91][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.9020 (-0.9020)
Epoch: [92][0/2]	Time  0.169 ( 0.169)	Data  0.059 ( 0.059)	Loss -0.8924 (-0.8924)
Epoch: [93][0/2]	Time  0.199 ( 0.199)	Data  0.084 ( 0.084)	Loss -0.9023 (-0.9023)
Epoch: [94][0/2]	Time  0.180 ( 0.180)	Data  0.072 ( 0.072)	Loss -0.8857 (-0.8857)
Epoch: [95][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9016 (-0.9016)
Epoch: [96][0/2]	Time  0.164 ( 0.164)	Data  0.057 ( 0.057)	Loss -0.8964 (-0.8964)
Epoch: [97][0/2]	Time  0.161 ( 0.161)	Data  0.056 ( 0.056)	Loss -0.8895 (-0.8895)
Epoch: [98][0/2]	Time  0.158 ( 0.158)	Data  0.057 ( 0.057)	Loss -0.8995 (-0.8995)
Epoch: [99][0/2]	Time  0.171 ( 0.171)	Data  0.060 ( 0.060)	Loss -0.8932 (-0.8932)
Epoch: [100][0/2]	Time  0.195 ( 0.195)	Data  0.066 ( 0.066)	Loss -0.8996 (-0.8996)
Epoch: [101][0/2]	Time  0.197 ( 0.197)	Data  0.075 ( 0.075)	Loss -0.9011 (-0.9011)
Epoch: [102][0/2]	Time  0.166 ( 0.166)	Data  0.057 ( 0.057)	Loss -0.9032 (-0.9032)
Epoch: [103][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.8948 (-0.8948)
Epoch: [104][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.8992 (-0.8992)
Epoch: [105][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.8917 (-0.8917)
Epoch: [106][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.8970 (-0.8970)
Epoch: [107][0/2]	Time  0.150 ( 0.150)	Data  0.058 ( 0.058)	Loss -0.8991 (-0.8991)
Epoch: [108][0/2]	Time  0.157 ( 0.157)	Data  0.057 ( 0.057)	Loss -0.9048 (-0.9048)
Epoch: [109][0/2]	Time  0.160 ( 0.160)	Data  0.057 ( 0.057)	Loss -0.9016 (-0.9016)
Epoch: [110][0/2]	Time  0.154 ( 0.154)	Data  0.056 ( 0.056)	Loss -0.8965 (-0.8965)
Epoch: [111][0/2]	Time  0.158 ( 0.158)	Data  0.057 ( 0.057)	Loss -0.9080 (-0.9080)
Epoch: [112][0/2]	Time  0.162 ( 0.162)	Data  0.057 ( 0.057)	Loss -0.9032 (-0.9032)
Epoch: [113][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.9051 (-0.9051)
Epoch: [114][0/2]	Time  0.166 ( 0.166)	Data  0.061 ( 0.061)	Loss -0.9053 (-0.9053)
Epoch: [115][0/2]	Time  0.155 ( 0.155)	Data  0.058 ( 0.058)	Loss -0.9116 (-0.9116)
Epoch: [116][0/2]	Time  0.164 ( 0.164)	Data  0.059 ( 0.059)	Loss -0.8900 (-0.8900)
Epoch: [117][0/2]	Time  0.163 ( 0.163)	Data  0.058 ( 0.058)	Loss -0.9068 (-0.9068)
Epoch: [118][0/2]	Time  0.160 ( 0.160)	Data  0.057 ( 0.057)	Loss -0.9123 (-0.9123)
Epoch: [119][0/2]	Time  0.160 ( 0.160)	Data  0.057 ( 0.057)	Loss -0.9071 (-0.9071)
Epoch: [120][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9027 (-0.9027)
Epoch: [121][0/2]	Time  0.162 ( 0.162)	Data  0.057 ( 0.057)	Loss -0.9085 (-0.9085)
Epoch: [122][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9115 (-0.9115)
Epoch: [123][0/2]	Time  0.158 ( 0.158)	Data  0.062 ( 0.062)	Loss -0.9101 (-0.9101)
Epoch: [124][0/2]	Time  0.164 ( 0.164)	Data  0.058 ( 0.058)	Loss -0.9073 (-0.9073)
Epoch: [125][0/2]	Time  0.157 ( 0.157)	Data  0.057 ( 0.057)	Loss -0.9036 (-0.9036)
Epoch: [126][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.9068 (-0.9068)
Epoch: [127][0/2]	Time  0.169 ( 0.169)	Data  0.057 ( 0.057)	Loss -0.9009 (-0.9009)
.\main_simsiam.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
Epoch: [128][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9139 (-0.9139)
Epoch: [129][0/2]	Time  0.164 ( 0.164)	Data  0.057 ( 0.057)	Loss -0.9069 (-0.9069)
Epoch: [130][0/2]	Time  0.158 ( 0.158)	Data  0.056 ( 0.056)	Loss -0.9109 (-0.9109)
Epoch: [131][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9075 (-0.9075)
Epoch: [132][0/2]	Time  0.147 ( 0.147)	Data  0.057 ( 0.057)	Loss -0.9030 (-0.9030)
Epoch: [133][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9037 (-0.9037)
Epoch: [134][0/2]	Time  0.152 ( 0.152)	Data  0.057 ( 0.057)	Loss -0.9074 (-0.9074)
Epoch: [135][0/2]	Time  0.163 ( 0.163)	Data  0.057 ( 0.057)	Loss -0.9113 (-0.9113)
Epoch: [136][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.9000 (-0.9000)
Epoch: [137][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.9063 (-0.9063)
Epoch: [138][0/2]	Time  0.167 ( 0.167)	Data  0.058 ( 0.058)	Loss -0.9101 (-0.9101)
Epoch: [139][0/2]	Time  0.163 ( 0.163)	Data  0.057 ( 0.057)	Loss -0.9017 (-0.9017)
Epoch: [140][0/2]	Time  0.165 ( 0.165)	Data  0.059 ( 0.059)	Loss -0.9049 (-0.9049)
Epoch: [141][0/2]	Time  0.156 ( 0.156)	Data  0.057 ( 0.057)	Loss -0.9013 (-0.9013)
Epoch: [142][0/2]	Time  0.161 ( 0.161)	Data  0.057 ( 0.057)	Loss -0.9021 (-0.9021)
Epoch: [143][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.9098 (-0.9098)
Epoch: [144][0/2]	Time  0.222 ( 0.222)	Data  0.065 ( 0.065)	Loss -0.8885 (-0.8885)
Epoch: [145][0/2]	Time  0.236 ( 0.236)	Data  0.130 ( 0.130)	Loss -0.8924 (-0.8924)
Epoch: [146][0/2]	Time  0.158 ( 0.158)	Data  0.057 ( 0.057)	Loss -0.8909 (-0.8909)
Epoch: [147][0/2]	Time  0.167 ( 0.167)	Data  0.057 ( 0.057)	Loss -0.9022 (-0.9022)
Epoch: [148][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9075 (-0.9075)
Epoch: [149][0/2]	Time  0.183 ( 0.183)	Data  0.079 ( 0.079)	Loss -0.9011 (-0.9011)
Epoch: [150][0/2]	Time  0.164 ( 0.164)	Data  0.057 ( 0.057)	Loss -0.9010 (-0.9010)
Epoch: [151][0/2]	Time  0.165 ( 0.165)	Data  0.060 ( 0.060)	Loss -0.9092 (-0.9092)
Epoch: [152][0/2]	Time  0.215 ( 0.215)	Data  0.063 ( 0.063)	Loss -0.9009 (-0.9009)
Epoch: [153][0/2]	Time  0.183 ( 0.183)	Data  0.060 ( 0.060)	Loss -0.9107 (-0.9107)
Epoch: [154][0/2]	Time  0.162 ( 0.162)	Data  0.058 ( 0.058)	Loss -0.9132 (-0.9132)
Epoch: [155][0/2]	Time  0.233 ( 0.233)	Data  0.098 ( 0.098)	Loss -0.9194 (-0.9194)
Epoch: [156][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.9140 (-0.9140)
Epoch: [157][0/2]	Time  0.163 ( 0.163)	Data  0.058 ( 0.058)	Loss -0.9094 (-0.9094)
Epoch: [158][0/2]	Time  0.156 ( 0.156)	Data  0.057 ( 0.057)	Loss -0.9075 (-0.9075)
Epoch: [159][0/2]	Time  0.178 ( 0.178)	Data  0.059 ( 0.059)	Loss -0.9236 (-0.9236)
Epoch: [160][0/2]	Time  0.163 ( 0.163)	Data  0.057 ( 0.057)	Loss -0.9163 (-0.9163)
Epoch: [161][0/2]	Time  0.160 ( 0.160)	Data  0.060 ( 0.060)	Loss -0.9037 (-0.9037)
Epoch: [162][0/2]	Time  0.164 ( 0.164)	Data  0.058 ( 0.058)	Loss -0.9149 (-0.9149)
Epoch: [163][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9086 (-0.9086)
Epoch: [164][0/2]	Time  0.157 ( 0.157)	Data  0.057 ( 0.057)	Loss -0.9146 (-0.9146)
Epoch: [165][0/2]	Time  0.192 ( 0.192)	Data  0.062 ( 0.062)	Loss -0.9053 (-0.9053)
Epoch: [166][0/2]	Time  0.157 ( 0.157)	Data  0.058 ( 0.058)	Loss -0.9176 (-0.9176)
Epoch: [167][0/2]	Time  0.224 ( 0.224)	Data  0.069 ( 0.069)	Loss -0.9149 (-0.9149)
Epoch: [168][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9147 (-0.9147)
Epoch: [169][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9258 (-0.9258)
Epoch: [170][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9257 (-0.9257)
Epoch: [171][0/2]	Time  0.206 ( 0.206)	Data  0.061 ( 0.061)	Loss -0.9148 (-0.9148)
Epoch: [172][0/2]	Time  0.212 ( 0.212)	Data  0.063 ( 0.063)	Loss -0.9154 (-0.9154)
Epoch: [173][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9190 (-0.9190)
Epoch: [174][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9151 (-0.9151)
Epoch: [175][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9214 (-0.9214)
Epoch: [176][0/2]	Time  0.181 ( 0.181)	Data  0.060 ( 0.060)	Loss -0.9222 (-0.9222)
Epoch: [177][0/2]	Time  0.215 ( 0.215)	Data  0.058 ( 0.058)	Loss -0.9266 (-0.9266)
Epoch: [178][0/2]	Time  0.174 ( 0.174)	Data  0.057 ( 0.057)	Loss -0.9192 (-0.9192)
Epoch: [179][0/2]	Time  0.217 ( 0.217)	Data  0.059 ( 0.059)	Loss -0.9153 (-0.9153)
Epoch: [180][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9168 (-0.9168)
Epoch: [181][0/2]	Time  0.208 ( 0.208)	Data  0.059 ( 0.059)	Loss -0.9193 (-0.9193)
Epoch: [182][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.9125 (-0.9125)
Epoch: [183][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9236 (-0.9236)
Epoch: [184][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9207 (-0.9207)
Epoch: [185][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9158 (-0.9158)
Epoch: [186][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9215 (-0.9215)
Epoch: [187][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9250 (-0.9250)
Epoch: [188][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9105 (-0.9105)
Epoch: [189][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9193 (-0.9193)
Epoch: [190][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.9177 (-0.9177)
Epoch: [191][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9162 (-0.9162)
Epoch: [192][0/2]	Time  0.215 ( 0.215)	Data  0.063 ( 0.063)	Loss -0.9043 (-0.9043)
Epoch: [193][0/2]	Time  0.205 ( 0.205)	Data  0.057 ( 0.057)	Loss -0.9205 (-0.9205)
Epoch: [194][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9288 (-0.9288)
Epoch: [195][0/2]	Time  0.213 ( 0.213)	Data  0.063 ( 0.063)	Loss -0.9140 (-0.9140)
Epoch: [196][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9187 (-0.9187)
Epoch: [197][0/2]	Time  0.208 ( 0.208)	Data  0.059 ( 0.059)	Loss -0.9189 (-0.9189)
Epoch: [198][0/2]	Time  0.213 ( 0.213)	Data  0.060 ( 0.060)	Loss -0.9271 (-0.9271)
Epoch: [199][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9189 (-0.9189)
Epoch: [200][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9126 (-0.9126)
Epoch: [201][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9110 (-0.9110)
Epoch: [202][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9180 (-0.9180)
Epoch: [203][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9202 (-0.9202)
Epoch: [204][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9241 (-0.9241)
Epoch: [205][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9160 (-0.9160)
Epoch: [206][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.9162 (-0.9162)
Epoch: [207][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9241 (-0.9241)
Epoch: [208][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9226 (-0.9226)
Epoch: [209][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9191 (-0.9191)
Epoch: [210][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.9249 (-0.9249)
Epoch: [211][0/2]	Time  0.214 ( 0.214)	Data  0.064 ( 0.064)	Loss -0.9186 (-0.9186)
Epoch: [212][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9229 (-0.9229)
Epoch: [213][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9177 (-0.9177)
Epoch: [214][0/2]	Time  0.209 ( 0.209)	Data  0.060 ( 0.060)	Loss -0.9204 (-0.9204)
Epoch: [215][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9159 (-0.9159)
Epoch: [216][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9231 (-0.9231)
Epoch: [217][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9127 (-0.9127)
Epoch: [218][0/2]	Time  0.199 ( 0.199)	Data  0.061 ( 0.061)	Loss -0.9096 (-0.9096)
Epoch: [219][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9221 (-0.9221)
Epoch: [220][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9182 (-0.9182)
Epoch: [221][0/2]	Time  0.202 ( 0.202)	Data  0.060 ( 0.060)	Loss -0.9190 (-0.9190)
Epoch: [222][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9197 (-0.9197)
Epoch: [223][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9163 (-0.9163)
Epoch: [224][0/2]	Time  0.214 ( 0.214)	Data  0.060 ( 0.060)	Loss -0.9267 (-0.9267)
Epoch: [225][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9219 (-0.9219)
Epoch: [226][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.9166 (-0.9166)
Epoch: [227][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9105 (-0.9105)
Epoch: [228][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9151 (-0.9151)
Epoch: [229][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.9151 (-0.9151)
Epoch: [230][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9156 (-0.9156)
Epoch: [231][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9224 (-0.9224)
Epoch: [232][0/2]	Time  0.216 ( 0.216)	Data  0.058 ( 0.058)	Loss -0.9196 (-0.9196)
Epoch: [233][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9269 (-0.9269)
Epoch: [234][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9182 (-0.9182)
Epoch: [235][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9216 (-0.9216)
Epoch: [236][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9123 (-0.9123)
Epoch: [237][0/2]	Time  0.206 ( 0.206)	Data  0.062 ( 0.062)	Loss -0.9159 (-0.9159)
Epoch: [238][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9217 (-0.9217)
Epoch: [239][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9223 (-0.9223)
Epoch: [240][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9198 (-0.9198)
Epoch: [241][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9286 (-0.9286)
Epoch: [242][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.9184 (-0.9184)
Epoch: [243][0/2]	Time  0.215 ( 0.215)	Data  0.058 ( 0.058)	Loss -0.9244 (-0.9244)
Epoch: [244][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9202 (-0.9202)
Epoch: [245][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9213 (-0.9213)
Epoch: [246][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9230 (-0.9230)
Epoch: [247][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.9182 (-0.9182)
Epoch: [248][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9254 (-0.9254)
Epoch: [249][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9124 (-0.9124)
Epoch: [250][0/2]	Time  0.208 ( 0.208)	Data  0.060 ( 0.060)	Loss -0.9172 (-0.9172)
Epoch: [251][0/2]	Time  0.218 ( 0.218)	Data  0.060 ( 0.060)	Loss -0.9179 (-0.9179)
Epoch: [252][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9090 (-0.9090)
Epoch: [253][0/2]	Time  0.211 ( 0.211)	Data  0.057 ( 0.057)	Loss -0.9250 (-0.9250)
Epoch: [254][0/2]	Time  0.217 ( 0.217)	Data  0.057 ( 0.057)	Loss -0.9227 (-0.9227)
Epoch: [255][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9090 (-0.9090)
Epoch: [256][0/2]	Time  0.215 ( 0.215)	Data  0.063 ( 0.063)	Loss -0.9209 (-0.9209)
Epoch: [257][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9178 (-0.9178)
Epoch: [258][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.9172 (-0.9172)
Epoch: [259][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9091 (-0.9091)
Epoch: [260][0/2]	Time  0.191 ( 0.191)	Data  0.058 ( 0.058)	Loss -0.9203 (-0.9203)
Epoch: [261][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.9144 (-0.9144)
Epoch: [262][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.9277 (-0.9277)
Epoch: [263][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9140 (-0.9140)
Epoch: [264][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9161 (-0.9161)
Epoch: [265][0/2]	Time  0.192 ( 0.192)	Data  0.057 ( 0.057)	Loss -0.9089 (-0.9089)
Epoch: [266][0/2]	Time  0.213 ( 0.213)	Data  0.059 ( 0.059)	Loss -0.9160 (-0.9160)
Epoch: [267][0/2]	Time  0.220 ( 0.220)	Data  0.065 ( 0.065)	Loss -0.9075 (-0.9075)
Epoch: [268][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9066 (-0.9066)
Epoch: [269][0/2]	Time  0.208 ( 0.208)	Data  0.060 ( 0.060)	Loss -0.9187 (-0.9187)
Epoch: [270][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.9246 (-0.9246)
Epoch: [271][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9138 (-0.9138)
Epoch: [272][0/2]	Time  0.205 ( 0.205)	Data  0.060 ( 0.060)	Loss -0.9220 (-0.9220)
Epoch: [273][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9076 (-0.9076)
Epoch: [274][0/2]	Time  0.180 ( 0.180)	Data  0.057 ( 0.057)	Loss -0.9173 (-0.9173)
Epoch: [275][0/2]	Time  0.215 ( 0.215)	Data  0.060 ( 0.060)	Loss -0.9124 (-0.9124)
Epoch: [276][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9245 (-0.9245)
Epoch: [277][0/2]	Time  0.208 ( 0.208)	Data  0.060 ( 0.060)	Loss -0.9114 (-0.9114)
Epoch: [278][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9127 (-0.9127)
Epoch: [279][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9175 (-0.9175)
Epoch: [280][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9184 (-0.9184)
Epoch: [281][0/2]	Time  0.183 ( 0.183)	Data  0.061 ( 0.061)	Loss -0.9044 (-0.9044)
Epoch: [282][0/2]	Time  0.212 ( 0.212)	Data  0.058 ( 0.058)	Loss -0.9180 (-0.9180)
Epoch: [283][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9110 (-0.9110)
Epoch: [284][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9140 (-0.9140)
Epoch: [285][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9218 (-0.9218)
Epoch: [286][0/2]	Time  0.174 ( 0.174)	Data  0.059 ( 0.059)	Loss -0.9192 (-0.9192)
Epoch: [287][0/2]	Time  0.213 ( 0.213)	Data  0.060 ( 0.060)	Loss -0.9039 (-0.9039)
Epoch: [288][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9233 (-0.9233)
Epoch: [289][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.9163 (-0.9163)
Epoch: [290][0/2]	Time  0.215 ( 0.215)	Data  0.062 ( 0.062)	Loss -0.9188 (-0.9188)
Epoch: [291][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.9269 (-0.9269)
Epoch: [292][0/2]	Time  0.208 ( 0.208)	Data  0.059 ( 0.059)	Loss -0.9278 (-0.9278)
Epoch: [293][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9161 (-0.9161)
Epoch: [294][0/2]	Time  0.166 ( 0.166)	Data  0.059 ( 0.059)	Loss -0.9167 (-0.9167)
Epoch: [295][0/2]	Time  0.173 ( 0.173)	Data  0.057 ( 0.057)	Loss -0.9253 (-0.9253)
Epoch: [296][0/2]	Time  0.156 ( 0.156)	Data  0.057 ( 0.057)	Loss -0.9158 (-0.9158)
Epoch: [297][0/2]	Time  0.160 ( 0.160)	Data  0.056 ( 0.056)	Loss -0.9186 (-0.9186)
Epoch: [298][0/2]	Time  0.160 ( 0.160)	Data  0.058 ( 0.058)	Loss -0.9152 (-0.9152)
Epoch: [299][0/2]	Time  0.156 ( 0.156)	Data  0.056 ( 0.056)	Loss -0.9272 (-0.9272)
simsiam program ends here, while ssv:200, normal:200, excep_size:100
Use GPU: 0 for training
=> creating model 'resnet50'
=> loading checkpoint 'checkpoints/checkpoint_0099.pth.tar'
=> loaded pre-trained model 'checkpoints/checkpoint_0099.pth.tar'
Epoch: [0][0/1]	Time  0.671 ( 0.671)	Data  0.059 ( 0.059)	Loss 1.7776e+00 (1.7776e+00)	Acc@1  16.86 ( 16.86)	Acc@5  93.86 ( 93.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.169 ( 0.169)	Loss 5.0927e-02 (5.0927e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.059 ( 0.046)	Loss 3.4292e+00 (2.2887e+00)	Acc@1   0.00 ( 34.23)	Acc@5   0.00 ( 83.33)
 * Acc@1 34.229 Acc@5 83.333
=> loading 'checkpoints/checkpoint_0099.pth.tar' for sanity check
=> sanity check passed.
Epoch: [1][0/1]	Time  0.097 ( 0.097)	Data  0.051 ( 0.051)	Loss 1.9675e+00 (1.9675e+00)	Acc@1  39.57 ( 39.57)	Acc@5  86.29 ( 86.29)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.033 ( 0.033)	Loss 4.1931e+00 (4.1931e+00)	Acc@1   0.00 (  0.00)	Acc@5   2.34 (  2.34)
Test: [10/11]	Time  0.004 ( 0.029)	Loss 1.4694e+00 (3.6940e+00)	Acc@1  12.50 ( 16.90)	Acc@5 100.00 ( 83.57)
 * Acc@1 16.900 Acc@5 83.567
Epoch: [2][0/1]	Time  0.084 ( 0.084)	Data  0.050 ( 0.050)	Loss 3.4922e+00 (3.4922e+00)	Acc@1  13.86 ( 13.86)	Acc@5  71.57 ( 71.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 1.9254e+00 (1.9254e+00)	Acc@1   0.78 (  0.78)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.027)	Loss 4.3102e+00 (8.2011e+00)	Acc@1   0.00 ( 29.17)	Acc@5 100.00 ( 88.24)
 * Acc@1 29.167 Acc@5 88.240
Epoch: [3][0/1]	Time  0.083 ( 0.083)	Data  0.049 ( 0.049)	Loss 6.4978e+00 (6.4978e+00)	Acc@1  18.71 ( 18.71)	Acc@5  89.14 ( 89.14)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.032 ( 0.032)	Loss 6.8404e-04 (6.8404e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.029)	Loss 3.4471e+00 (1.5754e+01)	Acc@1   0.00 ( 29.71)	Acc@5 100.00 ( 83.33)
 * Acc@1 29.712 Acc@5 83.333
Epoch: [4][0/1]	Time  0.080 ( 0.080)	Data  0.045 ( 0.045)	Loss 1.2297e+01 (1.2297e+01)	Acc@1  40.29 ( 40.29)	Acc@5  85.71 ( 85.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.029 ( 0.029)	Loss 1.2323e-04 (1.2323e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.029)	Loss 1.2601e-02 (2.0737e+01)	Acc@1 100.00 ( 33.26)	Acc@5 100.00 ( 83.33)
 * Acc@1 33.255 Acc@5 83.333
Epoch: [5][0/1]	Time  0.079 ( 0.079)	Data  0.044 ( 0.044)	Loss 1.6258e+01 (1.6258e+01)	Acc@1  41.86 ( 41.86)	Acc@5  85.86 ( 85.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.031 ( 0.031)	Loss 2.5131e-05 (2.5131e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.028)	Loss 1.3987e+00 (1.9024e+01)	Acc@1  25.00 ( 29.98)	Acc@5 100.00 ( 87.03)
 * Acc@1 29.984 Acc@5 87.033
Epoch: [6][0/1]	Time  0.086 ( 0.086)	Data  0.047 ( 0.047)	Loss 1.2772e+01 (1.2772e+01)	Acc@1  39.14 ( 39.14)	Acc@5  87.71 ( 87.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 1.3958e-05 (1.3958e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.030)	Loss 3.4244e+00 (1.4076e+01)	Acc@1   0.00 ( 40.81)	Acc@5 100.00 ( 94.12)
 * Acc@1 40.810 Acc@5 94.120
Epoch: [7][0/1]	Time  0.084 ( 0.084)	Data  0.051 ( 0.051)	Loss 1.0032e+01 (1.0032e+01)	Acc@1  44.43 ( 44.43)	Acc@5  93.00 ( 93.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 2.9617e-02 (2.9617e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 4.8571e-01 (3.3551e+00)	Acc@1  87.50 ( 53.50)	Acc@5 100.00 ( 96.73)
 * Acc@1 53.505 Acc@5 96.729
Epoch: [8][0/1]	Time  0.107 ( 0.107)	Data  0.046 ( 0.046)	Loss 2.9390e+00 (2.9390e+00)	Acc@1  54.00 ( 54.00)	Acc@5  96.57 ( 96.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.046 ( 0.046)	Loss 7.0003e+00 (7.0003e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.040)	Loss 2.0076e+01 (1.6324e+01)	Acc@1   0.00 ( 18.07)	Acc@5 100.00 (100.00)
 * Acc@1 18.069 Acc@5 100.000
Epoch: [9][0/1]	Time  0.106 ( 0.106)	Data  0.045 ( 0.045)	Loss 1.4854e+01 (1.4854e+01)	Acc@1  14.71 ( 14.71)	Acc@5  99.43 ( 99.43)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 8.7570e+00 (8.7570e+00)	Acc@1   0.00 (  0.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.041)	Loss 2.0894e+01 (1.5169e+01)	Acc@1   0.00 ( 21.11)	Acc@5 100.00 (100.00)
 * Acc@1 21.106 Acc@5 100.000
Epoch: [10][0/1]	Time  0.123 ( 0.123)	Data  0.054 ( 0.054)	Loss 1.6445e+01 (1.6445e+01)	Acc@1  15.71 ( 15.71)	Acc@5  99.00 ( 99.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.8021e-01 (4.8021e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.042)	Loss 5.6822e+00 (8.4205e+00)	Acc@1   0.00 ( 44.74)	Acc@5 100.00 ( 97.66)
 * Acc@1 44.743 Acc@5 97.664
Epoch: [11][0/1]	Time  0.136 ( 0.136)	Data  0.054 ( 0.054)	Loss 7.9432e+00 (7.9432e+00)	Acc@1  47.29 ( 47.29)	Acc@5  97.86 ( 97.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 9.3738e-06 (9.3738e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.040)	Loss 5.4856e-01 (7.6714e+00)	Acc@1  87.50 ( 44.74)	Acc@5 100.00 ( 95.29)
 * Acc@1 44.743 Acc@5 95.288
Epoch: [12][0/1]	Time  0.162 ( 0.162)	Data  0.065 ( 0.065)	Loss 7.4541e+00 (7.4541e+00)	Acc@1  47.43 ( 47.43)	Acc@5  92.86 ( 92.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.6322e-08 (3.6322e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 2.2368e+00 (1.6797e+01)	Acc@1  75.00 ( 42.52)	Acc@5 100.00 ( 93.30)
 * Acc@1 42.523 Acc@5 93.302
Epoch: [13][0/1]	Time  0.141 ( 0.141)	Data  0.052 ( 0.052)	Loss 1.3564e+01 (1.3564e+01)	Acc@1  47.86 ( 47.86)	Acc@5  91.57 ( 91.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 5.7741e-07 (5.7741e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 1.4222e-01 (1.3082e+01)	Acc@1  87.50 ( 46.42)	Acc@5 100.00 ( 92.91)
 * Acc@1 46.417 Acc@5 92.913
Epoch: [14][0/1]	Time  0.143 ( 0.143)	Data  0.053 ( 0.053)	Loss 1.1766e+01 (1.1766e+01)	Acc@1  48.43 ( 48.43)	Acc@5  90.57 ( 90.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 2.3444e-04 (2.3444e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 2.9649e+00 (1.3235e+01)	Acc@1  50.00 ( 45.05)	Acc@5 100.00 ( 93.38)
 * Acc@1 45.055 Acc@5 93.380
Epoch: [15][0/1]	Time  0.145 ( 0.145)	Data  0.053 ( 0.053)	Loss 1.1810e+01 (1.1810e+01)	Acc@1  50.71 ( 50.71)	Acc@5  91.57 ( 91.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 2.6541e-03 (2.6541e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 2.7178e+00 (8.6570e+00)	Acc@1  50.00 ( 56.93)	Acc@5 100.00 ( 95.17)
 * Acc@1 56.931 Acc@5 95.171
Epoch: [16][0/1]	Time  0.146 ( 0.146)	Data  0.051 ( 0.051)	Loss 8.4762e+00 (8.4762e+00)	Acc@1  56.29 ( 56.29)	Acc@5  92.71 ( 92.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 2.8550e-04 (2.8550e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.050)	Loss 4.2506e-01 (5.8070e+00)	Acc@1  87.50 ( 59.11)	Acc@5 100.00 ( 95.95)
 * Acc@1 59.112 Acc@5 95.950
Epoch: [17][0/1]	Time  0.143 ( 0.143)	Data  0.053 ( 0.053)	Loss 6.7276e+00 (6.7276e+00)	Acc@1  56.14 ( 56.14)	Acc@5  93.71 ( 93.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.6392e-05 (3.6392e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.051)	Loss 1.0404e+00 (6.4074e+00)	Acc@1  75.00 ( 51.32)	Acc@5 100.00 ( 96.22)
 * Acc@1 51.324 Acc@5 96.223
Epoch: [18][0/1]	Time  0.144 ( 0.144)	Data  0.053 ( 0.053)	Loss 7.3630e+00 (7.3630e+00)	Acc@1  52.29 ( 52.29)	Acc@5  94.71 ( 94.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.5066e-04 (2.5066e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 5.2664e+00 (7.8865e+00)	Acc@1   0.00 ( 42.68)	Acc@5 100.00 ( 97.43)
 * Acc@1 42.679 Acc@5 97.430
Epoch: [19][0/1]	Time  0.143 ( 0.143)	Data  0.050 ( 0.050)	Loss 7.8454e+00 (7.8454e+00)	Acc@1  48.14 ( 48.14)	Acc@5  96.43 ( 96.43)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.9194e-01 (1.9194e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.052)	Loss 6.8308e+00 (6.9063e+00)	Acc@1   0.00 ( 48.21)	Acc@5 100.00 ( 99.07)
 * Acc@1 48.209 Acc@5 99.065
Epoch: [20][0/1]	Time  0.153 ( 0.153)	Data  0.059 ( 0.059)	Loss 6.5260e+00 (6.5260e+00)	Acc@1  50.00 ( 50.00)	Acc@5  99.14 ( 99.14)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 2.9943e+00 (2.9943e+00)	Acc@1  26.56 ( 26.56)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.055)	Loss 3.2447e+00 (4.9555e+00)	Acc@1  37.50 ( 45.87)	Acc@5 100.00 ( 99.96)
 * Acc@1 45.872 Acc@5 99.961
Epoch: [21][0/1]	Time  0.197 ( 0.197)	Data  0.065 ( 0.065)	Loss 5.6725e+00 (5.6725e+00)	Acc@1  39.14 ( 39.14)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.073 ( 0.073)	Loss 2.1635e+00 (2.1635e+00)	Acc@1  37.11 ( 37.11)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.058)	Loss 4.5473e-01 (4.4091e+00)	Acc@1  87.50 ( 50.55)	Acc@5 100.00 ( 99.96)
 * Acc@1 50.545 Acc@5 99.961
Epoch: [22][0/1]	Time  0.167 ( 0.167)	Data  0.056 ( 0.056)	Loss 5.4277e+00 (5.4277e+00)	Acc@1  44.14 ( 44.14)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.067 ( 0.067)	Loss 1.8626e-01 (1.8626e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.012 ( 0.058)	Loss 2.5185e-01 (3.6092e+00)	Acc@1 100.00 ( 61.02)	Acc@5 100.00 ( 99.96)
 * Acc@1 61.020 Acc@5 99.961
Epoch: [23][0/1]	Time  0.200 ( 0.200)	Data  0.062 ( 0.062)	Loss 3.5966e+00 (3.5966e+00)	Acc@1  62.14 ( 62.14)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.063 ( 0.063)	Loss 6.9662e-02 (6.9662e-02)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.062)	Loss 4.5742e+00 (5.8416e+00)	Acc@1  12.50 ( 47.08)	Acc@5 100.00 (100.00)
 * Acc@1 47.079 Acc@5 100.000
Epoch: [24][0/1]	Time  0.180 ( 0.180)	Data  0.067 ( 0.067)	Loss 4.7856e+00 (4.7856e+00)	Acc@1  49.71 ( 49.71)	Acc@5  99.00 ( 99.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.063 ( 0.063)	Loss 4.1372e-03 (4.1372e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.056)	Loss 4.7374e+00 (6.0547e+00)	Acc@1   0.00 ( 47.51)	Acc@5 100.00 (100.00)
 * Acc@1 47.508 Acc@5 100.000
Epoch: [25][0/1]	Time  0.132 ( 0.132)	Data  0.053 ( 0.053)	Loss 4.8192e+00 (4.8192e+00)	Acc@1  52.29 ( 52.29)	Acc@5  98.57 ( 98.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.0104e-04 (3.0104e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.043)	Loss 2.8118e+00 (5.2817e+00)	Acc@1  12.50 ( 42.83)	Acc@5 100.00 ( 99.69)
 * Acc@1 42.835 Acc@5 99.688
Epoch: [26][0/1]	Time  0.127 ( 0.127)	Data  0.053 ( 0.053)	Loss 4.3852e+00 (4.3852e+00)	Acc@1  54.43 ( 54.43)	Acc@5  99.43 ( 99.43)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 1.3776e-05 (1.3776e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.043)	Loss 2.9373e+00 (5.7718e+00)	Acc@1  12.50 ( 46.30)	Acc@5 100.00 ( 99.07)
 * Acc@1 46.301 Acc@5 99.065
Epoch: [27][0/1]	Time  0.120 ( 0.120)	Data  0.046 ( 0.046)	Loss 4.8381e+00 (4.8381e+00)	Acc@1  55.00 ( 55.00)	Acc@5  99.43 ( 99.43)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 6.0904e-07 (6.0904e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 6.3844e-01 (4.0119e+00)	Acc@1  62.50 ( 55.06)	Acc@5 100.00 ( 98.91)
 * Acc@1 55.062 Acc@5 98.910
Epoch: [28][0/1]	Time  0.153 ( 0.153)	Data  0.053 ( 0.053)	Loss 3.5201e+00 (3.5201e+00)	Acc@1  57.14 ( 57.14)	Acc@5  99.71 ( 99.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 5.5879e-09 (5.5879e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.051)	Loss 1.7412e+00 (3.8221e+00)	Acc@1  75.00 ( 51.99)	Acc@5 100.00 ( 98.33)
 * Acc@1 51.986 Acc@5 98.326
Epoch: [29][0/1]	Time  0.151 ( 0.151)	Data  0.053 ( 0.053)	Loss 3.3486e+00 (3.3486e+00)	Acc@1  55.29 ( 55.29)	Acc@5  99.29 ( 99.29)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.059 ( 0.059)	Loss 1.3830e-07 (1.3830e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 1.5641e+00 (4.2022e+00)	Acc@1  75.00 ( 47.90)	Acc@5 100.00 ( 98.68)
 * Acc@1 47.897 Acc@5 98.676
Epoch: [30][0/1]	Time  0.149 ( 0.149)	Data  0.056 ( 0.056)	Loss 3.4942e+00 (3.4942e+00)	Acc@1  53.29 ( 53.29)	Acc@5  98.57 ( 98.57)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.058 ( 0.058)	Loss 2.2733e-06 (2.2733e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 6.3587e-01 (3.9178e+00)	Acc@1  87.50 ( 45.79)	Acc@5 100.00 ( 99.69)
 * Acc@1 45.794 Acc@5 99.688
Epoch: [31][0/1]	Time  0.145 ( 0.145)	Data  0.055 ( 0.055)	Loss 2.9305e+00 (2.9305e+00)	Acc@1  54.57 ( 54.57)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 7.4321e-04 (7.4321e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.051)	Loss 1.6155e+00 (3.6676e+00)	Acc@1  50.00 ( 52.92)	Acc@5 100.00 ( 99.57)
 * Acc@1 52.921 Acc@5 99.572
Epoch: [32][0/1]	Time  0.148 ( 0.148)	Data  0.056 ( 0.056)	Loss 2.8704e+00 (2.8704e+00)	Acc@1  60.86 ( 60.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 4.0547e-02 (4.0547e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 1.4849e+00 (3.3461e+00)	Acc@1  25.00 ( 56.66)	Acc@5 100.00 ( 99.65)
 * Acc@1 56.659 Acc@5 99.650
Epoch: [33][0/1]	Time  0.144 ( 0.144)	Data  0.053 ( 0.053)	Loss 2.7677e+00 (2.7677e+00)	Acc@1  61.14 ( 61.14)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 2.7506e-01 (2.7506e-01)	Acc@1  93.36 ( 93.36)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.051)	Loss 7.3361e-01 (3.0557e+00)	Acc@1  75.00 ( 60.01)	Acc@5 100.00 ( 99.84)
 * Acc@1 60.008 Acc@5 99.844
Epoch: [34][0/1]	Time  0.141 ( 0.141)	Data  0.054 ( 0.054)	Loss 2.7920e+00 (2.7920e+00)	Acc@1  63.29 ( 63.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 6.2896e-01 (6.2896e-01)	Acc@1  83.20 ( 83.20)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.051)	Loss 1.2668e-02 (2.9606e+00)	Acc@1 100.00 ( 54.13)	Acc@5 100.00 (100.00)
 * Acc@1 54.128 Acc@5 100.000
Epoch: [35][0/1]	Time  0.140 ( 0.140)	Data  0.054 ( 0.054)	Loss 2.7919e+00 (2.7919e+00)	Acc@1  57.29 ( 57.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 8.2919e-01 (8.2919e-01)	Acc@1  76.17 ( 76.17)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.054)	Loss 6.8765e-01 (2.1445e+00)	Acc@1  62.50 ( 56.89)	Acc@5 100.00 (100.00)
 * Acc@1 56.893 Acc@5 100.000
Epoch: [36][0/1]	Time  0.151 ( 0.151)	Data  0.060 ( 0.060)	Loss 2.1650e+00 (2.1650e+00)	Acc@1  57.57 ( 57.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 6.4132e-01 (6.4132e-01)	Acc@1  82.42 ( 82.42)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.051)	Loss 2.1223e+00 (2.5402e+00)	Acc@1  25.00 ( 45.95)	Acc@5 100.00 (100.00)
 * Acc@1 45.950 Acc@5 100.000
Epoch: [37][0/1]	Time  0.150 ( 0.150)	Data  0.053 ( 0.053)	Loss 2.2493e+00 (2.2493e+00)	Acc@1  54.86 ( 54.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 3.3329e-01 (3.3329e-01)	Acc@1  92.58 ( 92.58)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 2.3858e+00 (2.2746e+00)	Acc@1  25.00 ( 51.21)	Acc@5 100.00 (100.00)
 * Acc@1 51.207 Acc@5 100.000
Epoch: [38][0/1]	Time  0.150 ( 0.150)	Data  0.056 ( 0.056)	Loss 1.8980e+00 (1.8980e+00)	Acc@1  58.00 ( 58.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.3086e-01 (1.3086e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 6.2860e-01 (1.8738e+00)	Acc@1  75.00 ( 62.07)	Acc@5 100.00 (100.00)
 * Acc@1 62.072 Acc@5 100.000
Epoch: [39][0/1]	Time  0.149 ( 0.149)	Data  0.055 ( 0.055)	Loss 1.7157e+00 (1.7157e+00)	Acc@1  66.29 ( 66.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 5.7771e-02 (5.7771e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.051)	Loss 2.3988e-02 (1.8075e+00)	Acc@1 100.00 ( 61.21)	Acc@5 100.00 (100.00)
 * Acc@1 61.215 Acc@5 100.000
Epoch: [40][0/1]	Time  0.148 ( 0.148)	Data  0.052 ( 0.052)	Loss 1.8074e+00 (1.8074e+00)	Acc@1  63.29 ( 63.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.3838e-02 (1.3838e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 7.0689e-02 (1.4984e+00)	Acc@1 100.00 ( 64.14)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.136 Acc@5 99.961
Epoch: [41][0/1]	Time  0.147 ( 0.147)	Data  0.055 ( 0.055)	Loss 1.8062e+00 (1.8062e+00)	Acc@1  65.00 ( 65.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.5503e-02 (2.5503e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 1.1566e+00 (1.2447e+00)	Acc@1  62.50 ( 62.15)	Acc@5 100.00 (100.00)
 * Acc@1 62.150 Acc@5 100.000
Epoch: [42][0/1]	Time  0.149 ( 0.149)	Data  0.056 ( 0.056)	Loss 1.3785e+00 (1.3785e+00)	Acc@1  63.86 ( 63.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 7.5748e-02 (7.5748e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.055)	Loss 3.3560e+00 (1.6951e+00)	Acc@1  12.50 ( 57.67)	Acc@5 100.00 (100.00)
 * Acc@1 57.671 Acc@5 100.000
Epoch: [43][0/1]	Time  0.165 ( 0.165)	Data  0.071 ( 0.071)	Loss 1.6693e+00 (1.6693e+00)	Acc@1  60.00 ( 60.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.4744e-02 (1.4744e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.051)	Loss 2.7419e+00 (1.2741e+00)	Acc@1  12.50 ( 64.56)	Acc@5 100.00 (100.00)
 * Acc@1 64.564 Acc@5 100.000
Epoch: [44][0/1]	Time  0.158 ( 0.158)	Data  0.069 ( 0.069)	Loss 1.3938e+00 (1.3938e+00)	Acc@1  65.86 ( 65.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.5450e-02 (1.5450e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.053)	Loss 7.7654e-01 (1.2340e+00)	Acc@1  75.00 ( 65.97)	Acc@5 100.00 (100.00)
 * Acc@1 65.966 Acc@5 100.000
Epoch: [45][0/1]	Time  0.147 ( 0.147)	Data  0.054 ( 0.054)	Loss 1.3644e+00 (1.3644e+00)	Acc@1  67.00 ( 67.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.061 ( 0.061)	Loss 1.7935e-03 (1.7935e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 4.1976e-01 (1.1780e+00)	Acc@1  87.50 ( 66.59)	Acc@5 100.00 ( 99.84)
 * Acc@1 66.589 Acc@5 99.844
Epoch: [46][0/1]	Time  0.141 ( 0.141)	Data  0.053 ( 0.053)	Loss 1.3681e+00 (1.3681e+00)	Acc@1  66.00 ( 66.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 3.9511e-04 (3.9511e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.050)	Loss 8.0508e-01 (1.2927e+00)	Acc@1  75.00 ( 62.77)	Acc@5 100.00 ( 99.77)
 * Acc@1 62.773 Acc@5 99.766
Epoch: [47][0/1]	Time  0.148 ( 0.148)	Data  0.051 ( 0.051)	Loss 1.2554e+00 (1.2554e+00)	Acc@1  65.00 ( 65.00)	Acc@5  99.71 ( 99.71)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.059 ( 0.059)	Loss 2.9447e-04 (2.9447e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.051)	Loss 1.7373e+00 (1.3600e+00)	Acc@1  25.00 ( 57.13)	Acc@5 100.00 ( 99.84)
 * Acc@1 57.126 Acc@5 99.844
Epoch: [48][0/1]	Time  0.140 ( 0.140)	Data  0.050 ( 0.050)	Loss 1.3689e+00 (1.3689e+00)	Acc@1  61.14 ( 61.14)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 3.5406e-04 (3.5406e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.3652e+00 (1.2594e+00)	Acc@1  50.00 ( 59.15)	Acc@5 100.00 ( 99.88)
 * Acc@1 59.151 Acc@5 99.883
Epoch: [49][0/1]	Time  0.140 ( 0.140)	Data  0.051 ( 0.051)	Loss 1.2687e+00 (1.2687e+00)	Acc@1  62.29 ( 62.29)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 1.4072e-03 (1.4072e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 7.9171e-01 (1.1661e+00)	Acc@1  62.50 ( 64.02)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.019 Acc@5 99.961
Epoch: [50][0/1]	Time  0.231 ( 0.231)	Data  0.144 ( 0.144)	Loss 1.1623e+00 (1.1623e+00)	Acc@1  66.14 ( 66.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 7.4952e-03 (7.4952e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 6.1220e-01 (1.2464e+00)	Acc@1  75.00 ( 65.38)	Acc@5 100.00 ( 99.96)
 * Acc@1 65.382 Acc@5 99.961
Epoch: [51][0/1]	Time  0.148 ( 0.148)	Data  0.053 ( 0.053)	Loss 1.2877e+00 (1.2877e+00)	Acc@1  65.14 ( 65.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 2.9854e-03 (2.9854e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.052)	Loss 9.8618e-01 (1.2190e+00)	Acc@1  50.00 ( 65.15)	Acc@5 100.00 (100.00)
 * Acc@1 65.148 Acc@5 100.000
Epoch: [52][0/1]	Time  0.148 ( 0.148)	Data  0.054 ( 0.054)	Loss 1.1062e+00 (1.1062e+00)	Acc@1  65.14 ( 65.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 5.3518e-04 (5.3518e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.051)	Loss 1.2646e+00 (1.2109e+00)	Acc@1  37.50 ( 61.14)	Acc@5 100.00 ( 99.96)
 * Acc@1 61.137 Acc@5 99.961
Epoch: [53][0/1]	Time  0.148 ( 0.148)	Data  0.055 ( 0.055)	Loss 1.0936e+00 (1.0936e+00)	Acc@1  66.14 ( 66.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.5695e-03 (1.5695e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.050)	Loss 8.1528e-01 (1.1734e+00)	Acc@1  50.00 ( 61.92)	Acc@5 100.00 ( 99.92)
 * Acc@1 61.916 Acc@5 99.922
Epoch: [54][0/1]	Time  0.145 ( 0.145)	Data  0.055 ( 0.055)	Loss 1.1254e+00 (1.1254e+00)	Acc@1  63.14 ( 63.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.4022e-02 (1.4022e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.055)	Loss 5.9985e-01 (1.1061e+00)	Acc@1  87.50 ( 64.52)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.525 Acc@5 99.961
Epoch: [55][0/1]	Time  0.159 ( 0.159)	Data  0.066 ( 0.066)	Loss 1.0437e+00 (1.0437e+00)	Acc@1  68.00 ( 68.00)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 9.9444e-03 (9.9444e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 4.1203e-01 (1.1226e+00)	Acc@1  87.50 ( 64.84)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.836 Acc@5 99.961
Epoch: [56][0/1]	Time  0.148 ( 0.148)	Data  0.059 ( 0.059)	Loss 1.1038e+00 (1.1038e+00)	Acc@1  66.57 ( 66.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.063 ( 0.063)	Loss 1.0165e-02 (1.0165e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.053)	Loss 4.2299e-01 (1.1357e+00)	Acc@1  87.50 ( 64.95)	Acc@5 100.00 (100.00)
 * Acc@1 64.953 Acc@5 100.000
Epoch: [57][0/1]	Time  0.149 ( 0.149)	Data  0.055 ( 0.055)	Loss 1.0406e+00 (1.0406e+00)	Acc@1  68.57 ( 68.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 6.7535e-03 (6.7535e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.053)	Loss 8.0144e-01 (1.2003e+00)	Acc@1  62.50 ( 63.40)	Acc@5 100.00 (100.00)
 * Acc@1 63.396 Acc@5 100.000
Epoch: [58][0/1]	Time  0.154 ( 0.154)	Data  0.055 ( 0.055)	Loss 1.0710e+00 (1.0710e+00)	Acc@1  68.14 ( 68.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.059 ( 0.059)	Loss 3.8509e-02 (3.8509e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.053)	Loss 8.2907e-01 (1.1509e+00)	Acc@1  62.50 ( 61.49)	Acc@5 100.00 (100.00)
 * Acc@1 61.488 Acc@5 100.000
Epoch: [59][0/1]	Time  0.158 ( 0.158)	Data  0.054 ( 0.054)	Loss 1.0225e+00 (1.0225e+00)	Acc@1  66.00 ( 66.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.059 ( 0.059)	Loss 5.0506e-02 (5.0506e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 1.2631e+00 (1.1341e+00)	Acc@1  50.00 ( 62.89)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.889 Acc@5 99.961
Epoch: [60][0/1]	Time  0.148 ( 0.148)	Data  0.053 ( 0.053)	Loss 9.6480e-01 (9.6480e-01)	Acc@1  66.29 ( 66.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 4.0745e-02 (4.0745e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.052)	Loss 7.4327e-01 (1.1170e+00)	Acc@1  62.50 ( 63.59)	Acc@5 100.00 (100.00)
 * Acc@1 63.590 Acc@5 100.000
Epoch: [61][0/1]	Time  0.155 ( 0.155)	Data  0.055 ( 0.055)	Loss 9.9552e-01 (9.9552e-01)	Acc@1  67.43 ( 67.43)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 2.0668e-02 (2.0668e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 7.2391e-01 (1.1212e+00)	Acc@1  62.50 ( 63.28)	Acc@5 100.00 (100.00)
 * Acc@1 63.279 Acc@5 100.000
Epoch: [62][0/1]	Time  0.152 ( 0.152)	Data  0.052 ( 0.052)	Loss 1.0475e+00 (1.0475e+00)	Acc@1  66.71 ( 66.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 3.3576e-02 (3.3576e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.052)	Loss 3.8954e-01 (1.1025e+00)	Acc@1  87.50 ( 63.28)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.279 Acc@5 99.961
Epoch: [63][0/1]	Time  0.150 ( 0.150)	Data  0.052 ( 0.052)	Loss 9.6692e-01 (9.6692e-01)	Acc@1  69.00 ( 69.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.058 ( 0.058)	Loss 9.6739e-02 (9.6739e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.053)	Loss 6.4222e-01 (1.1189e+00)	Acc@1  87.50 ( 63.55)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.551 Acc@5 99.961
Epoch: [64][0/1]	Time  0.151 ( 0.151)	Data  0.054 ( 0.054)	Loss 1.0444e+00 (1.0444e+00)	Acc@1  68.14 ( 68.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.7272e-02 (4.7272e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.052)	Loss 1.3646e+00 (1.1447e+00)	Acc@1  50.00 ( 62.46)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.461 Acc@5 99.961
Epoch: [65][0/1]	Time  0.147 ( 0.147)	Data  0.053 ( 0.053)	Loss 9.9286e-01 (9.9286e-01)	Acc@1  67.57 ( 67.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.060 ( 0.060)	Loss 3.0466e-02 (3.0466e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.049)	Loss 8.9556e-01 (1.1528e+00)	Acc@1  62.50 ( 60.86)	Acc@5 100.00 (100.00)
 * Acc@1 60.864 Acc@5 100.000
Epoch: [66][0/1]	Time  0.117 ( 0.117)	Data  0.049 ( 0.049)	Loss 9.6335e-01 (9.6335e-01)	Acc@1  67.71 ( 67.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 3.7821e-02 (3.7821e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.041)	Loss 1.1088e+00 (1.0909e+00)	Acc@1  50.00 ( 61.76)	Acc@5 100.00 ( 99.92)
 * Acc@1 61.760 Acc@5 99.922
Epoch: [67][0/1]	Time  0.121 ( 0.121)	Data  0.055 ( 0.055)	Loss 1.0088e+00 (1.0088e+00)	Acc@1  68.29 ( 68.29)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 4.5843e-02 (4.5843e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 7.5647e-01 (1.1054e+00)	Acc@1  75.00 ( 62.27)	Acc@5 100.00 (100.00)
 * Acc@1 62.266 Acc@5 100.000
Epoch: [68][0/1]	Time  0.138 ( 0.138)	Data  0.052 ( 0.052)	Loss 9.3278e-01 (9.3278e-01)	Acc@1  68.71 ( 68.71)	Acc@5  99.86 ( 99.86)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 2.7282e-02 (2.7282e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.043)	Loss 7.8733e-01 (1.0726e+00)	Acc@1  62.50 ( 63.20)	Acc@5 100.00 (100.00)
 * Acc@1 63.201 Acc@5 100.000
Epoch: [69][0/1]	Time  0.123 ( 0.123)	Data  0.054 ( 0.054)	Loss 9.3608e-01 (9.3608e-01)	Acc@1  67.86 ( 67.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 2.3350e-02 (2.3350e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.045)	Loss 6.1973e-01 (1.0149e+00)	Acc@1  62.50 ( 64.37)	Acc@5 100.00 (100.00)
 * Acc@1 64.369 Acc@5 100.000
Epoch: [70][0/1]	Time  0.115 ( 0.115)	Data  0.054 ( 0.054)	Loss 9.5004e-01 (9.5004e-01)	Acc@1  68.86 ( 68.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 3.5887e-02 (3.5887e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.041)	Loss 6.4797e-01 (1.0267e+00)	Acc@1  62.50 ( 64.45)	Acc@5 100.00 ( 99.92)
 * Acc@1 64.447 Acc@5 99.922
Epoch: [71][0/1]	Time  0.115 ( 0.115)	Data  0.051 ( 0.051)	Loss 9.4348e-01 (9.4348e-01)	Acc@1  68.43 ( 68.43)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
.\main_lincls.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
.\main_lincls.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained, map_location="cpu")
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
.\main_lincls.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_weights, map_location="cpu")
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 8.2738e-02 (8.2738e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 7.7989e-01 (1.0396e+00)	Acc@1  62.50 ( 65.03)	Acc@5 100.00 (100.00)
 * Acc@1 65.031 Acc@5 100.000
Epoch: [72][0/1]	Time  0.112 ( 0.112)	Data  0.044 ( 0.044)	Loss 9.3946e-01 (9.3946e-01)	Acc@1  69.14 ( 69.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 6.4505e-02 (6.4505e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 7.7622e-01 (1.0183e+00)	Acc@1  50.00 ( 64.60)	Acc@5 100.00 (100.00)
 * Acc@1 64.603 Acc@5 100.000
Epoch: [73][0/1]	Time  0.126 ( 0.126)	Data  0.057 ( 0.057)	Loss 9.7883e-01 (9.7883e-01)	Acc@1  70.86 ( 70.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.046 ( 0.046)	Loss 4.8289e-02 (4.8289e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 7.2075e-01 (9.7346e-01)	Acc@1  75.00 ( 66.28)	Acc@5 100.00 ( 99.96)
 * Acc@1 66.277 Acc@5 99.961
Epoch: [74][0/1]	Time  0.172 ( 0.172)	Data  0.071 ( 0.071)	Loss 9.9423e-01 (9.9423e-01)	Acc@1  68.14 ( 68.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.2536e-02 (5.2536e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.043)	Loss 9.5890e-01 (9.8718e-01)	Acc@1  37.50 ( 64.76)	Acc@5 100.00 (100.00)
 * Acc@1 64.759 Acc@5 100.000
Epoch: [75][0/1]	Time  0.124 ( 0.124)	Data  0.052 ( 0.052)	Loss 9.4544e-01 (9.4544e-01)	Acc@1  68.00 ( 68.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 9.5452e-02 (9.5452e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.042)	Loss 3.6055e-01 (9.8894e-01)	Acc@1  87.50 ( 64.95)	Acc@5 100.00 (100.00)
 * Acc@1 64.953 Acc@5 100.000
Epoch: [76][0/1]	Time  0.126 ( 0.126)	Data  0.047 ( 0.047)	Loss 9.5709e-01 (9.5709e-01)	Acc@1  68.57 ( 68.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 6.5740e-02 (6.5740e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.008 ( 0.045)	Loss 8.7018e-01 (9.9421e-01)	Acc@1  62.50 ( 65.30)	Acc@5 100.00 (100.00)
 * Acc@1 65.304 Acc@5 100.000
Epoch: [77][0/1]	Time  0.120 ( 0.120)	Data  0.051 ( 0.051)	Loss 8.6891e-01 (8.6891e-01)	Acc@1  70.71 ( 70.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 9.5367e-02 (9.5367e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.044)	Loss 9.6143e-01 (9.9495e-01)	Acc@1  62.50 ( 65.19)	Acc@5 100.00 ( 99.96)
 * Acc@1 65.187 Acc@5 99.961
Epoch: [78][0/1]	Time  0.145 ( 0.145)	Data  0.065 ( 0.065)	Loss 9.2758e-01 (9.2758e-01)	Acc@1  68.71 ( 68.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 7.9787e-02 (7.9787e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.051)	Loss 1.0533e+00 (9.9261e-01)	Acc@1  50.00 ( 64.49)	Acc@5 100.00 (100.00)
 * Acc@1 64.486 Acc@5 100.000
Epoch: [79][0/1]	Time  0.132 ( 0.132)	Data  0.046 ( 0.046)	Loss 9.1908e-01 (9.1908e-01)	Acc@1  70.57 ( 70.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 7.2720e-02 (7.2720e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.044)	Loss 8.5276e-01 (1.0229e+00)	Acc@1  50.00 ( 63.98)	Acc@5 100.00 (100.00)
 * Acc@1 63.980 Acc@5 100.000
Epoch: [80][0/1]	Time  0.130 ( 0.130)	Data  0.061 ( 0.061)	Loss 8.7704e-01 (8.7704e-01)	Acc@1  71.00 ( 71.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 4.5846e-02 (4.5846e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.040)	Loss 8.0428e-01 (1.0125e+00)	Acc@1  75.00 ( 64.21)	Acc@5 100.00 (100.00)
 * Acc@1 64.213 Acc@5 100.000
Epoch: [81][0/1]	Time  0.119 ( 0.119)	Data  0.049 ( 0.049)	Loss 9.7281e-01 (9.7281e-01)	Acc@1  70.14 ( 70.14)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 3.6564e-02 (3.6564e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.043)	Loss 5.7739e-01 (1.0195e+00)	Acc@1  87.50 ( 63.16)	Acc@5 100.00 (100.00)
 * Acc@1 63.162 Acc@5 100.000
Epoch: [82][0/1]	Time  0.117 ( 0.117)	Data  0.053 ( 0.053)	Loss 8.9357e-01 (8.9357e-01)	Acc@1  70.00 ( 70.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 4.3760e-02 (4.3760e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.041)	Loss 7.8168e-01 (1.0110e+00)	Acc@1  62.50 ( 63.90)	Acc@5 100.00 (100.00)
 * Acc@1 63.902 Acc@5 100.000
Epoch: [83][0/1]	Time  0.119 ( 0.119)	Data  0.050 ( 0.050)	Loss 9.0589e-01 (9.0589e-01)	Acc@1  69.57 ( 69.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.046 ( 0.046)	Loss 3.3402e-02 (3.3402e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 6.6400e-01 (9.8459e-01)	Acc@1  75.00 ( 64.91)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.914 Acc@5 99.961
Epoch: [84][0/1]	Time  0.127 ( 0.127)	Data  0.056 ( 0.056)	Loss 9.2340e-01 (9.2340e-01)	Acc@1  68.86 ( 68.86)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 6.9006e-02 (6.9006e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.041)	Loss 7.6178e-01 (1.0101e+00)	Acc@1  62.50 ( 63.75)	Acc@5 100.00 (100.00)
 * Acc@1 63.746 Acc@5 100.000
Epoch: [85][0/1]	Time  0.131 ( 0.131)	Data  0.065 ( 0.065)	Loss 8.7201e-01 (8.7201e-01)	Acc@1  70.43 ( 70.43)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 3.3427e-02 (3.3427e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.042)	Loss 6.4803e-01 (9.9327e-01)	Acc@1  62.50 ( 64.49)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.486 Acc@5 99.961
Epoch: [86][0/1]	Time  0.114 ( 0.114)	Data  0.046 ( 0.046)	Loss 8.6337e-01 (8.6337e-01)	Acc@1  71.00 ( 71.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 2.3522e-02 (2.3522e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 6.2399e-01 (1.0171e+00)	Acc@1  75.00 ( 64.02)	Acc@5 100.00 (100.00)
 * Acc@1 64.019 Acc@5 100.000
Epoch: [87][0/1]	Time  0.137 ( 0.137)	Data  0.054 ( 0.054)	Loss 9.0390e-01 (9.0390e-01)	Acc@1  68.57 ( 68.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.6146e-02 (1.6146e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.044)	Loss 7.8995e-01 (1.0246e+00)	Acc@1  62.50 ( 63.90)	Acc@5 100.00 (100.00)
 * Acc@1 63.902 Acc@5 100.000
Epoch: [88][0/1]	Time  0.117 ( 0.117)	Data  0.049 ( 0.049)	Loss 9.6455e-01 (9.6455e-01)	Acc@1  68.71 ( 68.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 1.5672e-02 (1.5672e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.041)	Loss 8.1742e-01 (9.9713e-01)	Acc@1  62.50 ( 64.52)	Acc@5 100.00 (100.00)
 * Acc@1 64.525 Acc@5 100.000
Epoch: [89][0/1]	Time  0.117 ( 0.117)	Data  0.050 ( 0.050)	Loss 8.7165e-01 (8.7165e-01)	Acc@1  68.71 ( 68.71)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:100
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 3.0410e-02 (3.0410e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.042)	Loss 7.0504e-01 (1.0042e+00)	Acc@1  62.50 ( 63.86)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.863 Acc@5 99.961
Running with excep_size=50 
Use GPU: 0 for training
=> creating model 'resnet50'
SimSiam(
  (encoder): ResNet(
    (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=512, bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=512, out_features=2048, bias=True)
      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (predictor): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=2048, bias=True)
  )
)
Epoch: [0][0/2]	Time  1.477 ( 1.477)	Data  0.083 ( 0.083)	Loss 0.0052 (0.0052)
Epoch: [1][0/2]	Time  0.139 ( 0.139)	Data  0.063 ( 0.063)	Loss -0.0761 (-0.0761)
Epoch: [2][0/2]	Time  0.134 ( 0.134)	Data  0.059 ( 0.059)	Loss -0.1547 (-0.1547)
Epoch: [3][0/2]	Time  0.125 ( 0.125)	Data  0.059 ( 0.059)	Loss -0.2332 (-0.2332)
Epoch: [4][0/2]	Time  0.127 ( 0.127)	Data  0.059 ( 0.059)	Loss -0.3377 (-0.3377)
Epoch: [5][0/2]	Time  0.125 ( 0.125)	Data  0.059 ( 0.059)	Loss -0.4291 (-0.4291)
Epoch: [6][0/2]	Time  0.161 ( 0.161)	Data  0.059 ( 0.059)	Loss -0.4996 (-0.4996)
Epoch: [7][0/2]	Time  0.168 ( 0.168)	Data  0.059 ( 0.059)	Loss -0.5699 (-0.5699)
Epoch: [8][0/2]	Time  0.172 ( 0.172)	Data  0.058 ( 0.058)	Loss -0.6280 (-0.6280)
Epoch: [9][0/2]	Time  0.175 ( 0.175)	Data  0.059 ( 0.059)	Loss -0.6626 (-0.6626)
Epoch: [10][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.7021 (-0.7021)
Epoch: [11][0/2]	Time  0.160 ( 0.160)	Data  0.058 ( 0.058)	Loss -0.7175 (-0.7175)
Epoch: [12][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.7258 (-0.7258)
Epoch: [13][0/2]	Time  0.170 ( 0.170)	Data  0.059 ( 0.059)	Loss -0.7330 (-0.7330)
Epoch: [14][0/2]	Time  0.166 ( 0.166)	Data  0.059 ( 0.059)	Loss -0.7506 (-0.7506)
Epoch: [15][0/2]	Time  0.175 ( 0.175)	Data  0.059 ( 0.059)	Loss -0.7434 (-0.7434)
Epoch: [16][0/2]	Time  0.172 ( 0.172)	Data  0.059 ( 0.059)	Loss -0.7450 (-0.7450)
Epoch: [17][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.7426 (-0.7426)
Epoch: [18][0/2]	Time  0.178 ( 0.178)	Data  0.059 ( 0.059)	Loss -0.7573 (-0.7573)
Epoch: [19][0/2]	Time  0.173 ( 0.173)	Data  0.059 ( 0.059)	Loss -0.7592 (-0.7592)
Epoch: [20][0/2]	Time  0.149 ( 0.149)	Data  0.057 ( 0.057)	Loss -0.7580 (-0.7580)
Epoch: [21][0/2]	Time  0.176 ( 0.176)	Data  0.059 ( 0.059)	Loss -0.7613 (-0.7613)
Epoch: [22][0/2]	Time  0.171 ( 0.171)	Data  0.060 ( 0.060)	Loss -0.7853 (-0.7853)
Epoch: [23][0/2]	Time  0.182 ( 0.182)	Data  0.059 ( 0.059)	Loss -0.8072 (-0.8072)
Epoch: [24][0/2]	Time  0.156 ( 0.156)	Data  0.058 ( 0.058)	Loss -0.8121 (-0.8121)
Epoch: [25][0/2]	Time  0.173 ( 0.173)	Data  0.060 ( 0.060)	Loss -0.8092 (-0.8092)
Epoch: [26][0/2]	Time  0.176 ( 0.176)	Data  0.061 ( 0.061)	Loss -0.8293 (-0.8293)
Epoch: [27][0/2]	Time  0.165 ( 0.165)	Data  0.059 ( 0.059)	Loss -0.8326 (-0.8326)
Epoch: [28][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.8334 (-0.8334)
Epoch: [29][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.8345 (-0.8345)
Epoch: [30][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8417 (-0.8417)
Epoch: [31][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.8302 (-0.8302)
Epoch: [32][0/2]	Time  0.170 ( 0.170)	Data  0.058 ( 0.058)	Loss -0.8279 (-0.8279)
Epoch: [33][0/2]	Time  0.153 ( 0.153)	Data  0.059 ( 0.059)	Loss -0.8041 (-0.8041)
Epoch: [34][0/2]	Time  0.177 ( 0.177)	Data  0.061 ( 0.061)	Loss -0.7755 (-0.7755)
Epoch: [35][0/2]	Time  0.172 ( 0.172)	Data  0.059 ( 0.059)	Loss -0.7553 (-0.7553)
Epoch: [36][0/2]	Time  0.157 ( 0.157)	Data  0.059 ( 0.059)	Loss -0.7511 (-0.7511)
Epoch: [37][0/2]	Time  0.173 ( 0.173)	Data  0.059 ( 0.059)	Loss -0.7620 (-0.7620)
Epoch: [38][0/2]	Time  0.171 ( 0.171)	Data  0.057 ( 0.057)	Loss -0.7666 (-0.7666)
Epoch: [39][0/2]	Time  0.174 ( 0.174)	Data  0.058 ( 0.058)	Loss -0.7695 (-0.7695)
Epoch: [40][0/2]	Time  0.166 ( 0.166)	Data  0.058 ( 0.058)	Loss -0.7754 (-0.7754)
Epoch: [41][0/2]	Time  0.170 ( 0.170)	Data  0.058 ( 0.058)	Loss -0.8046 (-0.8046)
Epoch: [42][0/2]	Time  0.170 ( 0.170)	Data  0.059 ( 0.059)	Loss -0.8136 (-0.8136)
Epoch: [43][0/2]	Time  0.204 ( 0.204)	Data  0.069 ( 0.069)	Loss -0.7971 (-0.7971)
Epoch: [44][0/2]	Time  0.162 ( 0.162)	Data  0.058 ( 0.058)	Loss -0.8129 (-0.8129)
Epoch: [45][0/2]	Time  0.157 ( 0.157)	Data  0.058 ( 0.058)	Loss -0.8200 (-0.8200)
Epoch: [46][0/2]	Time  0.218 ( 0.218)	Data  0.062 ( 0.062)	Loss -0.8232 (-0.8232)
Epoch: [47][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8279 (-0.8279)
Epoch: [48][0/2]	Time  0.176 ( 0.176)	Data  0.068 ( 0.068)	Loss -0.8333 (-0.8333)
Epoch: [49][0/2]	Time  0.179 ( 0.179)	Data  0.063 ( 0.063)	Loss -0.8227 (-0.8227)
Epoch: [50][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.8443 (-0.8443)
Epoch: [51][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.8495 (-0.8495)
Epoch: [52][0/2]	Time  0.150 ( 0.150)	Data  0.059 ( 0.059)	Loss -0.8577 (-0.8577)
Epoch: [53][0/2]	Time  0.184 ( 0.184)	Data  0.061 ( 0.061)	Loss -0.8591 (-0.8591)
Epoch: [54][0/2]	Time  0.180 ( 0.180)	Data  0.063 ( 0.063)	Loss -0.8661 (-0.8661)
Epoch: [55][0/2]	Time  0.202 ( 0.202)	Data  0.089 ( 0.089)	Loss -0.8539 (-0.8539)
Epoch: [56][0/2]	Time  0.173 ( 0.173)	Data  0.063 ( 0.063)	Loss -0.8762 (-0.8762)
Epoch: [57][0/2]	Time  0.166 ( 0.166)	Data  0.059 ( 0.059)	Loss -0.8620 (-0.8620)
Epoch: [58][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8727 (-0.8727)
Epoch: [59][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8698 (-0.8698)
Epoch: [60][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8796 (-0.8796)
Epoch: [61][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.8768 (-0.8768)
Epoch: [62][0/2]	Time  0.185 ( 0.185)	Data  0.059 ( 0.059)	Loss -0.8677 (-0.8677)
Epoch: [63][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.8831 (-0.8831)
Epoch: [64][0/2]	Time  0.176 ( 0.176)	Data  0.060 ( 0.060)	Loss -0.8753 (-0.8753)
Epoch: [65][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.8772 (-0.8772)
Epoch: [66][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8823 (-0.8823)
Epoch: [67][0/2]	Time  0.181 ( 0.181)	Data  0.059 ( 0.059)	Loss -0.8873 (-0.8873)
Epoch: [68][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8783 (-0.8783)
Epoch: [69][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.8809 (-0.8809)
Epoch: [70][0/2]	Time  0.195 ( 0.195)	Data  0.062 ( 0.062)	Loss -0.8888 (-0.8888)
Epoch: [71][0/2]	Time  0.206 ( 0.206)	Data  0.065 ( 0.065)	Loss -0.8818 (-0.8818)
Epoch: [72][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.8945 (-0.8945)
Epoch: [73][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.8950 (-0.8950)
Epoch: [74][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8955 (-0.8955)
Epoch: [75][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.8812 (-0.8812)
Epoch: [76][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.8888 (-0.8888)
Epoch: [77][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.8977 (-0.8977)
Epoch: [78][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.8920 (-0.8920)
Epoch: [79][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.8975 (-0.8975)
Epoch: [80][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.9002 (-0.9002)
Epoch: [81][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9023 (-0.9023)
Epoch: [82][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.8910 (-0.8910)
Epoch: [83][0/2]	Time  0.191 ( 0.191)	Data  0.059 ( 0.059)	Loss -0.8990 (-0.8990)
Epoch: [84][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.8895 (-0.8895)
Epoch: [85][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9033 (-0.9033)
Epoch: [86][0/2]	Time  0.201 ( 0.201)	Data  0.060 ( 0.060)	Loss -0.8903 (-0.8903)
Epoch: [87][0/2]	Time  0.206 ( 0.206)	Data  0.062 ( 0.062)	Loss -0.8842 (-0.8842)
Epoch: [88][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.8966 (-0.8966)
Epoch: [89][0/2]	Time  0.202 ( 0.202)	Data  0.060 ( 0.060)	Loss -0.9006 (-0.9006)
Epoch: [90][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.8801 (-0.8801)
Epoch: [91][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.8921 (-0.8921)
Epoch: [92][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8740 (-0.8740)
Epoch: [93][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.8729 (-0.8729)
Epoch: [94][0/2]	Time  0.199 ( 0.199)	Data  0.059 ( 0.059)	Loss -0.8750 (-0.8750)
Epoch: [95][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.8814 (-0.8814)
Epoch: [96][0/2]	Time  0.202 ( 0.202)	Data  0.061 ( 0.061)	Loss -0.8722 (-0.8722)
Epoch: [97][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8723 (-0.8723)
Epoch: [98][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.8810 (-0.8810)
Epoch: [99][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.8791 (-0.8791)
Epoch: [100][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8807 (-0.8807)
Epoch: [101][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.8801 (-0.8801)
Epoch: [102][0/2]	Time  0.210 ( 0.210)	Data  0.064 ( 0.064)	Loss -0.8883 (-0.8883)
Epoch: [103][0/2]	Time  0.191 ( 0.191)	Data  0.059 ( 0.059)	Loss -0.8878 (-0.8878)
Epoch: [104][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.8923 (-0.8923)
Epoch: [105][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8921 (-0.8921)
Epoch: [106][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8936 (-0.8936)
Epoch: [107][0/2]	Time  0.199 ( 0.199)	Data  0.059 ( 0.059)	Loss -0.8856 (-0.8856)
Epoch: [108][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.8962 (-0.8962)
Epoch: [109][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.8816 (-0.8816)
Epoch: [110][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.8883 (-0.8883)
Epoch: [111][0/2]	Time  0.195 ( 0.195)	Data  0.059 ( 0.059)	Loss -0.9014 (-0.9014)
Epoch: [112][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.8993 (-0.8993)
Epoch: [113][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.8987 (-0.8987)
Epoch: [114][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8970 (-0.8970)
Epoch: [115][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9040 (-0.9040)
Epoch: [116][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.8933 (-0.8933)
Epoch: [117][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8931 (-0.8931)
Epoch: [118][0/2]	Time  0.179 ( 0.179)	Data  0.059 ( 0.059)	Loss -0.8901 (-0.8901)
Epoch: [119][0/2]	Time  0.194 ( 0.194)	Data  0.057 ( 0.057)	Loss -0.8919 (-0.8919)
Epoch: [120][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9039 (-0.9039)
Epoch: [121][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.8991 (-0.8991)
Epoch: [122][0/2]	Time  0.194 ( 0.194)	Data  0.057 ( 0.057)	Loss -0.9008 (-0.9008)
Epoch: [123][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9044 (-0.9044)
Epoch: [124][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9023 (-0.9023)
Epoch: [125][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8965 (-0.8965)
Epoch: [126][0/2]	Time  0.191 ( 0.191)	Data  0.059 ( 0.059)	Loss -0.9109 (-0.9109)
Epoch: [127][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9033 (-0.9033)
.\main_simsiam.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
Epoch: [128][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9088 (-0.9088)
Epoch: [129][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9047 (-0.9047)
Epoch: [130][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.9082 (-0.9082)
Epoch: [131][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9097 (-0.9097)
Epoch: [132][0/2]	Time  0.194 ( 0.194)	Data  0.060 ( 0.060)	Loss -0.9133 (-0.9133)
Epoch: [133][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9142 (-0.9142)
Epoch: [134][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.9057 (-0.9057)
Epoch: [135][0/2]	Time  0.203 ( 0.203)	Data  0.061 ( 0.061)	Loss -0.9122 (-0.9122)
Epoch: [136][0/2]	Time  0.173 ( 0.173)	Data  0.057 ( 0.057)	Loss -0.9099 (-0.9099)
Epoch: [137][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9017 (-0.9017)
Epoch: [138][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.9132 (-0.9132)
Epoch: [139][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.9074 (-0.9074)
Epoch: [140][0/2]	Time  0.195 ( 0.195)	Data  0.059 ( 0.059)	Loss -0.9101 (-0.9101)
Epoch: [141][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9000 (-0.9000)
Epoch: [142][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9051 (-0.9051)
Epoch: [143][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9011 (-0.9011)
Epoch: [144][0/2]	Time  0.173 ( 0.173)	Data  0.058 ( 0.058)	Loss -0.9051 (-0.9051)
Epoch: [145][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9167 (-0.9167)
Epoch: [146][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9078 (-0.9078)
Epoch: [147][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.8983 (-0.8983)
Epoch: [148][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9120 (-0.9120)
Epoch: [149][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9080 (-0.9080)
Epoch: [150][0/2]	Time  0.191 ( 0.191)	Data  0.062 ( 0.062)	Loss -0.9098 (-0.9098)
Epoch: [151][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9174 (-0.9174)
Epoch: [152][0/2]	Time  0.177 ( 0.177)	Data  0.057 ( 0.057)	Loss -0.9075 (-0.9075)
Epoch: [153][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9093 (-0.9093)
Epoch: [154][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9073 (-0.9073)
Epoch: [155][0/2]	Time  0.187 ( 0.187)	Data  0.059 ( 0.059)	Loss -0.9138 (-0.9138)
Epoch: [156][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9050 (-0.9050)
Epoch: [157][0/2]	Time  0.176 ( 0.176)	Data  0.059 ( 0.059)	Loss -0.9126 (-0.9126)
Epoch: [158][0/2]	Time  0.191 ( 0.191)	Data  0.058 ( 0.058)	Loss -0.9013 (-0.9013)
Epoch: [159][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9076 (-0.9076)
Epoch: [160][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9028 (-0.9028)
Epoch: [161][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.9119 (-0.9119)
Epoch: [162][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9074 (-0.9074)
Epoch: [163][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.9189 (-0.9189)
Epoch: [164][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9061 (-0.9061)
Epoch: [165][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9159 (-0.9159)
Epoch: [166][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9108 (-0.9108)
Epoch: [167][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9108 (-0.9108)
Epoch: [168][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9119 (-0.9119)
Epoch: [169][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9131 (-0.9131)
Epoch: [170][0/2]	Time  0.201 ( 0.201)	Data  0.060 ( 0.060)	Loss -0.9104 (-0.9104)
Epoch: [171][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.9035 (-0.9035)
Epoch: [172][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9116 (-0.9116)
Epoch: [173][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.9088 (-0.9088)
Epoch: [174][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.8966 (-0.8966)
Epoch: [175][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9154 (-0.9154)
Epoch: [176][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9179 (-0.9179)
Epoch: [177][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9071 (-0.9071)
Epoch: [178][0/2]	Time  0.182 ( 0.182)	Data  0.059 ( 0.059)	Loss -0.8970 (-0.8970)
Epoch: [179][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9047 (-0.9047)
Epoch: [180][0/2]	Time  0.205 ( 0.205)	Data  0.060 ( 0.060)	Loss -0.9043 (-0.9043)
Epoch: [181][0/2]	Time  0.194 ( 0.194)	Data  0.063 ( 0.063)	Loss -0.9085 (-0.9085)
Epoch: [182][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9155 (-0.9155)
Epoch: [183][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9101 (-0.9101)
Epoch: [184][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8985 (-0.8985)
Epoch: [185][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9097 (-0.9097)
Epoch: [186][0/2]	Time  0.192 ( 0.192)	Data  0.059 ( 0.059)	Loss -0.9018 (-0.9018)
Epoch: [187][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9133 (-0.9133)
Epoch: [188][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.8955 (-0.8955)
Epoch: [189][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9097 (-0.9097)
Epoch: [190][0/2]	Time  0.175 ( 0.175)	Data  0.059 ( 0.059)	Loss -0.9040 (-0.9040)
Epoch: [191][0/2]	Time  0.191 ( 0.191)	Data  0.057 ( 0.057)	Loss -0.8960 (-0.8960)
Epoch: [192][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9025 (-0.9025)
Epoch: [193][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9072 (-0.9072)
Epoch: [194][0/2]	Time  0.201 ( 0.201)	Data  0.061 ( 0.061)	Loss -0.9069 (-0.9069)
Epoch: [195][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.9114 (-0.9114)
Epoch: [196][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.9031 (-0.9031)
Epoch: [197][0/2]	Time  0.206 ( 0.206)	Data  0.061 ( 0.061)	Loss -0.9021 (-0.9021)
Epoch: [198][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9053 (-0.9053)
Epoch: [199][0/2]	Time  0.196 ( 0.196)	Data  0.059 ( 0.059)	Loss -0.9107 (-0.9107)
Epoch: [200][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8997 (-0.8997)
Epoch: [201][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9104 (-0.9104)
Epoch: [202][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9058 (-0.9058)
Epoch: [203][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9098 (-0.9098)
Epoch: [204][0/2]	Time  0.191 ( 0.191)	Data  0.058 ( 0.058)	Loss -0.8972 (-0.8972)
Epoch: [205][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8793 (-0.8793)
Epoch: [206][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9096 (-0.9096)
Epoch: [207][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9071 (-0.9071)
Epoch: [208][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.8963 (-0.8963)
Epoch: [209][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9063 (-0.9063)
Epoch: [210][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9100 (-0.9100)
Epoch: [211][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.9065 (-0.9065)
Epoch: [212][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9080 (-0.9080)
Epoch: [213][0/2]	Time  0.199 ( 0.199)	Data  0.057 ( 0.057)	Loss -0.9110 (-0.9110)
Epoch: [214][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.9068 (-0.9068)
Epoch: [215][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9057 (-0.9057)
Epoch: [216][0/2]	Time  0.169 ( 0.169)	Data  0.058 ( 0.058)	Loss -0.9069 (-0.9069)
Epoch: [217][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9163 (-0.9163)
Epoch: [218][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9172 (-0.9172)
Epoch: [219][0/2]	Time  0.186 ( 0.186)	Data  0.060 ( 0.060)	Loss -0.9092 (-0.9092)
Epoch: [220][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9166 (-0.9166)
Epoch: [221][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.8987 (-0.8987)
Epoch: [222][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.9086 (-0.9086)
Epoch: [223][0/2]	Time  0.211 ( 0.211)	Data  0.069 ( 0.069)	Loss -0.9034 (-0.9034)
Epoch: [224][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.9115 (-0.9115)
Epoch: [225][0/2]	Time  0.203 ( 0.203)	Data  0.061 ( 0.061)	Loss -0.9162 (-0.9162)
Epoch: [226][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9037 (-0.9037)
Epoch: [227][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9096 (-0.9096)
Epoch: [228][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9087 (-0.9087)
Epoch: [229][0/2]	Time  0.193 ( 0.193)	Data  0.062 ( 0.062)	Loss -0.9108 (-0.9108)
Epoch: [230][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9180 (-0.9180)
Epoch: [231][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9099 (-0.9099)
Epoch: [232][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9005 (-0.9005)
Epoch: [233][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9082 (-0.9082)
Epoch: [234][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9093 (-0.9093)
Epoch: [235][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9047 (-0.9047)
Epoch: [236][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.9037 (-0.9037)
Epoch: [237][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9136 (-0.9136)
Epoch: [238][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9122 (-0.9122)
Epoch: [239][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.9138 (-0.9138)
Epoch: [240][0/2]	Time  0.197 ( 0.197)	Data  0.059 ( 0.059)	Loss -0.9146 (-0.9146)
Epoch: [241][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9141 (-0.9141)
Epoch: [242][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.9140 (-0.9140)
Epoch: [243][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9142 (-0.9142)
Epoch: [244][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9042 (-0.9042)
Epoch: [245][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9021 (-0.9021)
Epoch: [246][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9063 (-0.9063)
Epoch: [247][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9046 (-0.9046)
Epoch: [248][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9058 (-0.9058)
Epoch: [249][0/2]	Time  0.176 ( 0.176)	Data  0.057 ( 0.057)	Loss -0.9039 (-0.9039)
Epoch: [250][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9166 (-0.9166)
Epoch: [251][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9148 (-0.9148)
Epoch: [252][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.9077 (-0.9077)
Epoch: [253][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9040 (-0.9040)
Epoch: [254][0/2]	Time  0.193 ( 0.193)	Data  0.060 ( 0.060)	Loss -0.9104 (-0.9104)
Epoch: [255][0/2]	Time  0.196 ( 0.196)	Data  0.059 ( 0.059)	Loss -0.8994 (-0.8994)
Epoch: [256][0/2]	Time  0.204 ( 0.204)	Data  0.064 ( 0.064)	Loss -0.9002 (-0.9002)
Epoch: [257][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9097 (-0.9097)
Epoch: [258][0/2]	Time  0.207 ( 0.207)	Data  0.064 ( 0.064)	Loss -0.9144 (-0.9144)
Epoch: [259][0/2]	Time  0.192 ( 0.192)	Data  0.066 ( 0.066)	Loss -0.9106 (-0.9106)
Epoch: [260][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9015 (-0.9015)
Epoch: [261][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9115 (-0.9115)
Epoch: [262][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9073 (-0.9073)
Epoch: [263][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.9046 (-0.9046)
Epoch: [264][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9099 (-0.9099)
Epoch: [265][0/2]	Time  0.207 ( 0.207)	Data  0.062 ( 0.062)	Loss -0.9093 (-0.9093)
Epoch: [266][0/2]	Time  0.183 ( 0.183)	Data  0.063 ( 0.063)	Loss -0.9168 (-0.9168)
Epoch: [267][0/2]	Time  0.197 ( 0.197)	Data  0.062 ( 0.062)	Loss -0.9115 (-0.9115)
Epoch: [268][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9159 (-0.9159)
Epoch: [269][0/2]	Time  0.185 ( 0.185)	Data  0.059 ( 0.059)	Loss -0.9062 (-0.9062)
Epoch: [270][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9126 (-0.9126)
Epoch: [271][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9098 (-0.9098)
Epoch: [272][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9109 (-0.9109)
Epoch: [273][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9095 (-0.9095)
Epoch: [274][0/2]	Time  0.181 ( 0.181)	Data  0.059 ( 0.059)	Loss -0.9077 (-0.9077)
Epoch: [275][0/2]	Time  0.191 ( 0.191)	Data  0.059 ( 0.059)	Loss -0.9080 (-0.9080)
Epoch: [276][0/2]	Time  0.206 ( 0.206)	Data  0.064 ( 0.064)	Loss -0.9060 (-0.9060)
Epoch: [277][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9072 (-0.9072)
Epoch: [278][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9087 (-0.9087)
Epoch: [279][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.9126 (-0.9126)
Epoch: [280][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9079 (-0.9079)
Epoch: [281][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9028 (-0.9028)
Epoch: [282][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.9122 (-0.9122)
Epoch: [283][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9193 (-0.9193)
Epoch: [284][0/2]	Time  0.205 ( 0.205)	Data  0.060 ( 0.060)	Loss -0.9161 (-0.9161)
Epoch: [285][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9085 (-0.9085)
Epoch: [286][0/2]	Time  0.182 ( 0.182)	Data  0.061 ( 0.061)	Loss -0.9100 (-0.9100)
Epoch: [287][0/2]	Time  0.223 ( 0.223)	Data  0.059 ( 0.059)	Loss -0.9042 (-0.9042)
Epoch: [288][0/2]	Time  0.212 ( 0.212)	Data  0.059 ( 0.059)	Loss -0.9129 (-0.9129)
Epoch: [289][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9131 (-0.9131)
Epoch: [290][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9213 (-0.9213)
Epoch: [291][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9129 (-0.9129)
Epoch: [292][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9152 (-0.9152)
Epoch: [293][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9085 (-0.9085)
Epoch: [294][0/2]	Time  0.176 ( 0.176)	Data  0.059 ( 0.059)	Loss -0.9090 (-0.9090)
Epoch: [295][0/2]	Time  0.216 ( 0.216)	Data  0.059 ( 0.059)	Loss -0.9078 (-0.9078)
Epoch: [296][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9122 (-0.9122)
Epoch: [297][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9167 (-0.9167)
Epoch: [298][0/2]	Time  0.217 ( 0.217)	Data  0.059 ( 0.059)	Loss -0.9184 (-0.9184)
Epoch: [299][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.9047 (-0.9047)
simsiam program ends here, while ssv:200, normal:200, excep_size:50
Use GPU: 0 for training
=> creating model 'resnet50'
=> loading checkpoint 'checkpoints/checkpoint_0099.pth.tar'
=> loaded pre-trained model 'checkpoints/checkpoint_0099.pth.tar'
Epoch: [0][0/1]	Time  0.637 ( 0.637)	Data  0.048 ( 0.048)	Loss 1.7867e+00 (1.7867e+00)	Acc@1  23.11 ( 23.11)	Acc@5  86.00 ( 86.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.202 ( 0.202)	Loss 1.6440e-04 (1.6440e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.073 ( 0.053)	Loss 7.2546e+00 (5.0018e+00)	Acc@1   0.00 ( 27.57)	Acc@5  87.50 ( 93.81)
 * Acc@1 27.570 Acc@5 93.808
=> loading 'checkpoints/checkpoint_0099.pth.tar' for sanity check
=> sanity check passed.
Epoch: [1][0/1]	Time  0.066 ( 0.066)	Data  0.029 ( 0.029)	Loss 3.6515e+00 (3.6515e+00)	Acc@1  49.56 ( 49.56)	Acc@5  92.00 ( 92.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.033 ( 0.033)	Loss 7.9160e-02 (7.9160e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.033)	Loss 1.5201e+00 (1.9944e+00)	Acc@1   0.00 ( 40.81)	Acc@5 100.00 (100.00)
 * Acc@1 40.810 Acc@5 100.000
Epoch: [2][0/1]	Time  0.053 ( 0.053)	Data  0.027 ( 0.027)	Loss 1.2429e+00 (1.2429e+00)	Acc@1  56.44 ( 56.44)	Acc@5  98.00 ( 98.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.036 ( 0.036)	Loss 6.5560e+00 (6.5560e+00)	Acc@1   0.00 (  0.00)	Acc@5   5.47 (  5.47)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 7.9589e-01 (4.8335e+00)	Acc@1  87.50 ( 29.67)	Acc@5 100.00 ( 84.07)
 * Acc@1 29.673 Acc@5 84.073
Epoch: [3][0/1]	Time  0.063 ( 0.063)	Data  0.035 ( 0.035)	Loss 5.1687e+00 (5.1687e+00)	Acc@1  15.33 ( 15.33)	Acc@5  56.67 ( 56.67)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.036 ( 0.036)	Loss 9.3381e-02 (9.3381e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.031)	Loss 9.2999e-01 (5.3127e+00)	Acc@1  37.50 ( 38.47)	Acc@5 100.00 ( 90.23)
 * Acc@1 38.474 Acc@5 90.226
Epoch: [4][0/1]	Time  0.062 ( 0.062)	Data  0.033 ( 0.033)	Loss 3.2874e+00 (3.2874e+00)	Acc@1  56.00 ( 56.00)	Acc@5  92.44 ( 92.44)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.037 ( 0.037)	Loss 2.5004e-03 (2.5004e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.031)	Loss 2.6195e+00 (6.4639e+00)	Acc@1   0.00 ( 36.57)	Acc@5 100.00 ( 87.77)
 * Acc@1 36.565 Acc@5 87.773
Epoch: [5][0/1]	Time  0.060 ( 0.060)	Data  0.031 ( 0.031)	Loss 4.7143e+00 (4.7143e+00)	Acc@1  56.00 ( 56.00)	Acc@5  90.89 ( 90.89)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.037 ( 0.037)	Loss 1.0427e-04 (1.0427e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.033)	Loss 3.3150e-01 (9.7399e+00)	Acc@1  87.50 ( 41.24)	Acc@5 100.00 ( 88.08)
 * Acc@1 41.238 Acc@5 88.084
Epoch: [6][0/1]	Time  0.055 ( 0.055)	Data  0.027 ( 0.027)	Loss 7.3165e+00 (7.3165e+00)	Acc@1  57.78 ( 57.78)	Acc@5  91.11 ( 91.11)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.032 ( 0.032)	Loss 1.1691e-04 (1.1691e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.031)	Loss 7.1682e-02 (9.4109e+00)	Acc@1 100.00 ( 40.77)	Acc@5 100.00 ( 89.17)
 * Acc@1 40.771 Acc@5 89.174
Epoch: [7][0/1]	Time  0.058 ( 0.058)	Data  0.030 ( 0.030)	Loss 6.6696e+00 (6.6696e+00)	Acc@1  56.89 ( 56.89)	Acc@5  91.33 ( 91.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.033 ( 0.033)	Loss 1.8746e-03 (1.8746e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.043)	Loss 4.7053e+00 (6.4003e+00)	Acc@1   0.00 ( 30.14)	Acc@5 100.00 ( 92.72)
 * Acc@1 30.140 Acc@5 92.718
Epoch: [8][0/1]	Time  0.085 ( 0.085)	Data  0.030 ( 0.030)	Loss 3.9456e+00 (3.9456e+00)	Acc@1  54.67 ( 54.67)	Acc@5  93.33 ( 93.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 4.4172e-01 (4.4172e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 1.6382e+01 (1.0836e+01)	Acc@1   0.00 ( 36.68)	Acc@5 100.00 ( 99.53)
 * Acc@1 36.682 Acc@5 99.533
Epoch: [9][0/1]	Time  0.086 ( 0.086)	Data  0.031 ( 0.031)	Loss 6.8973e+00 (6.8973e+00)	Acc@1  51.33 ( 51.33)	Acc@5  99.33 ( 99.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.8553e+00 (1.8553e+00)	Acc@1  19.53 ( 19.53)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 1.6393e+01 (1.3791e+01)	Acc@1   0.00 ( 31.97)	Acc@5 100.00 ( 96.50)
 * Acc@1 31.970 Acc@5 96.495
Epoch: [10][0/1]	Time  0.088 ( 0.088)	Data  0.035 ( 0.035)	Loss 8.0660e+00 (8.0660e+00)	Acc@1  25.78 ( 25.78)	Acc@5  98.44 ( 98.44)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 5.3432e-01 (5.3432e-01)	Acc@1  82.42 ( 82.42)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.5934e+01 (1.2332e+01)	Acc@1   0.00 ( 42.02)	Acc@5 100.00 ( 96.46)
 * Acc@1 42.017 Acc@5 96.456
Epoch: [11][0/1]	Time  0.091 ( 0.091)	Data  0.034 ( 0.034)	Loss 7.6296e+00 (7.6296e+00)	Acc@1  56.22 ( 56.22)	Acc@5  97.11 ( 97.11)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 2.1806e-05 (2.1806e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 5.4699e+00 (7.5335e+00)	Acc@1   0.00 ( 39.29)	Acc@5 100.00 ( 97.12)
 * Acc@1 39.291 Acc@5 97.118
Epoch: [12][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 5.2292e+00 (5.2292e+00)	Acc@1  55.11 ( 55.11)	Acc@5  97.11 ( 97.11)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 3.4671e-06 (3.4671e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.6305e-03 (8.3414e+00)	Acc@1 100.00 ( 45.99)	Acc@5 100.00 ( 95.87)
 * Acc@1 45.989 Acc@5 95.872
Epoch: [13][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 7.3665e+00 (7.3665e+00)	Acc@1  60.00 ( 60.00)	Acc@5  96.00 ( 96.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.5274e-05 (1.5274e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 2.0174e-01 (1.3263e+01)	Acc@1  87.50 ( 43.03)	Acc@5 100.00 ( 95.44)
 * Acc@1 43.030 Acc@5 95.444
Epoch: [14][0/1]	Time  0.088 ( 0.088)	Data  0.031 ( 0.031)	Loss 9.4780e+00 (9.4780e+00)	Acc@1  58.44 ( 58.44)	Acc@5  95.33 ( 95.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.0220e-06 (2.0220e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 2.1721e+00 (8.5599e+00)	Acc@1  75.00 ( 52.22)	Acc@5 100.00 ( 95.40)
 * Acc@1 52.220 Acc@5 95.405
Epoch: [15][0/1]	Time  0.092 ( 0.092)	Data  0.038 ( 0.038)	Loss 7.9127e+00 (7.9127e+00)	Acc@1  61.56 ( 61.56)	Acc@5  96.22 ( 96.22)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 1.5553e-07 (1.5553e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 4.7509e+00 (9.2215e+00)	Acc@1  50.00 ( 45.05)	Acc@5 100.00 ( 95.68)
 * Acc@1 45.055 Acc@5 95.678
Epoch: [16][0/1]	Time  0.085 ( 0.085)	Data  0.029 ( 0.029)	Loss 8.0904e+00 (8.0904e+00)	Acc@1  58.22 ( 58.22)	Acc@5  96.00 ( 96.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 7.6086e-04 (7.6086e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 7.2030e+00 (6.9508e+00)	Acc@1   0.00 ( 48.95)	Acc@5 100.00 ( 96.46)
 * Acc@1 48.949 Acc@5 96.456
Epoch: [17][0/1]	Time  0.092 ( 0.092)	Data  0.035 ( 0.035)	Loss 6.0521e+00 (6.0521e+00)	Acc@1  59.56 ( 59.56)	Acc@5  96.89 ( 96.89)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 6.4818e-03 (6.4818e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 6.1607e+00 (7.0546e+00)	Acc@1   0.00 ( 41.12)	Acc@5 100.00 ( 97.74)
 * Acc@1 41.121 Acc@5 97.741
Epoch: [18][0/1]	Time  0.090 ( 0.090)	Data  0.033 ( 0.033)	Loss 4.6995e+00 (4.6995e+00)	Acc@1  56.44 ( 56.44)	Acc@5  97.78 ( 97.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 4.4242e-01 (4.4242e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 3.9343e+00 (9.1401e+00)	Acc@1   0.00 ( 43.50)	Acc@5 100.00 ( 98.95)
 * Acc@1 43.497 Acc@5 98.949
Epoch: [19][0/1]	Time  0.089 ( 0.089)	Data  0.034 ( 0.034)	Loss 6.4342e+00 (6.4342e+00)	Acc@1  56.44 ( 56.44)	Acc@5  98.22 ( 98.22)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 4.7768e-01 (4.7768e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 2.6584e-01 (6.4166e+00)	Acc@1  87.50 ( 55.80)	Acc@5 100.00 ( 99.45)
 * Acc@1 55.802 Acc@5 99.455
Epoch: [20][0/1]	Time  0.083 ( 0.083)	Data  0.028 ( 0.028)	Loss 4.2972e+00 (4.2972e+00)	Acc@1  62.44 ( 62.44)	Acc@5  98.89 ( 98.89)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 5.4457e-01 (5.4457e-01)	Acc@1  83.98 ( 83.98)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 1.8216e-04 (5.6675e+00)	Acc@1 100.00 ( 57.20)	Acc@5 100.00 ( 99.42)
 * Acc@1 57.204 Acc@5 99.416
Epoch: [21][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 4.5651e+00 (4.5651e+00)	Acc@1  62.67 ( 62.67)	Acc@5  99.11 ( 99.11)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.2339e-01 (1.2339e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 1.8664e-02 (6.3523e+00)	Acc@1 100.00 ( 58.02)	Acc@5 100.00 ( 99.57)
 * Acc@1 58.022 Acc@5 99.572
Epoch: [22][0/1]	Time  0.090 ( 0.090)	Data  0.034 ( 0.034)	Loss 4.9234e+00 (4.9234e+00)	Acc@1  66.67 ( 66.67)	Acc@5  99.33 ( 99.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.4041e-03 (5.4041e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 6.0176e-01 (5.2863e+00)	Acc@1  87.50 ( 52.57)	Acc@5 100.00 ( 99.84)
 * Acc@1 52.570 Acc@5 99.844
Epoch: [23][0/1]	Time  0.090 ( 0.090)	Data  0.032 ( 0.032)	Loss 4.1813e+00 (4.1813e+00)	Acc@1  63.78 ( 63.78)	Acc@5  99.56 ( 99.56)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 2.2030e-03 (2.2030e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.3978e+00 (5.6495e+00)	Acc@1  62.50 ( 48.52)	Acc@5 100.00 ( 99.96)
 * Acc@1 48.520 Acc@5 99.961
Epoch: [24][0/1]	Time  0.087 ( 0.087)	Data  0.030 ( 0.030)	Loss 3.8450e+00 (3.8450e+00)	Acc@1  60.67 ( 60.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.0512e-03 (1.0512e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.046)	Loss 3.1081e+00 (5.1686e+00)	Acc@1   0.00 ( 52.06)	Acc@5 100.00 ( 99.96)
 * Acc@1 52.064 Acc@5 99.961
Epoch: [25][0/1]	Time  0.087 ( 0.087)	Data  0.029 ( 0.029)	Loss 3.5850e+00 (3.5850e+00)	Acc@1  65.33 ( 65.33)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 9.3053e-02 (9.3053e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 3.8885e+00 (4.9473e+00)	Acc@1  12.50 ( 57.75)	Acc@5 100.00 ( 99.96)
 * Acc@1 57.749 Acc@5 99.961
Epoch: [26][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 3.9206e+00 (3.9206e+00)	Acc@1  68.67 ( 68.67)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 3.7766e-01 (3.7766e-01)	Acc@1  83.98 ( 83.98)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 4.7628e+00 (5.4791e+00)	Acc@1  25.00 ( 52.80)	Acc@5 100.00 (100.00)
 * Acc@1 52.804 Acc@5 100.000
Epoch: [27][0/1]	Time  0.085 ( 0.085)	Data  0.030 ( 0.030)	Loss 4.2982e+00 (4.2982e+00)	Acc@1  66.67 ( 66.67)	Acc@5  99.56 ( 99.56)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.2028e-02 (8.2028e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.0094e+00 (5.5286e+00)	Acc@1  75.00 ( 49.26)	Acc@5 100.00 (100.00)
 * Acc@1 49.260 Acc@5 100.000
Epoch: [28][0/1]	Time  0.086 ( 0.086)	Data  0.030 ( 0.030)	Loss 3.9762e+00 (3.9762e+00)	Acc@1  66.67 ( 66.67)	Acc@5  98.89 ( 98.89)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.2975e-03 (4.2975e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.7721e+00 (4.4213e+00)	Acc@1  12.50 ( 48.52)	Acc@5 100.00 (100.00)
 * Acc@1 48.520 Acc@5 100.000
Epoch: [29][0/1]	Time  0.091 ( 0.091)	Data  0.034 ( 0.034)	Loss 2.7966e+00 (2.7966e+00)	Acc@1  66.22 ( 66.22)	Acc@5  98.89 ( 98.89)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.1184e-02 (2.1184e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 4.8205e+00 (4.3206e+00)	Acc@1   0.00 ( 49.26)	Acc@5 100.00 (100.00)
 * Acc@1 49.260 Acc@5 100.000
Epoch: [30][0/1]	Time  0.082 ( 0.082)	Data  0.028 ( 0.028)	Loss 3.3846e+00 (3.3846e+00)	Acc@1  63.56 ( 63.56)	Acc@5  99.56 ( 99.56)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 6.3998e-03 (6.3998e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 4.8400e+00 (3.6271e+00)	Acc@1   0.00 ( 51.56)	Acc@5 100.00 (100.00)
 * Acc@1 51.558 Acc@5 100.000
Epoch: [31][0/1]	Time  0.089 ( 0.089)	Data  0.034 ( 0.034)	Loss 2.9274e+00 (2.9274e+00)	Acc@1  64.22 ( 64.22)	Acc@5  99.33 ( 99.33)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 5.5176e-04 (5.5176e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 8.1240e-01 (2.7012e+00)	Acc@1  75.00 ( 58.68)	Acc@5 100.00 ( 99.88)
 * Acc@1 58.684 Acc@5 99.883
Epoch: [32][0/1]	Time  0.089 ( 0.089)	Data  0.033 ( 0.033)	Loss 2.0213e+00 (2.0213e+00)	Acc@1  70.67 ( 70.67)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.5894e-03 (2.5894e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.4044e-01 (2.7008e+00)	Acc@1 100.00 ( 60.48)	Acc@5 100.00 ( 99.77)
 * Acc@1 60.475 Acc@5 99.766
Epoch: [33][0/1]	Time  0.090 ( 0.090)	Data  0.036 ( 0.036)	Loss 2.0194e+00 (2.0194e+00)	Acc@1  71.33 ( 71.33)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 3.6048e-02 (3.6048e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 5.9799e-01 (2.7479e+00)	Acc@1  75.00 ( 56.85)	Acc@5 100.00 ( 99.84)
 * Acc@1 56.854 Acc@5 99.844
Epoch: [34][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 2.0371e+00 (2.0371e+00)	Acc@1  70.00 ( 70.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.3242e-01 (1.3242e-01)	Acc@1  94.14 ( 94.14)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 2.3080e+00 (3.2335e+00)	Acc@1   0.00 ( 50.23)	Acc@5 100.00 ( 99.96)
 * Acc@1 50.234 Acc@5 99.961
Epoch: [35][0/1]	Time  0.088 ( 0.088)	Data  0.033 ( 0.033)	Loss 2.3906e+00 (2.3906e+00)	Acc@1  64.00 ( 64.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 6.2685e-02 (6.2685e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 9.9882e-01 (2.7241e+00)	Acc@1  50.00 ( 51.40)	Acc@5 100.00 ( 99.88)
 * Acc@1 51.402 Acc@5 99.883
Epoch: [36][0/1]	Time  0.087 ( 0.087)	Data  0.029 ( 0.029)	Loss 2.0607e+00 (2.0607e+00)	Acc@1  68.00 ( 68.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.5500e-02 (3.5500e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 2.1408e-01 (2.3936e+00)	Acc@1 100.00 ( 61.57)	Acc@5 100.00 ( 99.84)
 * Acc@1 61.565 Acc@5 99.844
Epoch: [37][0/1]	Time  0.090 ( 0.090)	Data  0.033 ( 0.033)	Loss 1.8100e+00 (1.8100e+00)	Acc@1  74.89 ( 74.89)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 1.1589e-03 (1.1589e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 4.6735e-02 (2.4583e+00)	Acc@1 100.00 ( 62.54)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.539 Acc@5 99.961
Epoch: [38][0/1]	Time  0.090 ( 0.090)	Data  0.035 ( 0.035)	Loss 1.8036e+00 (1.8036e+00)	Acc@1  73.56 ( 73.56)	Acc@5  99.56 ( 99.56)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 7.7879e-03 (7.7879e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.7099e-01 (2.2352e+00)	Acc@1 100.00 ( 62.62)	Acc@5 100.00 ( 99.92)
 * Acc@1 62.617 Acc@5 99.922
Epoch: [39][0/1]	Time  0.085 ( 0.085)	Data  0.029 ( 0.029)	Loss 1.6926e+00 (1.6926e+00)	Acc@1  70.89 ( 70.89)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.4016e-02 (2.4016e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 5.2993e-01 (2.0101e+00)	Acc@1  75.00 ( 62.27)	Acc@5 100.00 (100.00)
 * Acc@1 62.266 Acc@5 100.000
Epoch: [40][0/1]	Time  0.091 ( 0.091)	Data  0.034 ( 0.034)	Loss 1.5734e+00 (1.5734e+00)	Acc@1  70.22 ( 70.22)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.7323e-02 (1.7323e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.0170e+00 (1.8995e+00)	Acc@1  50.00 ( 60.16)	Acc@5 100.00 (100.00)
 * Acc@1 60.164 Acc@5 100.000
Epoch: [41][0/1]	Time  0.090 ( 0.090)	Data  0.033 ( 0.033)	Loss 1.4014e+00 (1.4014e+00)	Acc@1  69.78 ( 69.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.046 ( 0.046)	Loss 5.7353e-02 (5.7353e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.7152e+00 (1.7765e+00)	Acc@1  37.50 ( 60.83)	Acc@5 100.00 (100.00)
 * Acc@1 60.826 Acc@5 100.000
Epoch: [42][0/1]	Time  0.084 ( 0.084)	Data  0.030 ( 0.030)	Loss 1.4617e+00 (1.4617e+00)	Acc@1  70.22 ( 70.22)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 9.6547e-02 (9.6547e-02)	Acc@1  96.48 ( 96.48)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 3.6456e-01 (1.5764e+00)	Acc@1  87.50 ( 65.34)	Acc@5 100.00 (100.00)
 * Acc@1 65.343 Acc@5 100.000
Epoch: [43][0/1]	Time  0.088 ( 0.088)	Data  0.031 ( 0.031)	Loss 1.3939e+00 (1.3939e+00)	Acc@1  72.67 ( 72.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 4.8591e-02 (4.8591e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.9912e-01 (1.3074e+00)	Acc@1 100.00 ( 68.34)	Acc@5 100.00 (100.00)
 * Acc@1 68.341 Acc@5 100.000
Epoch: [44][0/1]	Time  0.092 ( 0.092)	Data  0.036 ( 0.036)	Loss 1.2534e+00 (1.2534e+00)	Acc@1  73.11 ( 73.11)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.6451e-02 (4.6451e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 5.8605e-01 (1.3075e+00)	Acc@1  87.50 ( 66.08)	Acc@5 100.00 (100.00)
 * Acc@1 66.083 Acc@5 100.000
Epoch: [45][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 1.0336e+00 (1.0336e+00)	Acc@1  72.67 ( 72.67)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.5465e-02 (2.5465e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 3.6008e-01 (1.5845e+00)	Acc@1  87.50 ( 62.58)	Acc@5 100.00 (100.00)
 * Acc@1 62.578 Acc@5 100.000
Epoch: [46][0/1]	Time  0.095 ( 0.095)	Data  0.040 ( 0.040)	Loss 1.1316e+00 (1.1316e+00)	Acc@1  69.56 ( 69.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.3424e-02 (3.3424e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.046)	Loss 8.0614e-01 (1.4593e+00)	Acc@1  50.00 ( 61.25)	Acc@5 100.00 (100.00)
 * Acc@1 61.254 Acc@5 100.000
Epoch: [47][0/1]	Time  0.092 ( 0.092)	Data  0.035 ( 0.035)	Loss 1.1366e+00 (1.1366e+00)	Acc@1  70.89 ( 70.89)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 2.5679e-02 (2.5679e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.047)	Loss 8.1352e-01 (1.4262e+00)	Acc@1  62.50 ( 61.72)	Acc@5 100.00 (100.00)
 * Acc@1 61.721 Acc@5 100.000
Epoch: [48][0/1]	Time  0.087 ( 0.087)	Data  0.030 ( 0.030)	Loss 1.0275e+00 (1.0275e+00)	Acc@1  72.67 ( 72.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 4.8003e-02 (4.8003e-02)	Acc@1  97.27 ( 97.27)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 3.4878e-01 (1.5429e+00)	Acc@1  87.50 ( 61.02)	Acc@5 100.00 ( 99.84)
 * Acc@1 61.020 Acc@5 99.844
Epoch: [49][0/1]	Time  0.084 ( 0.084)	Data  0.030 ( 0.030)	Loss 1.0745e+00 (1.0745e+00)	Acc@1  73.56 ( 73.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.9890e-02 (2.9890e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 5.1898e-01 (1.6040e+00)	Acc@1  62.50 ( 59.81)	Acc@5 100.00 ( 99.88)
 * Acc@1 59.813 Acc@5 99.883
Epoch: [50][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 1.0321e+00 (1.0321e+00)	Acc@1  75.11 ( 75.11)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.4064e-02 (3.4064e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 6.8101e-01 (1.4884e+00)	Acc@1  75.00 ( 61.25)	Acc@5 100.00 ( 99.84)
 * Acc@1 61.254 Acc@5 99.844
Epoch: [51][0/1]	Time  0.088 ( 0.088)	Data  0.030 ( 0.030)	Loss 1.0437e+00 (1.0437e+00)	Acc@1  71.78 ( 71.78)	Acc@5  99.78 ( 99.78)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.4212e-02 (4.4212e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.047)	Loss 9.0644e-01 (1.3545e+00)	Acc@1  75.00 ( 61.84)	Acc@5 100.00 (100.00)
 * Acc@1 61.838 Acc@5 100.000
Epoch: [52][0/1]	Time  0.088 ( 0.088)	Data  0.033 ( 0.033)	Loss 9.2708e-01 (9.2708e-01)	Acc@1  73.11 ( 73.11)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 4.6459e-02 (4.6459e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 6.0468e-01 (1.2271e+00)	Acc@1  75.00 ( 63.59)	Acc@5 100.00 (100.00)
 * Acc@1 63.590 Acc@5 100.000
Epoch: [53][0/1]	Time  0.084 ( 0.084)	Data  0.029 ( 0.029)	Loss 7.8547e-01 (7.8547e-01)	Acc@1  75.11 ( 75.11)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 6.6324e-02 (6.6324e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 1.4454e+00 (1.2784e+00)	Acc@1  37.50 ( 60.44)	Acc@5 100.00 (100.00)
 * Acc@1 60.436 Acc@5 100.000
Epoch: [54][0/1]	Time  0.091 ( 0.091)	Data  0.033 ( 0.033)	Loss 9.4844e-01 (9.4844e-01)	Acc@1  72.44 ( 72.44)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 4.7756e-02 (4.7756e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 9.7858e-01 (1.2235e+00)	Acc@1  62.50 ( 61.95)	Acc@5 100.00 (100.00)
 * Acc@1 61.955 Acc@5 100.000
Epoch: [55][0/1]	Time  0.091 ( 0.091)	Data  0.033 ( 0.033)	Loss 9.1496e-01 (9.1496e-01)	Acc@1  72.44 ( 72.44)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.8477e-02 (1.8477e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 4.7082e-01 (1.1133e+00)	Acc@1  87.50 ( 65.11)	Acc@5 100.00 (100.00)
 * Acc@1 65.109 Acc@5 100.000
Epoch: [56][0/1]	Time  0.087 ( 0.087)	Data  0.029 ( 0.029)	Loss 8.6499e-01 (8.6499e-01)	Acc@1  71.33 ( 71.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.6118e-02 (2.6118e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 3.4658e-01 (1.1422e+00)	Acc@1  87.50 ( 65.54)	Acc@5 100.00 (100.00)
 * Acc@1 65.537 Acc@5 100.000
Epoch: [57][0/1]	Time  0.082 ( 0.082)	Data  0.028 ( 0.028)	Loss 9.7902e-01 (9.7902e-01)	Acc@1  72.22 ( 72.22)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 4.9740e-02 (4.9740e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 3.8365e-01 (1.0610e+00)	Acc@1  87.50 ( 67.33)	Acc@5 100.00 (100.00)
 * Acc@1 67.329 Acc@5 100.000
Epoch: [58][0/1]	Time  0.090 ( 0.090)	Data  0.034 ( 0.034)	Loss 8.0230e-01 (8.0230e-01)	Acc@1  75.56 ( 75.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 3.4245e-02 (3.4245e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.045)	Loss 8.7198e-01 (1.0308e+00)	Acc@1  75.00 ( 67.76)	Acc@5 100.00 (100.00)
 * Acc@1 67.757 Acc@5 100.000
Epoch: [59][0/1]	Time  0.087 ( 0.087)	Data  0.032 ( 0.032)	Loss 7.6486e-01 (7.6486e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.7594e-02 (8.7594e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 8.0998e-01 (9.6195e-01)	Acc@1  62.50 ( 67.83)	Acc@5 100.00 (100.00)
 * Acc@1 67.835 Acc@5 100.000
Epoch: [60][0/1]	Time  0.093 ( 0.093)	Data  0.035 ( 0.035)	Loss 8.0617e-01 (8.0617e-01)	Acc@1  74.00 ( 74.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.056 ( 0.056)	Loss 4.0508e-02 (4.0508e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.2761e+00 (1.0096e+00)	Acc@1  50.00 ( 65.69)	Acc@5 100.00 (100.00)
 * Acc@1 65.693 Acc@5 100.000
Epoch: [61][0/1]	Time  0.086 ( 0.086)	Data  0.029 ( 0.029)	Loss 7.6603e-01 (7.6603e-01)	Acc@1  76.89 ( 76.89)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 6.9036e-02 (6.9036e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 9.4232e-01 (9.8367e-01)	Acc@1  62.50 ( 66.47)	Acc@5 100.00 (100.00)
 * Acc@1 66.472 Acc@5 100.000
Epoch: [62][0/1]	Time  0.090 ( 0.090)	Data  0.034 ( 0.034)	Loss 7.2619e-01 (7.2619e-01)	Acc@1  76.89 ( 76.89)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 5.1713e-02 (5.1713e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 9.6929e-01 (9.9002e-01)	Acc@1  50.00 ( 65.85)	Acc@5 100.00 (100.00)
 * Acc@1 65.849 Acc@5 100.000
Epoch: [63][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 8.0033e-01 (8.0033e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.7801e-02 (5.7801e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 5.1012e-01 (9.8849e-01)	Acc@1  75.00 ( 66.28)	Acc@5 100.00 (100.00)
 * Acc@1 66.277 Acc@5 100.000
Epoch: [64][0/1]	Time  0.089 ( 0.089)	Data  0.032 ( 0.032)	Loss 7.3604e-01 (7.3604e-01)	Acc@1  75.78 ( 75.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.4351e-02 (4.4351e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 6.1852e-01 (9.9730e-01)	Acc@1  62.50 ( 66.67)	Acc@5 100.00 (100.00)
 * Acc@1 66.667 Acc@5 100.000
Epoch: [65][0/1]	Time  0.085 ( 0.085)	Data  0.029 ( 0.029)	Loss 7.1578e-01 (7.1578e-01)	Acc@1  76.67 ( 76.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 2.9077e-02 (2.9077e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.8751e-01 (9.5915e-01)	Acc@1 100.00 ( 66.59)	Acc@5 100.00 (100.00)
 * Acc@1 66.589 Acc@5 100.000
Epoch: [66][0/1]	Time  0.091 ( 0.091)	Data  0.033 ( 0.033)	Loss 7.2787e-01 (7.2787e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.6580e-02 (1.6580e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 2.5199e-01 (1.0225e+00)	Acc@1 100.00 ( 66.55)	Acc@5 100.00 (100.00)
 * Acc@1 66.550 Acc@5 100.000
Epoch: [67][0/1]	Time  0.087 ( 0.087)	Data  0.032 ( 0.032)	Loss 8.3512e-01 (8.3512e-01)	Acc@1  74.67 ( 74.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 4.9232e-02 (4.9232e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.047)	Loss 3.0054e-01 (1.0020e+00)	Acc@1 100.00 ( 67.48)	Acc@5 100.00 (100.00)
 * Acc@1 67.484 Acc@5 100.000
Epoch: [68][0/1]	Time  0.092 ( 0.092)	Data  0.035 ( 0.035)	Loss 7.2291e-01 (7.2291e-01)	Acc@1  77.56 ( 77.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 5.6454e-02 (5.6454e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 4.8031e-01 (9.9048e-01)	Acc@1  87.50 ( 66.71)	Acc@5 100.00 (100.00)
 * Acc@1 66.706 Acc@5 100.000
Epoch: [69][0/1]	Time  0.091 ( 0.091)	Data  0.035 ( 0.035)	Loss 7.2854e-01 (7.2854e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.6483e-02 (3.6483e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 4.5564e-01 (9.6131e-01)	Acc@1  87.50 ( 67.21)	Acc@5 100.00 (100.00)
 * Acc@1 67.212 Acc@5 100.000
Epoch: [70][0/1]	Time  0.088 ( 0.088)	Data  0.033 ( 0.033)	Loss 7.2063e-01 (7.2063e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.0489e-02 (2.0489e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 7.4429e-01 (9.6834e-01)	Acc@1  87.50 ( 67.13)	Acc@5 100.00 (100.00)
 * Acc@1 67.134 Acc@5 100.000
Epoch: [71][0/1]	Time  0.083 ( 0.083)	Data  0.027 ( 0.027)	Loss 7.6180e-01 (7.6180e-01)	Acc@1  74.67 ( 74.67)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
.\main_lincls.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
.\main_lincls.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained, map_location="cpu")
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
.\main_lincls.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_weights, map_location="cpu")
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.1308e-02 (3.1308e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 3.9788e-01 (9.6499e-01)	Acc@1  87.50 ( 66.59)	Acc@5 100.00 (100.00)
 * Acc@1 66.589 Acc@5 100.000
Epoch: [72][0/1]	Time  0.083 ( 0.083)	Data  0.029 ( 0.029)	Loss 7.6113e-01 (7.6113e-01)	Acc@1  77.56 ( 77.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 5.1248e-02 (5.1248e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 8.9448e-01 (9.4849e-01)	Acc@1  62.50 ( 66.08)	Acc@5 100.00 (100.00)
 * Acc@1 66.083 Acc@5 100.000
Epoch: [73][0/1]	Time  0.084 ( 0.084)	Data  0.030 ( 0.030)	Loss 7.4386e-01 (7.4386e-01)	Acc@1  75.78 ( 75.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.0074e-02 (3.0074e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 9.8481e-01 (9.4985e-01)	Acc@1  37.50 ( 66.43)	Acc@5 100.00 (100.00)
 * Acc@1 66.433 Acc@5 100.000
Epoch: [74][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 7.2188e-01 (7.2188e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.0096e-02 (3.0096e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 6.3872e-01 (9.6087e-01)	Acc@1  87.50 ( 65.89)	Acc@5 100.00 (100.00)
 * Acc@1 65.888 Acc@5 100.000
Epoch: [75][0/1]	Time  0.083 ( 0.083)	Data  0.026 ( 0.026)	Loss 7.3563e-01 (7.3563e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.7533e-02 (3.7533e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 6.3142e-01 (9.2985e-01)	Acc@1  87.50 ( 65.77)	Acc@5 100.00 (100.00)
 * Acc@1 65.771 Acc@5 100.000
Epoch: [76][0/1]	Time  0.087 ( 0.087)	Data  0.031 ( 0.031)	Loss 7.3733e-01 (7.3733e-01)	Acc@1  75.56 ( 75.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.0870e-02 (3.0870e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 5.7129e-01 (9.5729e-01)	Acc@1  87.50 ( 66.86)	Acc@5 100.00 (100.00)
 * Acc@1 66.861 Acc@5 100.000
Epoch: [77][0/1]	Time  0.089 ( 0.089)	Data  0.034 ( 0.034)	Loss 6.9076e-01 (6.9076e-01)	Acc@1  75.78 ( 75.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 6.0144e-02 (6.0144e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.046)	Loss 3.6207e-01 (9.3101e-01)	Acc@1 100.00 ( 66.47)	Acc@5 100.00 (100.00)
 * Acc@1 66.472 Acc@5 100.000
Epoch: [78][0/1]	Time  0.087 ( 0.087)	Data  0.030 ( 0.030)	Loss 7.1630e-01 (7.1630e-01)	Acc@1  75.78 ( 75.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 6.8234e-02 (6.8234e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 4.7127e-01 (9.2983e-01)	Acc@1 100.00 ( 66.98)	Acc@5 100.00 (100.00)
 * Acc@1 66.978 Acc@5 100.000
Epoch: [79][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 6.8257e-01 (6.8257e-01)	Acc@1  78.00 ( 78.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.5615e-02 (3.5615e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 5.9719e-01 (9.4834e-01)	Acc@1  75.00 ( 66.28)	Acc@5 100.00 (100.00)
 * Acc@1 66.277 Acc@5 100.000
Epoch: [80][0/1]	Time  0.087 ( 0.087)	Data  0.033 ( 0.033)	Loss 7.5120e-01 (7.5120e-01)	Acc@1  76.22 ( 76.22)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.7043e-02 (3.7043e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 6.9845e-01 (9.0589e-01)	Acc@1  87.50 ( 67.13)	Acc@5 100.00 (100.00)
 * Acc@1 67.134 Acc@5 100.000
Epoch: [81][0/1]	Time  0.087 ( 0.087)	Data  0.032 ( 0.032)	Loss 7.0941e-01 (7.0941e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 4.5232e-02 (4.5232e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 6.0886e-01 (9.1775e-01)	Acc@1  75.00 ( 66.04)	Acc@5 100.00 (100.00)
 * Acc@1 66.044 Acc@5 100.000
Epoch: [82][0/1]	Time  0.087 ( 0.087)	Data  0.031 ( 0.031)	Loss 7.1009e-01 (7.1009e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.0731e-02 (3.0731e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 8.0040e-01 (9.2185e-01)	Acc@1  62.50 ( 66.04)	Acc@5 100.00 (100.00)
 * Acc@1 66.044 Acc@5 100.000
Epoch: [83][0/1]	Time  0.090 ( 0.090)	Data  0.034 ( 0.034)	Loss 8.0761e-01 (8.0761e-01)	Acc@1  76.22 ( 76.22)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 3.2659e-02 (3.2659e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 5.0536e-01 (9.1702e-01)	Acc@1  87.50 ( 66.67)	Acc@5 100.00 (100.00)
 * Acc@1 66.667 Acc@5 100.000
Epoch: [84][0/1]	Time  0.084 ( 0.084)	Data  0.031 ( 0.031)	Loss 7.1403e-01 (7.1403e-01)	Acc@1  77.56 ( 77.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 3.1060e-02 (3.1060e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 5.9430e-01 (9.3462e-01)	Acc@1  75.00 ( 66.28)	Acc@5 100.00 (100.00)
 * Acc@1 66.277 Acc@5 100.000
Epoch: [85][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 7.1472e-01 (7.1472e-01)	Acc@1  77.33 ( 77.33)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 3.3955e-02 (3.3955e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 7.8480e-01 (9.2018e-01)	Acc@1  75.00 ( 66.39)	Acc@5 100.00 (100.00)
 * Acc@1 66.394 Acc@5 100.000
Epoch: [86][0/1]	Time  0.088 ( 0.088)	Data  0.031 ( 0.031)	Loss 6.6848e-01 (6.6848e-01)	Acc@1  77.78 ( 77.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 3.1203e-02 (3.1203e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 3.5632e-01 (9.0801e-01)	Acc@1  87.50 ( 67.17)	Acc@5 100.00 (100.00)
 * Acc@1 67.173 Acc@5 100.000
Epoch: [87][0/1]	Time  0.088 ( 0.088)	Data  0.032 ( 0.032)	Loss 7.4526e-01 (7.4526e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 7.0600e-02 (7.0600e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 4.3413e-01 (9.2759e-01)	Acc@1  87.50 ( 66.47)	Acc@5 100.00 (100.00)
 * Acc@1 66.472 Acc@5 100.000
Epoch: [88][0/1]	Time  0.089 ( 0.089)	Data  0.035 ( 0.035)	Loss 6.9988e-01 (6.9988e-01)	Acc@1  77.78 ( 77.78)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 6.6461e-02 (6.6461e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 9.2081e-01 (9.1041e-01)	Acc@1  37.50 ( 66.90)	Acc@5 100.00 (100.00)
 * Acc@1 66.900 Acc@5 100.000
Epoch: [89][0/1]	Time  0.088 ( 0.088)	Data  0.031 ( 0.031)	Loss 7.6228e-01 (7.6228e-01)	Acc@1  75.56 ( 75.56)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:50
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 3.5561e-02 (3.5561e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 5.9216e-01 (9.3156e-01)	Acc@1  87.50 ( 66.08)	Acc@5 100.00 (100.00)
 * Acc@1 66.083 Acc@5 100.000
Running with excep_size=25 
Use GPU: 0 for training
=> creating model 'resnet50'
SimSiam(
  (encoder): ResNet(
    (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=512, bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=512, out_features=2048, bias=True)
      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (predictor): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=2048, bias=True)
  )
)
Epoch: [0][0/2]	Time  1.606 ( 1.606)	Data  0.089 ( 0.089)	Loss -0.0135 (-0.0135)
Epoch: [1][0/2]	Time  0.135 ( 0.135)	Data  0.060 ( 0.060)	Loss -0.0977 (-0.0977)
Epoch: [2][0/2]	Time  0.133 ( 0.133)	Data  0.057 ( 0.057)	Loss -0.1896 (-0.1896)
Epoch: [3][0/2]	Time  0.133 ( 0.133)	Data  0.056 ( 0.056)	Loss -0.2875 (-0.2875)
Epoch: [4][0/2]	Time  0.133 ( 0.133)	Data  0.057 ( 0.057)	Loss -0.3841 (-0.3841)
Epoch: [5][0/2]	Time  0.132 ( 0.132)	Data  0.057 ( 0.057)	Loss -0.4941 (-0.4941)
Epoch: [6][0/2]	Time  0.133 ( 0.133)	Data  0.058 ( 0.058)	Loss -0.5817 (-0.5817)
Epoch: [7][0/2]	Time  0.135 ( 0.135)	Data  0.058 ( 0.058)	Loss -0.6526 (-0.6526)
Epoch: [8][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.7010 (-0.7010)
Epoch: [9][0/2]	Time  0.170 ( 0.170)	Data  0.058 ( 0.058)	Loss -0.7283 (-0.7283)
Epoch: [10][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.7342 (-0.7342)
Epoch: [11][0/2]	Time  0.204 ( 0.204)	Data  0.056 ( 0.056)	Loss -0.7420 (-0.7420)
Epoch: [12][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.7635 (-0.7635)
Epoch: [13][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.7772 (-0.7772)
Epoch: [14][0/2]	Time  0.181 ( 0.181)	Data  0.059 ( 0.059)	Loss -0.7684 (-0.7684)
Epoch: [15][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.7774 (-0.7774)
Epoch: [16][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.7880 (-0.7880)
Epoch: [17][0/2]	Time  0.188 ( 0.188)	Data  0.056 ( 0.056)	Loss -0.7876 (-0.7876)
Epoch: [18][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.7698 (-0.7698)
Epoch: [19][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.7622 (-0.7622)
Epoch: [20][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.7618 (-0.7618)
Epoch: [21][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.7634 (-0.7634)
Epoch: [22][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.7627 (-0.7627)
Epoch: [23][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.7583 (-0.7583)
Epoch: [24][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.7709 (-0.7709)
Epoch: [25][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.7762 (-0.7762)
Epoch: [26][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.7809 (-0.7809)
Epoch: [27][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.7911 (-0.7911)
Epoch: [28][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8063 (-0.8063)
Epoch: [29][0/2]	Time  0.182 ( 0.182)	Data  0.056 ( 0.056)	Loss -0.8074 (-0.8074)
Epoch: [30][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.8152 (-0.8152)
Epoch: [31][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.8159 (-0.8159)
Epoch: [32][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.8109 (-0.8109)
Epoch: [33][0/2]	Time  0.202 ( 0.202)	Data  0.056 ( 0.056)	Loss -0.8146 (-0.8146)
Epoch: [34][0/2]	Time  0.186 ( 0.186)	Data  0.057 ( 0.057)	Loss -0.8202 (-0.8202)
Epoch: [35][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.8061 (-0.8061)
Epoch: [36][0/2]	Time  0.180 ( 0.180)	Data  0.057 ( 0.057)	Loss -0.7987 (-0.7987)
Epoch: [37][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.7979 (-0.7979)
Epoch: [38][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.7988 (-0.7988)
Epoch: [39][0/2]	Time  0.201 ( 0.201)	Data  0.062 ( 0.062)	Loss -0.8085 (-0.8085)
Epoch: [40][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.8102 (-0.8102)
Epoch: [41][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.8228 (-0.8228)
Epoch: [42][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.8232 (-0.8232)
Epoch: [43][0/2]	Time  0.187 ( 0.187)	Data  0.062 ( 0.062)	Loss -0.8397 (-0.8397)
Epoch: [44][0/2]	Time  0.199 ( 0.199)	Data  0.057 ( 0.057)	Loss -0.8498 (-0.8498)
Epoch: [45][0/2]	Time  0.173 ( 0.173)	Data  0.057 ( 0.057)	Loss -0.8465 (-0.8465)
Epoch: [46][0/2]	Time  0.192 ( 0.192)	Data  0.056 ( 0.056)	Loss -0.8566 (-0.8566)
Epoch: [47][0/2]	Time  0.201 ( 0.201)	Data  0.056 ( 0.056)	Loss -0.8492 (-0.8492)
Epoch: [48][0/2]	Time  0.186 ( 0.186)	Data  0.057 ( 0.057)	Loss -0.8688 (-0.8688)
Epoch: [49][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.8641 (-0.8641)
Epoch: [50][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.8674 (-0.8674)
Epoch: [51][0/2]	Time  0.195 ( 0.195)	Data  0.056 ( 0.056)	Loss -0.8541 (-0.8541)
Epoch: [52][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8579 (-0.8579)
Epoch: [53][0/2]	Time  0.184 ( 0.184)	Data  0.057 ( 0.057)	Loss -0.8564 (-0.8564)
Epoch: [54][0/2]	Time  0.208 ( 0.208)	Data  0.062 ( 0.062)	Loss -0.8391 (-0.8391)
Epoch: [55][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.8396 (-0.8396)
Epoch: [56][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.8396 (-0.8396)
Epoch: [57][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.8327 (-0.8327)
Epoch: [58][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.8519 (-0.8519)
Epoch: [59][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8555 (-0.8555)
Epoch: [60][0/2]	Time  0.177 ( 0.177)	Data  0.057 ( 0.057)	Loss -0.8678 (-0.8678)
Epoch: [61][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8538 (-0.8538)
Epoch: [62][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.8552 (-0.8552)
Epoch: [63][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.8755 (-0.8755)
Epoch: [64][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.8775 (-0.8775)
Epoch: [65][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.8891 (-0.8891)
Epoch: [66][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8692 (-0.8692)
Epoch: [67][0/2]	Time  0.176 ( 0.176)	Data  0.057 ( 0.057)	Loss -0.8846 (-0.8846)
Epoch: [68][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8876 (-0.8876)
Epoch: [69][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.8831 (-0.8831)
Epoch: [70][0/2]	Time  0.185 ( 0.185)	Data  0.056 ( 0.056)	Loss -0.8754 (-0.8754)
Epoch: [71][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8913 (-0.8913)
Epoch: [72][0/2]	Time  0.179 ( 0.179)	Data  0.056 ( 0.056)	Loss -0.8951 (-0.8951)
Epoch: [73][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.8949 (-0.8949)
Epoch: [74][0/2]	Time  0.207 ( 0.207)	Data  0.060 ( 0.060)	Loss -0.8911 (-0.8911)
Epoch: [75][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.8952 (-0.8952)
Epoch: [76][0/2]	Time  0.216 ( 0.216)	Data  0.057 ( 0.057)	Loss -0.8916 (-0.8916)
Epoch: [77][0/2]	Time  0.221 ( 0.221)	Data  0.058 ( 0.058)	Loss -0.8832 (-0.8832)
Epoch: [78][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.8972 (-0.8972)
Epoch: [79][0/2]	Time  0.188 ( 0.188)	Data  0.057 ( 0.057)	Loss -0.8968 (-0.8968)
Epoch: [80][0/2]	Time  0.217 ( 0.217)	Data  0.057 ( 0.057)	Loss -0.8984 (-0.8984)
Epoch: [81][0/2]	Time  0.214 ( 0.214)	Data  0.058 ( 0.058)	Loss -0.9015 (-0.9015)
Epoch: [82][0/2]	Time  0.210 ( 0.210)	Data  0.059 ( 0.059)	Loss -0.8986 (-0.8986)
Epoch: [83][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.8988 (-0.8988)
Epoch: [84][0/2]	Time  0.213 ( 0.213)	Data  0.057 ( 0.057)	Loss -0.8801 (-0.8801)
Epoch: [85][0/2]	Time  0.218 ( 0.218)	Data  0.058 ( 0.058)	Loss -0.8846 (-0.8846)
Epoch: [86][0/2]	Time  0.191 ( 0.191)	Data  0.057 ( 0.057)	Loss -0.9002 (-0.9002)
Epoch: [87][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9069 (-0.9069)
Epoch: [88][0/2]	Time  0.218 ( 0.218)	Data  0.060 ( 0.060)	Loss -0.9001 (-0.9001)
Epoch: [89][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.8991 (-0.8991)
Epoch: [90][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9010 (-0.9010)
Epoch: [91][0/2]	Time  0.218 ( 0.218)	Data  0.060 ( 0.060)	Loss -0.9058 (-0.9058)
Epoch: [92][0/2]	Time  0.210 ( 0.210)	Data  0.057 ( 0.057)	Loss -0.9015 (-0.9015)
Epoch: [93][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9004 (-0.9004)
Epoch: [94][0/2]	Time  0.216 ( 0.216)	Data  0.061 ( 0.061)	Loss -0.8978 (-0.8978)
Epoch: [95][0/2]	Time  0.223 ( 0.223)	Data  0.061 ( 0.061)	Loss -0.9030 (-0.9030)
Epoch: [96][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.9049 (-0.9049)
Epoch: [97][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9068 (-0.9068)
Epoch: [98][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.8944 (-0.8944)
Epoch: [99][0/2]	Time  0.213 ( 0.213)	Data  0.058 ( 0.058)	Loss -0.8972 (-0.8972)
Epoch: [100][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9085 (-0.9085)
Epoch: [101][0/2]	Time  0.229 ( 0.229)	Data  0.064 ( 0.064)	Loss -0.9142 (-0.9142)
Epoch: [102][0/2]	Time  0.211 ( 0.211)	Data  0.058 ( 0.058)	Loss -0.9067 (-0.9067)
Epoch: [103][0/2]	Time  0.201 ( 0.201)	Data  0.061 ( 0.061)	Loss -0.9050 (-0.9050)
Epoch: [104][0/2]	Time  0.212 ( 0.212)	Data  0.060 ( 0.060)	Loss -0.8954 (-0.8954)
Epoch: [105][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.8990 (-0.8990)
Epoch: [106][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9081 (-0.9081)
Epoch: [107][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.9020 (-0.9020)
Epoch: [108][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9102 (-0.9102)
Epoch: [109][0/2]	Time  0.174 ( 0.174)	Data  0.057 ( 0.057)	Loss -0.8986 (-0.8986)
Epoch: [110][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9085 (-0.9085)
Epoch: [111][0/2]	Time  0.199 ( 0.199)	Data  0.057 ( 0.057)	Loss -0.9026 (-0.9026)
Epoch: [112][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.9061 (-0.9061)
Epoch: [113][0/2]	Time  0.207 ( 0.207)	Data  0.059 ( 0.059)	Loss -0.9031 (-0.9031)
Epoch: [114][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.8983 (-0.8983)
Epoch: [115][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9151 (-0.9151)
Epoch: [116][0/2]	Time  0.171 ( 0.171)	Data  0.057 ( 0.057)	Loss -0.8982 (-0.8982)
Epoch: [117][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9091 (-0.9091)
Epoch: [118][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9026 (-0.9026)
Epoch: [119][0/2]	Time  0.184 ( 0.184)	Data  0.056 ( 0.056)	Loss -0.9020 (-0.9020)
Epoch: [120][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9009 (-0.9009)
Epoch: [121][0/2]	Time  0.180 ( 0.180)	Data  0.057 ( 0.057)	Loss -0.9030 (-0.9030)
Epoch: [122][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9041 (-0.9041)
Epoch: [123][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9072 (-0.9072)
Epoch: [124][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9117 (-0.9117)
Epoch: [125][0/2]	Time  0.197 ( 0.197)	Data  0.056 ( 0.056)	Loss -0.8976 (-0.8976)
Epoch: [126][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.9132 (-0.9132)
Epoch: [127][0/2]	Time  0.193 ( 0.193)	Data  0.057 ( 0.057)	Loss -0.9114 (-0.9114)
.\main_simsiam.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
Epoch: [128][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9040 (-0.9040)
Epoch: [129][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9088 (-0.9088)
Epoch: [130][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.8957 (-0.8957)
Epoch: [131][0/2]	Time  0.177 ( 0.177)	Data  0.057 ( 0.057)	Loss -0.9155 (-0.9155)
Epoch: [132][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9089 (-0.9089)
Epoch: [133][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9033 (-0.9033)
Epoch: [134][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9148 (-0.9148)
Epoch: [135][0/2]	Time  0.199 ( 0.199)	Data  0.059 ( 0.059)	Loss -0.9124 (-0.9124)
Epoch: [136][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.9113 (-0.9113)
Epoch: [137][0/2]	Time  0.201 ( 0.201)	Data  0.056 ( 0.056)	Loss -0.9076 (-0.9076)
Epoch: [138][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9022 (-0.9022)
Epoch: [139][0/2]	Time  0.200 ( 0.200)	Data  0.056 ( 0.056)	Loss -0.9203 (-0.9203)
Epoch: [140][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9036 (-0.9036)
Epoch: [141][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9176 (-0.9176)
Epoch: [142][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9057 (-0.9057)
Epoch: [143][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9048 (-0.9048)
Epoch: [144][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9051 (-0.9051)
Epoch: [145][0/2]	Time  0.173 ( 0.173)	Data  0.058 ( 0.058)	Loss -0.9149 (-0.9149)
Epoch: [146][0/2]	Time  0.192 ( 0.192)	Data  0.056 ( 0.056)	Loss -0.9159 (-0.9159)
Epoch: [147][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9139 (-0.9139)
Epoch: [148][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9105 (-0.9105)
Epoch: [149][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9128 (-0.9128)
Epoch: [150][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9078 (-0.9078)
Epoch: [151][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9065 (-0.9065)
Epoch: [152][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9083 (-0.9083)
Epoch: [153][0/2]	Time  0.188 ( 0.188)	Data  0.057 ( 0.057)	Loss -0.9145 (-0.9145)
Epoch: [154][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9106 (-0.9106)
Epoch: [155][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9160 (-0.9160)
Epoch: [156][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9185 (-0.9185)
Epoch: [157][0/2]	Time  0.174 ( 0.174)	Data  0.056 ( 0.056)	Loss -0.9091 (-0.9091)
Epoch: [158][0/2]	Time  0.192 ( 0.192)	Data  0.057 ( 0.057)	Loss -0.9014 (-0.9014)
Epoch: [159][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9086 (-0.9086)
Epoch: [160][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.9165 (-0.9165)
Epoch: [161][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9114 (-0.9114)
Epoch: [162][0/2]	Time  0.176 ( 0.176)	Data  0.057 ( 0.057)	Loss -0.9242 (-0.9242)
Epoch: [163][0/2]	Time  0.195 ( 0.195)	Data  0.057 ( 0.057)	Loss -0.9143 (-0.9143)
Epoch: [164][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9264 (-0.9264)
Epoch: [165][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9231 (-0.9231)
Epoch: [166][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9096 (-0.9096)
Epoch: [167][0/2]	Time  0.181 ( 0.181)	Data  0.057 ( 0.057)	Loss -0.9193 (-0.9193)
Epoch: [168][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9092 (-0.9092)
Epoch: [169][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9137 (-0.9137)
Epoch: [170][0/2]	Time  0.192 ( 0.192)	Data  0.057 ( 0.057)	Loss -0.9104 (-0.9104)
Epoch: [171][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9095 (-0.9095)
Epoch: [172][0/2]	Time  0.184 ( 0.184)	Data  0.057 ( 0.057)	Loss -0.9259 (-0.9259)
Epoch: [173][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9257 (-0.9257)
Epoch: [174][0/2]	Time  0.174 ( 0.174)	Data  0.058 ( 0.058)	Loss -0.9171 (-0.9171)
Epoch: [175][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9029 (-0.9029)
Epoch: [176][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9153 (-0.9153)
Epoch: [177][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9074 (-0.9074)
Epoch: [178][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9174 (-0.9174)
Epoch: [179][0/2]	Time  0.180 ( 0.180)	Data  0.057 ( 0.057)	Loss -0.9165 (-0.9165)
Epoch: [180][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9138 (-0.9138)
Epoch: [181][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9079 (-0.9079)
Epoch: [182][0/2]	Time  0.192 ( 0.192)	Data  0.064 ( 0.064)	Loss -0.9209 (-0.9209)
Epoch: [183][0/2]	Time  0.205 ( 0.205)	Data  0.060 ( 0.060)	Loss -0.8996 (-0.8996)
Epoch: [184][0/2]	Time  0.214 ( 0.214)	Data  0.076 ( 0.076)	Loss -0.9171 (-0.9171)
Epoch: [185][0/2]	Time  0.203 ( 0.203)	Data  0.057 ( 0.057)	Loss -0.9180 (-0.9180)
Epoch: [186][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.9172 (-0.9172)
Epoch: [187][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9153 (-0.9153)
Epoch: [188][0/2]	Time  0.177 ( 0.177)	Data  0.057 ( 0.057)	Loss -0.9187 (-0.9187)
Epoch: [189][0/2]	Time  0.193 ( 0.193)	Data  0.057 ( 0.057)	Loss -0.9267 (-0.9267)
Epoch: [190][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9222 (-0.9222)
Epoch: [191][0/2]	Time  0.193 ( 0.193)	Data  0.057 ( 0.057)	Loss -0.9166 (-0.9166)
Epoch: [192][0/2]	Time  0.196 ( 0.196)	Data  0.056 ( 0.056)	Loss -0.9117 (-0.9117)
Epoch: [193][0/2]	Time  0.178 ( 0.178)	Data  0.057 ( 0.057)	Loss -0.9248 (-0.9248)
Epoch: [194][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9167 (-0.9167)
Epoch: [195][0/2]	Time  0.201 ( 0.201)	Data  0.057 ( 0.057)	Loss -0.9083 (-0.9083)
Epoch: [196][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9152 (-0.9152)
Epoch: [197][0/2]	Time  0.197 ( 0.197)	Data  0.057 ( 0.057)	Loss -0.9270 (-0.9270)
Epoch: [198][0/2]	Time  0.182 ( 0.182)	Data  0.057 ( 0.057)	Loss -0.9170 (-0.9170)
Epoch: [199][0/2]	Time  0.197 ( 0.197)	Data  0.057 ( 0.057)	Loss -0.9233 (-0.9233)
Epoch: [200][0/2]	Time  0.201 ( 0.201)	Data  0.056 ( 0.056)	Loss -0.9197 (-0.9197)
Epoch: [201][0/2]	Time  0.186 ( 0.186)	Data  0.058 ( 0.058)	Loss -0.9205 (-0.9205)
Epoch: [202][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9206 (-0.9206)
Epoch: [203][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.9212 (-0.9212)
Epoch: [204][0/2]	Time  0.194 ( 0.194)	Data  0.056 ( 0.056)	Loss -0.9150 (-0.9150)
Epoch: [205][0/2]	Time  0.210 ( 0.210)	Data  0.060 ( 0.060)	Loss -0.9112 (-0.9112)
Epoch: [206][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.9122 (-0.9122)
Epoch: [207][0/2]	Time  0.213 ( 0.213)	Data  0.066 ( 0.066)	Loss -0.9105 (-0.9105)
Epoch: [208][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.9243 (-0.9243)
Epoch: [209][0/2]	Time  0.163 ( 0.163)	Data  0.057 ( 0.057)	Loss -0.9169 (-0.9169)
Epoch: [210][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9276 (-0.9276)
Epoch: [211][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9226 (-0.9226)
Epoch: [212][0/2]	Time  0.186 ( 0.186)	Data  0.057 ( 0.057)	Loss -0.9194 (-0.9194)
Epoch: [213][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9125 (-0.9125)
Epoch: [214][0/2]	Time  0.177 ( 0.177)	Data  0.057 ( 0.057)	Loss -0.9228 (-0.9228)
Epoch: [215][0/2]	Time  0.195 ( 0.195)	Data  0.057 ( 0.057)	Loss -0.9198 (-0.9198)
Epoch: [216][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9167 (-0.9167)
Epoch: [217][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.9241 (-0.9241)
Epoch: [218][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9184 (-0.9184)
Epoch: [219][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.9172 (-0.9172)
Epoch: [220][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.9191 (-0.9191)
Epoch: [221][0/2]	Time  0.175 ( 0.175)	Data  0.057 ( 0.057)	Loss -0.9253 (-0.9253)
Epoch: [222][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9153 (-0.9153)
Epoch: [223][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9180 (-0.9180)
Epoch: [224][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9058 (-0.9058)
Epoch: [225][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9178 (-0.9178)
Epoch: [226][0/2]	Time  0.187 ( 0.187)	Data  0.057 ( 0.057)	Loss -0.9127 (-0.9127)
Epoch: [227][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9166 (-0.9166)
Epoch: [228][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9139 (-0.9139)
Epoch: [229][0/2]	Time  0.200 ( 0.200)	Data  0.057 ( 0.057)	Loss -0.9226 (-0.9226)
Epoch: [230][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.9157 (-0.9157)
Epoch: [231][0/2]	Time  0.195 ( 0.195)	Data  0.057 ( 0.057)	Loss -0.9196 (-0.9196)
Epoch: [232][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9152 (-0.9152)
Epoch: [233][0/2]	Time  0.191 ( 0.191)	Data  0.057 ( 0.057)	Loss -0.9218 (-0.9218)
Epoch: [234][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9118 (-0.9118)
Epoch: [235][0/2]	Time  0.185 ( 0.185)	Data  0.059 ( 0.059)	Loss -0.9172 (-0.9172)
Epoch: [236][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9223 (-0.9223)
Epoch: [237][0/2]	Time  0.182 ( 0.182)	Data  0.057 ( 0.057)	Loss -0.9152 (-0.9152)
Epoch: [238][0/2]	Time  0.205 ( 0.205)	Data  0.057 ( 0.057)	Loss -0.9154 (-0.9154)
Epoch: [239][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.9038 (-0.9038)
Epoch: [240][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9171 (-0.9171)
Epoch: [241][0/2]	Time  0.202 ( 0.202)	Data  0.057 ( 0.057)	Loss -0.9213 (-0.9213)
Epoch: [242][0/2]	Time  0.190 ( 0.190)	Data  0.057 ( 0.057)	Loss -0.9118 (-0.9118)
Epoch: [243][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.9117 (-0.9117)
Epoch: [244][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9189 (-0.9189)
Epoch: [245][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9092 (-0.9092)
Epoch: [246][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9117 (-0.9117)
Epoch: [247][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9168 (-0.9168)
Epoch: [248][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9118 (-0.9118)
Epoch: [249][0/2]	Time  0.204 ( 0.204)	Data  0.057 ( 0.057)	Loss -0.9180 (-0.9180)
Epoch: [250][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9213 (-0.9213)
Epoch: [251][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.9222 (-0.9222)
Epoch: [252][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9186 (-0.9186)
Epoch: [253][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.9200 (-0.9200)
Epoch: [254][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.9115 (-0.9115)
Epoch: [255][0/2]	Time  0.213 ( 0.213)	Data  0.056 ( 0.056)	Loss -0.9107 (-0.9107)
Epoch: [256][0/2]	Time  0.206 ( 0.206)	Data  0.057 ( 0.057)	Loss -0.9127 (-0.9127)
Epoch: [257][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.9125 (-0.9125)
Epoch: [258][0/2]	Time  0.214 ( 0.214)	Data  0.059 ( 0.059)	Loss -0.8999 (-0.8999)
Epoch: [259][0/2]	Time  0.219 ( 0.219)	Data  0.057 ( 0.057)	Loss -0.9089 (-0.9089)
Epoch: [260][0/2]	Time  0.226 ( 0.226)	Data  0.063 ( 0.063)	Loss -0.9109 (-0.9109)
Epoch: [261][0/2]	Time  0.209 ( 0.209)	Data  0.057 ( 0.057)	Loss -0.9179 (-0.9179)
Epoch: [262][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.9179 (-0.9179)
Epoch: [263][0/2]	Time  0.217 ( 0.217)	Data  0.057 ( 0.057)	Loss -0.9195 (-0.9195)
Epoch: [264][0/2]	Time  0.217 ( 0.217)	Data  0.057 ( 0.057)	Loss -0.9199 (-0.9199)
Epoch: [265][0/2]	Time  0.212 ( 0.212)	Data  0.057 ( 0.057)	Loss -0.9152 (-0.9152)
Epoch: [266][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9102 (-0.9102)
Epoch: [267][0/2]	Time  0.221 ( 0.221)	Data  0.059 ( 0.059)	Loss -0.9207 (-0.9207)
Epoch: [268][0/2]	Time  0.222 ( 0.222)	Data  0.057 ( 0.057)	Loss -0.9113 (-0.9113)
Epoch: [269][0/2]	Time  0.217 ( 0.217)	Data  0.057 ( 0.057)	Loss -0.9133 (-0.9133)
Epoch: [270][0/2]	Time  0.215 ( 0.215)	Data  0.058 ( 0.058)	Loss -0.9165 (-0.9165)
Epoch: [271][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9241 (-0.9241)
Epoch: [272][0/2]	Time  0.218 ( 0.218)	Data  0.058 ( 0.058)	Loss -0.9155 (-0.9155)
Epoch: [273][0/2]	Time  0.216 ( 0.216)	Data  0.057 ( 0.057)	Loss -0.9249 (-0.9249)
Epoch: [274][0/2]	Time  0.215 ( 0.215)	Data  0.057 ( 0.057)	Loss -0.9227 (-0.9227)
Epoch: [275][0/2]	Time  0.212 ( 0.212)	Data  0.056 ( 0.056)	Loss -0.9227 (-0.9227)
Epoch: [276][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9201 (-0.9201)
Epoch: [277][0/2]	Time  0.215 ( 0.215)	Data  0.056 ( 0.056)	Loss -0.9263 (-0.9263)
Epoch: [278][0/2]	Time  0.222 ( 0.222)	Data  0.058 ( 0.058)	Loss -0.9148 (-0.9148)
Epoch: [279][0/2]	Time  0.215 ( 0.215)	Data  0.058 ( 0.058)	Loss -0.9194 (-0.9194)
Epoch: [280][0/2]	Time  0.208 ( 0.208)	Data  0.057 ( 0.057)	Loss -0.9163 (-0.9163)
Epoch: [281][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.9158 (-0.9158)
Epoch: [282][0/2]	Time  0.208 ( 0.208)	Data  0.058 ( 0.058)	Loss -0.9191 (-0.9191)
Epoch: [283][0/2]	Time  0.189 ( 0.189)	Data  0.060 ( 0.060)	Loss -0.9204 (-0.9204)
Epoch: [284][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9117 (-0.9117)
Epoch: [285][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9155 (-0.9155)
Epoch: [286][0/2]	Time  0.201 ( 0.201)	Data  0.056 ( 0.056)	Loss -0.9026 (-0.9026)
Epoch: [287][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9230 (-0.9230)
Epoch: [288][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.9144 (-0.9144)
Epoch: [289][0/2]	Time  0.207 ( 0.207)	Data  0.063 ( 0.063)	Loss -0.9194 (-0.9194)
Epoch: [290][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.9167 (-0.9167)
Epoch: [291][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9124 (-0.9124)
Epoch: [292][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9251 (-0.9251)
Epoch: [293][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9208 (-0.9208)
Epoch: [294][0/2]	Time  0.186 ( 0.186)	Data  0.056 ( 0.056)	Loss -0.9174 (-0.9174)
Epoch: [295][0/2]	Time  0.206 ( 0.206)	Data  0.060 ( 0.060)	Loss -0.9168 (-0.9168)
Epoch: [296][0/2]	Time  0.179 ( 0.179)	Data  0.056 ( 0.056)	Loss -0.9221 (-0.9221)
Epoch: [297][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9124 (-0.9124)
Epoch: [298][0/2]	Time  0.171 ( 0.171)	Data  0.058 ( 0.058)	Loss -0.9152 (-0.9152)
Epoch: [299][0/2]	Time  0.196 ( 0.196)	Data  0.057 ( 0.057)	Loss -0.9167 (-0.9167)
simsiam program ends here, while ssv:200, normal:200, excep_size:25
Use GPU: 0 for training
=> creating model 'resnet50'
=> loading checkpoint 'checkpoints/checkpoint_0099.pth.tar'
=> loaded pre-trained model 'checkpoints/checkpoint_0099.pth.tar'
Epoch: [0][0/1]	Time  0.506 ( 0.506)	Data  0.039 ( 0.039)	Loss 1.8068e+00 (1.8068e+00)	Acc@1   7.69 (  7.69)	Acc@5  49.54 ( 49.54)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.196 ( 0.196)	Loss 5.8720e-07 (5.8720e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.066 ( 0.052)	Loss 1.3753e+01 (9.8448e+00)	Acc@1   0.00 ( 20.02)	Acc@5   0.00 ( 83.33)
 * Acc@1 20.016 Acc@5 83.333
=> loading 'checkpoints/checkpoint_0099.pth.tar' for sanity check
=> sanity check passed.
Epoch: [1][0/1]	Time  0.050 ( 0.050)	Data  0.023 ( 0.023)	Loss 4.6617e+00 (4.6617e+00)	Acc@1  62.46 ( 62.46)	Acc@5  92.31 ( 92.31)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 1.3178e-07 (1.3178e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.032)	Loss 8.7194e+00 (6.1001e+00)	Acc@1   0.00 ( 33.68)	Acc@5   0.00 ( 83.84)
 * Acc@1 33.684 Acc@5 83.840
Epoch: [2][0/1]	Time  0.044 ( 0.044)	Data  0.024 ( 0.024)	Loss 3.0240e+00 (3.0240e+00)	Acc@1  68.62 ( 68.62)	Acc@5  92.62 ( 92.62)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.032 ( 0.032)	Loss 1.6362e-04 (1.6362e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 5.5003e+00 (3.4651e+00)	Acc@1   0.00 ( 31.04)	Acc@5  75.00 ( 97.94)
 * Acc@1 31.036 Acc@5 97.936
Epoch: [3][0/1]	Time  0.044 ( 0.044)	Data  0.024 ( 0.024)	Loss 1.8354e+00 (1.8354e+00)	Acc@1  67.38 ( 67.38)	Acc@5  98.46 ( 98.46)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 1.0373e+00 (1.0373e+00)	Acc@1  57.81 ( 57.81)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.031)	Loss 4.1706e+00 (4.0622e+00)	Acc@1   0.00 ( 26.21)	Acc@5 100.00 ( 99.84)
 * Acc@1 26.207 Acc@5 99.844
Epoch: [4][0/1]	Time  0.046 ( 0.046)	Data  0.025 ( 0.025)	Loss 2.0884e+00 (2.0884e+00)	Acc@1  50.77 ( 50.77)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.036 ( 0.036)	Loss 6.2175e-01 (6.2175e-01)	Acc@1  74.22 ( 74.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 1.6923e-01 (5.0110e+00)	Acc@1 100.00 ( 41.20)	Acc@5 100.00 ( 99.92)
 * Acc@1 41.199 Acc@5 99.922
Epoch: [5][0/1]	Time  0.047 ( 0.047)	Data  0.026 ( 0.026)	Loss 2.1685e+00 (2.1685e+00)	Acc@1  60.31 ( 60.31)	Acc@5  99.38 ( 99.38)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 1.2525e-04 (1.2525e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.032)	Loss 1.0613e+00 (3.1835e+00)	Acc@1  37.50 ( 48.83)	Acc@5 100.00 ( 98.60)
 * Acc@1 48.832 Acc@5 98.598
Epoch: [6][0/1]	Time  0.045 ( 0.045)	Data  0.024 ( 0.024)	Loss 1.4186e+00 (1.4186e+00)	Acc@1  73.85 ( 73.85)	Acc@5  98.46 ( 98.46)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.031 ( 0.031)	Loss 2.5116e-06 (2.5116e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.031)	Loss 6.5589e+00 (4.2362e+00)	Acc@1   0.00 ( 34.50)	Acc@5 100.00 ( 97.70)
 * Acc@1 34.502 Acc@5 97.702
Epoch: [7][0/1]	Time  0.046 ( 0.046)	Data  0.025 ( 0.025)	Loss 2.6249e+00 (2.6249e+00)	Acc@1  69.23 ( 69.23)	Acc@5  97.85 ( 97.85)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 1.8626e-09 (1.8626e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.032)	Loss 1.7250e+00 (3.8453e+00)	Acc@1  50.00 ( 40.93)	Acc@5 100.00 ( 97.66)
 * Acc@1 40.927 Acc@5 97.664
Epoch: [8][0/1]	Time  0.044 ( 0.044)	Data  0.023 ( 0.023)	Loss 2.1698e+00 (2.1698e+00)	Acc@1  68.92 ( 68.92)	Acc@5  97.85 ( 97.85)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.035 ( 0.035)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.032)	Loss 2.3371e+00 (4.9338e+00)	Acc@1  50.00 ( 38.90)	Acc@5 100.00 ( 98.01)
 * Acc@1 38.902 Acc@5 98.014
Epoch: [9][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 2.6329e+00 (2.6329e+00)	Acc@1  70.46 ( 70.46)	Acc@5  97.54 ( 97.54)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.4901e-08 (1.4901e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 1.2282e+00 (6.4122e+00)	Acc@1  25.00 ( 38.43)	Acc@5 100.00 ( 98.68)
 * Acc@1 38.435 Acc@5 98.676
Epoch: [10][0/1]	Time  0.066 ( 0.066)	Data  0.023 ( 0.023)	Loss 3.0773e+00 (3.0773e+00)	Acc@1  68.92 ( 68.92)	Acc@5  97.54 ( 97.54)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 7.6054e-06 (7.6054e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 5.4304e-01 (6.9422e+00)	Acc@1  87.50 ( 41.94)	Acc@5 100.00 (100.00)
 * Acc@1 41.939 Acc@5 100.000
Epoch: [11][0/1]	Time  0.062 ( 0.062)	Data  0.020 ( 0.020)	Loss 2.8063e+00 (2.8063e+00)	Acc@1  70.15 ( 70.15)	Acc@5  99.08 ( 99.08)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.8772e-05 (1.8772e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.046)	Loss 2.2999e+00 (4.5886e+00)	Acc@1  12.50 ( 43.77)	Acc@5 100.00 (100.00)
 * Acc@1 43.769 Acc@5 100.000
Epoch: [12][0/1]	Time  0.065 ( 0.065)	Data  0.023 ( 0.023)	Loss 1.8405e+00 (1.8405e+00)	Acc@1  72.92 ( 72.92)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 1.7479e-04 (1.7479e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 3.2439e+00 (3.2730e+00)	Acc@1  12.50 ( 49.96)	Acc@5 100.00 (100.00)
 * Acc@1 49.961 Acc@5 100.000
Epoch: [13][0/1]	Time  0.064 ( 0.064)	Data  0.021 ( 0.021)	Loss 1.4322e+00 (1.4322e+00)	Acc@1  74.77 ( 74.77)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 2.0689e-02 (2.0689e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 3.3322e+00 (4.8432e+00)	Acc@1  25.00 ( 52.02)	Acc@5 100.00 (100.00)
 * Acc@1 52.025 Acc@5 100.000
Epoch: [14][0/1]	Time  0.061 ( 0.061)	Data  0.022 ( 0.022)	Loss 2.0320e+00 (2.0320e+00)	Acc@1  74.15 ( 74.15)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.2247e-01 (1.2247e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.049)	Loss 1.7223e+00 (4.4176e+00)	Acc@1  75.00 ( 57.32)	Acc@5 100.00 (100.00)
 * Acc@1 57.321 Acc@5 100.000
Epoch: [15][0/1]	Time  0.063 ( 0.063)	Data  0.023 ( 0.023)	Loss 1.8362e+00 (1.8362e+00)	Acc@1  76.92 ( 76.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.8031e-01 (3.8031e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 7.2598e-02 (3.8440e+00)	Acc@1 100.00 ( 57.05)	Acc@5 100.00 ( 99.96)
 * Acc@1 57.048 Acc@5 99.961
Epoch: [16][0/1]	Time  0.066 ( 0.066)	Data  0.025 ( 0.025)	Loss 1.6653e+00 (1.6653e+00)	Acc@1  75.08 ( 75.08)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.2107e+00 (1.2107e+00)	Acc@1  64.45 ( 64.45)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 3.3498e-01 (4.0304e+00)	Acc@1  87.50 ( 47.51)	Acc@5 100.00 ( 99.96)
 * Acc@1 47.508 Acc@5 99.961
Epoch: [17][0/1]	Time  0.067 ( 0.067)	Data  0.023 ( 0.023)	Loss 2.2914e+00 (2.2914e+00)	Acc@1  57.54 ( 57.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 5.7682e-02 (5.7682e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.046)	Loss 1.2413e+00 (3.4878e+00)	Acc@1  37.50 ( 47.78)	Acc@5 100.00 (100.00)
 * Acc@1 47.780 Acc@5 100.000
Epoch: [18][0/1]	Time  0.065 ( 0.065)	Data  0.023 ( 0.023)	Loss 1.4386e+00 (1.4386e+00)	Acc@1  76.62 ( 76.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 6.4292e-03 (6.4292e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.046)	Loss 1.2442e+00 (2.7683e+00)	Acc@1  50.00 ( 52.38)	Acc@5 100.00 (100.00)
 * Acc@1 52.375 Acc@5 100.000
Epoch: [19][0/1]	Time  0.064 ( 0.064)	Data  0.024 ( 0.024)	Loss 1.3153e+00 (1.3153e+00)	Acc@1  76.31 ( 76.31)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.046 ( 0.046)	Loss 1.3841e-04 (1.3841e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 1.2257e+00 (2.1195e+00)	Acc@1  50.00 ( 53.54)	Acc@5 100.00 (100.00)
 * Acc@1 53.544 Acc@5 100.000
Epoch: [20][0/1]	Time  0.064 ( 0.064)	Data  0.025 ( 0.025)	Loss 1.0368e+00 (1.0368e+00)	Acc@1  79.38 ( 79.38)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 7.0022e-06 (7.0022e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 2.9539e+00 (2.0414e+00)	Acc@1  12.50 ( 52.69)	Acc@5 100.00 ( 99.92)
 * Acc@1 52.687 Acc@5 99.922
Epoch: [21][0/1]	Time  0.067 ( 0.067)	Data  0.026 ( 0.026)	Loss 1.1470e+00 (1.1470e+00)	Acc@1  76.92 ( 76.92)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 4.3584e-07 (4.3584e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 3.0605e+00 (2.0388e+00)	Acc@1  25.00 ( 53.70)	Acc@5 100.00 ( 99.53)
 * Acc@1 53.699 Acc@5 99.533
Epoch: [22][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 1.3337e+00 (1.3337e+00)	Acc@1  76.62 ( 76.62)	Acc@5  99.38 ( 99.38)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 1.4435e-08 (1.4435e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 1.3594e+00 (1.7262e+00)	Acc@1  50.00 ( 55.14)	Acc@5 100.00 ( 99.49)
 * Acc@1 55.140 Acc@5 99.494
Epoch: [23][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 1.1415e+00 (1.1415e+00)	Acc@1  78.46 ( 78.46)	Acc@5  99.08 ( 99.08)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.3306e-08 (4.3306e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 5.6866e-01 (1.6160e+00)	Acc@1  62.50 ( 55.57)	Acc@5 100.00 ( 99.57)
 * Acc@1 55.569 Acc@5 99.572
Epoch: [24][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 1.2094e+00 (1.2094e+00)	Acc@1  79.08 ( 79.08)	Acc@5  98.77 ( 98.77)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.6566e-10 (4.6566e-10)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.1533e+00 (1.8597e+00)	Acc@1  75.00 ( 53.19)	Acc@5 100.00 ( 99.77)
 * Acc@1 53.193 Acc@5 99.766
Epoch: [25][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 1.2201e+00 (1.2201e+00)	Acc@1  75.08 ( 75.08)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 6.1467e-08 (6.1467e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 4.7411e-01 (1.6718e+00)	Acc@1  87.50 ( 55.61)	Acc@5 100.00 ( 99.81)
 * Acc@1 55.607 Acc@5 99.805
Epoch: [26][0/1]	Time  0.067 ( 0.067)	Data  0.025 ( 0.025)	Loss 1.2132e+00 (1.2132e+00)	Acc@1  75.08 ( 75.08)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.2107e-08 (1.2107e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 5.6928e-01 (1.4573e+00)	Acc@1  75.00 ( 59.27)	Acc@5 100.00 ( 99.73)
 * Acc@1 59.268 Acc@5 99.727
Epoch: [27][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 8.9583e-01 (8.9583e-01)	Acc@1  77.54 ( 77.54)	Acc@5  99.08 ( 99.08)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.2477e-07 (5.2477e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 9.3936e-01 (1.5733e+00)	Acc@1  50.00 ( 55.10)	Acc@5 100.00 ( 99.88)
 * Acc@1 55.101 Acc@5 99.883
Epoch: [28][0/1]	Time  0.061 ( 0.061)	Data  0.019 ( 0.019)	Loss 9.5192e-01 (9.5192e-01)	Acc@1  77.54 ( 77.54)	Acc@5  99.08 ( 99.08)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 4.0978e-08 (4.0978e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 9.5819e-01 (1.5644e+00)	Acc@1  62.50 ( 53.19)	Acc@5 100.00 ( 99.92)
 * Acc@1 53.193 Acc@5 99.922
Epoch: [29][0/1]	Time  0.066 ( 0.066)	Data  0.026 ( 0.026)	Loss 8.5618e-01 (8.5618e-01)	Acc@1  79.08 ( 79.08)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.9360e-08 (4.9360e-08)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 1.5841e+00 (1.6290e+00)	Acc@1  37.50 ( 57.59)	Acc@5 100.00 (100.00)
 * Acc@1 57.593 Acc@5 100.000
Epoch: [30][0/1]	Time  0.066 ( 0.066)	Data  0.024 ( 0.024)	Loss 7.8541e-01 (7.8541e-01)	Acc@1  78.15 ( 78.15)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.1595e-07 (1.1595e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.049)	Loss 8.2130e-01 (1.3971e+00)	Acc@1  75.00 ( 63.51)	Acc@5 100.00 (100.00)
 * Acc@1 63.512 Acc@5 100.000
Epoch: [31][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 6.7864e-01 (6.7864e-01)	Acc@1  81.23 ( 81.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 5.6771e-06 (5.6771e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.0237e-01 (1.3639e+00)	Acc@1 100.00 ( 57.83)	Acc@5 100.00 ( 99.92)
 * Acc@1 57.827 Acc@5 99.922
Epoch: [32][0/1]	Time  0.066 ( 0.066)	Data  0.024 ( 0.024)	Loss 7.9863e-01 (7.9863e-01)	Acc@1  80.31 ( 80.31)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 3.1919e-05 (3.1919e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 3.6384e-01 (1.2593e+00)	Acc@1  87.50 ( 57.40)	Acc@5 100.00 (100.00)
 * Acc@1 57.399 Acc@5 100.000
Epoch: [33][0/1]	Time  0.062 ( 0.062)	Data  0.023 ( 0.023)	Loss 6.2986e-01 (6.2986e-01)	Acc@1  81.23 ( 81.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 3.9951e-03 (3.9951e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.047)	Loss 7.8361e-01 (1.2016e+00)	Acc@1  75.00 ( 53.62)	Acc@5 100.00 (100.00)
 * Acc@1 53.621 Acc@5 100.000
Epoch: [34][0/1]	Time  0.069 ( 0.069)	Data  0.025 ( 0.025)	Loss 5.3830e-01 (5.3830e-01)	Acc@1  81.54 ( 81.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.6907e-02 (1.6907e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.4634e+00 (1.4120e+00)	Acc@1  37.50 ( 46.22)	Acc@5 100.00 (100.00)
 * Acc@1 46.223 Acc@5 100.000
Epoch: [35][0/1]	Time  0.071 ( 0.071)	Data  0.028 ( 0.028)	Loss 6.3548e-01 (6.3548e-01)	Acc@1  77.23 ( 77.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.3743e-02 (1.3743e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.5952e+00 (1.3724e+00)	Acc@1  12.50 ( 55.72)	Acc@5 100.00 (100.00)
 * Acc@1 55.724 Acc@5 100.000
Epoch: [36][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 4.9222e-01 (4.9222e-01)	Acc@1  82.77 ( 82.77)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.9199e-02 (2.9199e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 9.2773e-01 (1.3789e+00)	Acc@1  50.00 ( 59.00)	Acc@5 100.00 (100.00)
 * Acc@1 58.995 Acc@5 100.000
Epoch: [37][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 5.9486e-01 (5.9486e-01)	Acc@1  81.54 ( 81.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 3.3275e-02 (3.3275e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.048)	Loss 6.1030e-01 (1.1835e+00)	Acc@1  87.50 ( 58.26)	Acc@5 100.00 (100.00)
 * Acc@1 58.255 Acc@5 100.000
Epoch: [38][0/1]	Time  0.069 ( 0.069)	Data  0.025 ( 0.025)	Loss 4.9239e-01 (4.9239e-01)	Acc@1  81.54 ( 81.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 3.6845e-02 (3.6845e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 9.3318e-01 (1.1622e+00)	Acc@1  50.00 ( 58.10)	Acc@5 100.00 (100.00)
 * Acc@1 58.100 Acc@5 100.000
Epoch: [39][0/1]	Time  0.065 ( 0.065)	Data  0.023 ( 0.023)	Loss 5.4900e-01 (5.4900e-01)	Acc@1  79.69 ( 79.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.0878e-01 (1.0878e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.0784e+00 (1.1745e+00)	Acc@1  62.50 ( 60.28)	Acc@5 100.00 (100.00)
 * Acc@1 60.280 Acc@5 100.000
Epoch: [40][0/1]	Time  0.067 ( 0.067)	Data  0.025 ( 0.025)	Loss 5.2511e-01 (5.2511e-01)	Acc@1  83.08 ( 83.08)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 1.6940e-01 (1.6940e-01)	Acc@1  96.09 ( 96.09)	Acc@5  99.22 ( 99.22)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 1.4536e+00 (1.1687e+00)	Acc@1  50.00 ( 58.80)	Acc@5 100.00 ( 99.92)
 * Acc@1 58.801 Acc@5 99.922
Epoch: [41][0/1]	Time  0.068 ( 0.068)	Data  0.026 ( 0.026)	Loss 5.5217e-01 (5.5217e-01)	Acc@1  81.23 ( 81.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.5807e-01 (1.5807e-01)	Acc@1  96.09 ( 96.09)	Acc@5  99.22 ( 99.22)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 1.4513e+00 (1.0821e+00)	Acc@1  37.50 ( 59.54)	Acc@5 100.00 ( 99.92)
 * Acc@1 59.540 Acc@5 99.922
Epoch: [42][0/1]	Time  0.060 ( 0.060)	Data  0.020 ( 0.020)	Loss 4.7416e-01 (4.7416e-01)	Acc@1  83.69 ( 83.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.2363e-01 (1.2363e-01)	Acc@1  96.48 ( 96.48)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 1.1286e+00 (9.5806e-01)	Acc@1  37.50 ( 62.11)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.111 Acc@5 99.961
Epoch: [43][0/1]	Time  0.064 ( 0.064)	Data  0.020 ( 0.020)	Loss 5.1124e-01 (5.1124e-01)	Acc@1  80.31 ( 80.31)	Acc@5  99.69 ( 99.69)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 1.0564e-01 (1.0564e-01)	Acc@1  96.88 ( 96.88)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.3649e+00 (9.1372e-01)	Acc@1  25.00 ( 64.41)	Acc@5 100.00 ( 99.96)
 * Acc@1 64.408 Acc@5 99.961
Epoch: [44][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 4.7351e-01 (4.7351e-01)	Acc@1  81.23 ( 81.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 6.9517e-02 (6.9517e-02)	Acc@1  98.44 ( 98.44)	Acc@5  99.22 ( 99.22)
Test: [10/11]	Time  0.005 ( 0.050)	Loss 1.4892e+00 (8.7362e-01)	Acc@1  37.50 ( 67.33)	Acc@5 100.00 ( 99.92)
 * Acc@1 67.329 Acc@5 99.922
Epoch: [45][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 4.7242e-01 (4.7242e-01)	Acc@1  81.23 ( 81.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.3699e-02 (2.3699e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 7.8696e-01 (9.1138e-01)	Acc@1  62.50 ( 66.59)	Acc@5 100.00 (100.00)
 * Acc@1 66.589 Acc@5 100.000
Epoch: [46][0/1]	Time  0.071 ( 0.071)	Data  0.024 ( 0.024)	Loss 4.5658e-01 (4.5658e-01)	Acc@1  83.69 ( 83.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 6.7179e-03 (6.7179e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.2185e+00 (9.7083e-01)	Acc@1  37.50 ( 66.59)	Acc@5 100.00 (100.00)
 * Acc@1 66.589 Acc@5 100.000
Epoch: [47][0/1]	Time  0.067 ( 0.067)	Data  0.023 ( 0.023)	Loss 4.6502e-01 (4.6502e-01)	Acc@1  83.38 ( 83.38)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 1.4664e-02 (1.4664e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 9.9436e-01 (1.0198e+00)	Acc@1  62.50 ( 66.59)	Acc@5 100.00 ( 99.96)
 * Acc@1 66.589 Acc@5 99.961
Epoch: [48][0/1]	Time  0.068 ( 0.068)	Data  0.024 ( 0.024)	Loss 4.8256e-01 (4.8256e-01)	Acc@1  82.15 ( 82.15)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 2.5211e-02 (2.5211e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.3788e+00 (8.2878e-01)	Acc@1  37.50 ( 68.54)	Acc@5 100.00 (100.00)
 * Acc@1 68.536 Acc@5 100.000
Epoch: [49][0/1]	Time  0.064 ( 0.064)	Data  0.021 ( 0.021)	Loss 4.4659e-01 (4.4659e-01)	Acc@1  83.69 ( 83.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.5317e-02 (1.5317e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 9.5492e-01 (8.0889e-01)	Acc@1  75.00 ( 65.73)	Acc@5 100.00 (100.00)
 * Acc@1 65.732 Acc@5 100.000
Epoch: [50][0/1]	Time  0.062 ( 0.062)	Data  0.021 ( 0.021)	Loss 4.5680e-01 (4.5680e-01)	Acc@1  81.85 ( 81.85)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.9399e-02 (1.9399e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 1.3355e+00 (8.0668e-01)	Acc@1  50.00 ( 66.82)	Acc@5 100.00 (100.00)
 * Acc@1 66.822 Acc@5 100.000
Epoch: [51][0/1]	Time  0.066 ( 0.066)	Data  0.025 ( 0.025)	Loss 4.5636e-01 (4.5636e-01)	Acc@1  82.46 ( 82.46)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 3.4072e-03 (3.4072e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 1.2415e+00 (8.6934e-01)	Acc@1  37.50 ( 66.00)	Acc@5 100.00 (100.00)
 * Acc@1 66.005 Acc@5 100.000
Epoch: [52][0/1]	Time  0.068 ( 0.068)	Data  0.026 ( 0.026)	Loss 4.0696e-01 (4.0696e-01)	Acc@1  85.23 ( 85.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.5361e-02 (1.5361e-02)	Acc@1  99.61 ( 99.61)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.4010e+00 (8.6758e-01)	Acc@1  37.50 ( 68.38)	Acc@5 100.00 ( 99.92)
 * Acc@1 68.380 Acc@5 99.922
Epoch: [53][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 4.2351e-01 (4.2351e-01)	Acc@1  85.85 ( 85.85)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 1.6615e-02 (1.6615e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 1.1575e+00 (8.7223e-01)	Acc@1  50.00 ( 66.86)	Acc@5 100.00 ( 99.96)
 * Acc@1 66.861 Acc@5 99.961
Epoch: [54][0/1]	Time  0.067 ( 0.067)	Data  0.025 ( 0.025)	Loss 4.2635e-01 (4.2635e-01)	Acc@1  84.92 ( 84.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 1.9036e-02 (1.9036e-02)	Acc@1  99.61 ( 99.61)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 8.6438e-01 (8.9695e-01)	Acc@1  75.00 ( 66.16)	Acc@5 100.00 ( 99.96)
 * Acc@1 66.160 Acc@5 99.961
Epoch: [55][0/1]	Time  0.067 ( 0.067)	Data  0.023 ( 0.023)	Loss 4.6304e-01 (4.6304e-01)	Acc@1  83.38 ( 83.38)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 5.6295e-03 (5.6295e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 9.4034e-01 (8.8544e-01)	Acc@1  37.50 ( 66.00)	Acc@5 100.00 (100.00)
 * Acc@1 66.005 Acc@5 100.000
Epoch: [56][0/1]	Time  0.068 ( 0.068)	Data  0.024 ( 0.024)	Loss 4.2072e-01 (4.2072e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.2383e-02 (1.2383e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 7.8647e-01 (8.7469e-01)	Acc@1  87.50 ( 66.82)	Acc@5 100.00 (100.00)
 * Acc@1 66.822 Acc@5 100.000
Epoch: [57][0/1]	Time  0.063 ( 0.063)	Data  0.022 ( 0.022)	Loss 4.2668e-01 (4.2668e-01)	Acc@1  87.69 ( 87.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 6.0823e-04 (6.0823e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.0351e+00 (9.1001e-01)	Acc@1  37.50 ( 64.14)	Acc@5 100.00 (100.00)
 * Acc@1 64.136 Acc@5 100.000
Epoch: [58][0/1]	Time  0.060 ( 0.060)	Data  0.020 ( 0.020)	Loss 4.3686e-01 (4.3686e-01)	Acc@1  85.23 ( 85.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 8.6243e-04 (8.6243e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 9.5434e-01 (9.0944e-01)	Acc@1  62.50 ( 64.84)	Acc@5 100.00 (100.00)
 * Acc@1 64.836 Acc@5 100.000
Epoch: [59][0/1]	Time  0.067 ( 0.067)	Data  0.026 ( 0.026)	Loss 4.1495e-01 (4.1495e-01)	Acc@1  82.46 ( 82.46)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.7915e-03 (1.7915e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 9.5003e-01 (9.0724e-01)	Acc@1  50.00 ( 64.52)	Acc@5 100.00 (100.00)
 * Acc@1 64.525 Acc@5 100.000
Epoch: [60][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 4.1076e-01 (4.1076e-01)	Acc@1  84.31 ( 84.31)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 4.1862e-03 (4.1862e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 8.2482e-01 (8.9070e-01)	Acc@1  75.00 ( 63.94)	Acc@5 100.00 (100.00)
 * Acc@1 63.941 Acc@5 100.000
Epoch: [61][0/1]	Time  0.068 ( 0.068)	Data  0.023 ( 0.023)	Loss 4.0086e-01 (4.0086e-01)	Acc@1  84.92 ( 84.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.4528e-03 (5.4528e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 8.4105e-01 (8.7110e-01)	Acc@1  75.00 ( 64.37)	Acc@5 100.00 (100.00)
 * Acc@1 64.369 Acc@5 100.000
Epoch: [62][0/1]	Time  0.068 ( 0.068)	Data  0.026 ( 0.026)	Loss 3.9404e-01 (3.9404e-01)	Acc@1  84.92 ( 84.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.1551e-03 (1.1551e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.047)	Loss 6.1073e-01 (9.1240e-01)	Acc@1  75.00 ( 65.19)	Acc@5 100.00 (100.00)
 * Acc@1 65.187 Acc@5 100.000
Epoch: [63][0/1]	Time  0.066 ( 0.066)	Data  0.022 ( 0.022)	Loss 3.9528e-01 (3.9528e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.3462e-02 (1.3462e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 7.8574e-01 (8.9956e-01)	Acc@1  75.00 ( 65.26)	Acc@5 100.00 (100.00)
 * Acc@1 65.265 Acc@5 100.000
Epoch: [64][0/1]	Time  0.066 ( 0.066)	Data  0.022 ( 0.022)	Loss 3.9320e-01 (3.9320e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 8.4767e-04 (8.4767e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 7.2890e-01 (9.2354e-01)	Acc@1  75.00 ( 65.19)	Acc@5 100.00 (100.00)
 * Acc@1 65.187 Acc@5 100.000
Epoch: [65][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 4.4876e-01 (4.4876e-01)	Acc@1  81.54 ( 81.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.049 ( 0.049)	Loss 2.1351e-03 (2.1351e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 7.7260e-01 (8.9604e-01)	Acc@1  75.00 ( 65.62)	Acc@5 100.00 (100.00)
 * Acc@1 65.615 Acc@5 100.000
Epoch: [66][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 3.8529e-01 (3.8529e-01)	Acc@1  83.38 ( 83.38)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.7998e-04 (1.7998e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 9.9779e-01 (8.8157e-01)	Acc@1  62.50 ( 65.85)	Acc@5 100.00 (100.00)
 * Acc@1 65.849 Acc@5 100.000
Epoch: [67][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 4.3459e-01 (4.3459e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 3.9787e-03 (3.9787e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 8.2605e-01 (8.7980e-01)	Acc@1  62.50 ( 64.88)	Acc@5 100.00 (100.00)
 * Acc@1 64.875 Acc@5 100.000
Epoch: [68][0/1]	Time  0.064 ( 0.064)	Data  0.020 ( 0.020)	Loss 4.0139e-01 (4.0139e-01)	Acc@1  85.85 ( 85.85)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 1.0369e-03 (1.0369e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 7.8381e-01 (8.5137e-01)	Acc@1  62.50 ( 65.77)	Acc@5 100.00 (100.00)
 * Acc@1 65.771 Acc@5 100.000
Epoch: [69][0/1]	Time  0.065 ( 0.065)	Data  0.025 ( 0.025)	Loss 4.1431e-01 (4.1431e-01)	Acc@1  85.23 ( 85.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 9.6811e-03 (9.6811e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 8.2789e-01 (8.6268e-01)	Acc@1  62.50 ( 64.68)	Acc@5 100.00 (100.00)
 * Acc@1 64.681 Acc@5 100.000
Epoch: [70][0/1]	Time  0.064 ( 0.064)	Data  0.026 ( 0.026)	Loss 3.8921e-01 (3.8921e-01)	Acc@1  84.31 ( 84.31)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.055 ( 0.055)	Loss 7.9133e-04 (7.9133e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 7.5902e-01 (8.4707e-01)	Acc@1  75.00 ( 65.07)	Acc@5 100.00 (100.00)
 * Acc@1 65.070 Acc@5 100.000
Epoch: [71][0/1]	Time  0.068 ( 0.068)	Data  0.026 ( 0.026)	Loss 4.2145e-01 (4.2145e-01)	Acc@1  85.23 ( 85.23)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
.\main_lincls.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
.\main_lincls.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained, map_location="cpu")
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
.\main_lincls.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_weights, map_location="cpu")
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 1.0384e-04 (1.0384e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 5.3319e-01 (8.4709e-01)	Acc@1  87.50 ( 65.81)	Acc@5 100.00 (100.00)
 * Acc@1 65.810 Acc@5 100.000
Epoch: [72][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 3.6902e-01 (3.6902e-01)	Acc@1  83.69 ( 83.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 6.0331e-03 (6.0331e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 9.3935e-01 (8.5681e-01)	Acc@1  62.50 ( 64.21)	Acc@5 100.00 (100.00)
 * Acc@1 64.213 Acc@5 100.000
Epoch: [73][0/1]	Time  0.066 ( 0.066)	Data  0.024 ( 0.024)	Loss 3.9291e-01 (3.9291e-01)	Acc@1  83.69 ( 83.69)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 7.0002e-03 (7.0002e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.046)	Loss 9.3328e-01 (8.3905e-01)	Acc@1  50.00 ( 64.68)	Acc@5 100.00 (100.00)
 * Acc@1 64.681 Acc@5 100.000
Epoch: [74][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 4.2037e-01 (4.2037e-01)	Acc@1  84.92 ( 84.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 5.4733e-03 (5.4733e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 8.7000e-01 (8.3422e-01)	Acc@1  75.00 ( 66.12)	Acc@5 100.00 (100.00)
 * Acc@1 66.121 Acc@5 100.000
Epoch: [75][0/1]	Time  0.063 ( 0.063)	Data  0.021 ( 0.021)	Loss 3.7702e-01 (3.7702e-01)	Acc@1  85.54 ( 85.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 1.3117e-03 (1.3117e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 7.0676e-01 (8.1604e-01)	Acc@1  87.50 ( 66.12)	Acc@5 100.00 (100.00)
 * Acc@1 66.121 Acc@5 100.000
Epoch: [76][0/1]	Time  0.065 ( 0.065)	Data  0.021 ( 0.021)	Loss 4.0656e-01 (4.0656e-01)	Acc@1  85.85 ( 85.85)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 3.0045e-03 (3.0045e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 1.0156e+00 (8.2853e-01)	Acc@1  50.00 ( 66.32)	Acc@5 100.00 (100.00)
 * Acc@1 66.316 Acc@5 100.000
Epoch: [77][0/1]	Time  0.066 ( 0.066)	Data  0.025 ( 0.025)	Loss 4.0159e-01 (4.0159e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 2.9265e-03 (2.9265e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 1.0203e+00 (8.2223e-01)	Acc@1  87.50 ( 65.73)	Acc@5 100.00 (100.00)
 * Acc@1 65.732 Acc@5 100.000
Epoch: [78][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 3.8604e-01 (3.8604e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 4.1955e-04 (4.1955e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.046)	Loss 5.3553e-01 (8.1371e-01)	Acc@1  87.50 ( 66.86)	Acc@5 100.00 (100.00)
 * Acc@1 66.861 Acc@5 100.000
Epoch: [79][0/1]	Time  0.066 ( 0.066)	Data  0.024 ( 0.024)	Loss 3.8881e-01 (3.8881e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.1223e-04 (2.1223e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 8.5497e-01 (8.2178e-01)	Acc@1  75.00 ( 66.36)	Acc@5 100.00 (100.00)
 * Acc@1 66.355 Acc@5 100.000
Epoch: [80][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 3.9109e-01 (3.9109e-01)	Acc@1  84.92 ( 84.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.052 ( 0.052)	Loss 2.0266e-02 (2.0266e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 7.4579e-01 (8.0864e-01)	Acc@1  75.00 ( 67.13)	Acc@5 100.00 (100.00)
 * Acc@1 67.134 Acc@5 100.000
Epoch: [81][0/1]	Time  0.068 ( 0.068)	Data  0.025 ( 0.025)	Loss 3.7198e-01 (3.7198e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 1.4860e-03 (1.4860e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 8.9879e-01 (8.1352e-01)	Acc@1  50.00 ( 67.06)	Acc@5 100.00 (100.00)
 * Acc@1 67.056 Acc@5 100.000
Epoch: [82][0/1]	Time  0.065 ( 0.065)	Data  0.023 ( 0.023)	Loss 3.9318e-01 (3.9318e-01)	Acc@1  86.77 ( 86.77)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 2.2294e-03 (2.2294e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.047)	Loss 6.7333e-01 (8.0802e-01)	Acc@1  75.00 ( 67.10)	Acc@5 100.00 (100.00)
 * Acc@1 67.095 Acc@5 100.000
Epoch: [83][0/1]	Time  0.064 ( 0.064)	Data  0.021 ( 0.021)	Loss 3.8436e-01 (3.8436e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 5.2001e-04 (5.2001e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.047)	Loss 8.2805e-01 (8.2492e-01)	Acc@1  75.00 ( 67.60)	Acc@5 100.00 (100.00)
 * Acc@1 67.601 Acc@5 100.000
Epoch: [84][0/1]	Time  0.067 ( 0.067)	Data  0.025 ( 0.025)	Loss 3.9988e-01 (3.9988e-01)	Acc@1  84.31 ( 84.31)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.051 ( 0.051)	Loss 8.4193e-04 (8.4193e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.048)	Loss 7.9526e-01 (8.1842e-01)	Acc@1  50.00 ( 66.94)	Acc@5 100.00 (100.00)
 * Acc@1 66.939 Acc@5 100.000
Epoch: [85][0/1]	Time  0.067 ( 0.067)	Data  0.023 ( 0.023)	Loss 3.7568e-01 (3.7568e-01)	Acc@1  84.62 ( 84.62)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.057 ( 0.057)	Loss 7.1463e-03 (7.1463e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.049)	Loss 9.5962e-01 (8.0984e-01)	Acc@1  62.50 ( 66.74)	Acc@5 100.00 (100.00)
 * Acc@1 66.745 Acc@5 100.000
Epoch: [86][0/1]	Time  0.067 ( 0.067)	Data  0.024 ( 0.024)	Loss 4.0534e-01 (4.0534e-01)	Acc@1  83.38 ( 83.38)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.048 ( 0.048)	Loss 2.8421e-03 (2.8421e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.047)	Loss 7.7618e-01 (8.2945e-01)	Acc@1  62.50 ( 66.71)	Acc@5 100.00 (100.00)
 * Acc@1 66.706 Acc@5 100.000
Epoch: [87][0/1]	Time  0.065 ( 0.065)	Data  0.022 ( 0.022)	Loss 3.9182e-01 (3.9182e-01)	Acc@1  85.54 ( 85.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.054 ( 0.054)	Loss 3.8968e-03 (3.8968e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.048)	Loss 1.0503e+00 (8.1350e-01)	Acc@1  25.00 ( 67.52)	Acc@5 100.00 (100.00)
 * Acc@1 67.523 Acc@5 100.000
Epoch: [88][0/1]	Time  0.063 ( 0.063)	Data  0.020 ( 0.020)	Loss 3.6627e-01 (3.6627e-01)	Acc@1  85.54 ( 85.54)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.050 ( 0.050)	Loss 1.1485e-03 (1.1485e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.048)	Loss 7.2149e-01 (8.0145e-01)	Acc@1  75.00 ( 67.10)	Acc@5 100.00 (100.00)
 * Acc@1 67.095 Acc@5 100.000
Epoch: [89][0/1]	Time  0.065 ( 0.065)	Data  0.024 ( 0.024)	Loss 4.1202e-01 (4.1202e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:25
Test: [ 0/11]	Time  0.053 ( 0.053)	Loss 4.9770e-03 (4.9770e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.007 ( 0.046)	Loss 6.2155e-01 (8.0492e-01)	Acc@1  87.50 ( 67.52)	Acc@5 100.00 (100.00)
 * Acc@1 67.523 Acc@5 100.000
Running with excep_size=13 
Use GPU: 0 for training
=> creating model 'resnet50'
SimSiam(
  (encoder): ResNet(
    (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (fc): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=512, bias=False)
      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=512, out_features=2048, bias=True)
      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (predictor): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=2048, bias=True)
  )
)
Epoch: [0][0/2]	Time  1.623 ( 1.623)	Data  0.083 ( 0.083)	Loss 0.0011 (0.0011)
Epoch: [1][0/2]	Time  0.137 ( 0.137)	Data  0.063 ( 0.063)	Loss -0.0885 (-0.0885)
Epoch: [2][0/2]	Time  0.134 ( 0.134)	Data  0.059 ( 0.059)	Loss -0.1814 (-0.1814)
Epoch: [3][0/2]	Time  0.133 ( 0.133)	Data  0.059 ( 0.059)	Loss -0.2783 (-0.2783)
Epoch: [4][0/2]	Time  0.136 ( 0.136)	Data  0.060 ( 0.060)	Loss -0.3909 (-0.3909)
Epoch: [5][0/2]	Time  0.133 ( 0.133)	Data  0.058 ( 0.058)	Loss -0.4935 (-0.4935)
Epoch: [6][0/2]	Time  0.162 ( 0.162)	Data  0.058 ( 0.058)	Loss -0.5835 (-0.5835)
Epoch: [7][0/2]	Time  0.190 ( 0.190)	Data  0.060 ( 0.060)	Loss -0.6492 (-0.6492)
Epoch: [8][0/2]	Time  0.196 ( 0.196)	Data  0.060 ( 0.060)	Loss -0.7047 (-0.7047)
Epoch: [9][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.7021 (-0.7021)
Epoch: [10][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.7379 (-0.7379)
Epoch: [11][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.7638 (-0.7638)
Epoch: [12][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.7689 (-0.7689)
Epoch: [13][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.7622 (-0.7622)
Epoch: [14][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.7696 (-0.7696)
Epoch: [15][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.7677 (-0.7677)
Epoch: [16][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.7705 (-0.7705)
Epoch: [17][0/2]	Time  0.196 ( 0.196)	Data  0.059 ( 0.059)	Loss -0.7653 (-0.7653)
Epoch: [18][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.7610 (-0.7610)
Epoch: [19][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.7539 (-0.7539)
Epoch: [20][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.7574 (-0.7574)
Epoch: [21][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.7765 (-0.7765)
Epoch: [22][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.7766 (-0.7766)
Epoch: [23][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8043 (-0.8043)
Epoch: [24][0/2]	Time  0.196 ( 0.196)	Data  0.064 ( 0.064)	Loss -0.8034 (-0.8034)
Epoch: [25][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8043 (-0.8043)
Epoch: [26][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.8215 (-0.8215)
Epoch: [27][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.8137 (-0.8137)
Epoch: [28][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.8160 (-0.8160)
Epoch: [29][0/2]	Time  0.193 ( 0.193)	Data  0.060 ( 0.060)	Loss -0.8214 (-0.8214)
Epoch: [30][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.8381 (-0.8381)
Epoch: [31][0/2]	Time  0.191 ( 0.191)	Data  0.058 ( 0.058)	Loss -0.8267 (-0.8267)
Epoch: [32][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8301 (-0.8301)
Epoch: [33][0/2]	Time  0.181 ( 0.181)	Data  0.059 ( 0.059)	Loss -0.8356 (-0.8356)
Epoch: [34][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.8340 (-0.8340)
Epoch: [35][0/2]	Time  0.174 ( 0.174)	Data  0.059 ( 0.059)	Loss -0.8371 (-0.8371)
Epoch: [36][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.8255 (-0.8255)
Epoch: [37][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.7897 (-0.7897)
Epoch: [38][0/2]	Time  0.191 ( 0.191)	Data  0.062 ( 0.062)	Loss -0.7783 (-0.7783)
Epoch: [39][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.7630 (-0.7630)
Epoch: [40][0/2]	Time  0.191 ( 0.191)	Data  0.060 ( 0.060)	Loss -0.7587 (-0.7587)
Epoch: [41][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.7726 (-0.7726)
Epoch: [42][0/2]	Time  0.187 ( 0.187)	Data  0.060 ( 0.060)	Loss -0.7795 (-0.7795)
Epoch: [43][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.7980 (-0.7980)
Epoch: [44][0/2]	Time  0.181 ( 0.181)	Data  0.058 ( 0.058)	Loss -0.7995 (-0.7995)
Epoch: [45][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.8067 (-0.8067)
Epoch: [46][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.8273 (-0.8273)
Epoch: [47][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8105 (-0.8105)
Epoch: [48][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.8158 (-0.8158)
Epoch: [49][0/2]	Time  0.193 ( 0.193)	Data  0.060 ( 0.060)	Loss -0.8234 (-0.8234)
Epoch: [50][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.8224 (-0.8224)
Epoch: [51][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.8358 (-0.8358)
Epoch: [52][0/2]	Time  0.201 ( 0.201)	Data  0.060 ( 0.060)	Loss -0.8404 (-0.8404)
Epoch: [53][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.8547 (-0.8547)
Epoch: [54][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.8462 (-0.8462)
Epoch: [55][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.8552 (-0.8552)
Epoch: [56][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.8467 (-0.8467)
Epoch: [57][0/2]	Time  0.202 ( 0.202)	Data  0.060 ( 0.060)	Loss -0.8608 (-0.8608)
Epoch: [58][0/2]	Time  0.178 ( 0.178)	Data  0.059 ( 0.059)	Loss -0.8716 (-0.8716)
Epoch: [59][0/2]	Time  0.199 ( 0.199)	Data  0.060 ( 0.060)	Loss -0.8760 (-0.8760)
Epoch: [60][0/2]	Time  0.206 ( 0.206)	Data  0.060 ( 0.060)	Loss -0.8779 (-0.8779)
Epoch: [61][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.8811 (-0.8811)
Epoch: [62][0/2]	Time  0.216 ( 0.216)	Data  0.059 ( 0.059)	Loss -0.8892 (-0.8892)
Epoch: [63][0/2]	Time  0.202 ( 0.202)	Data  0.060 ( 0.060)	Loss -0.8883 (-0.8883)
Epoch: [64][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.8931 (-0.8931)
Epoch: [65][0/2]	Time  0.206 ( 0.206)	Data  0.060 ( 0.060)	Loss -0.8937 (-0.8937)
Epoch: [66][0/2]	Time  0.190 ( 0.190)	Data  0.058 ( 0.058)	Loss -0.8953 (-0.8953)
Epoch: [67][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.8954 (-0.8954)
Epoch: [68][0/2]	Time  0.186 ( 0.186)	Data  0.060 ( 0.060)	Loss -0.8960 (-0.8960)
Epoch: [69][0/2]	Time  0.200 ( 0.200)	Data  0.058 ( 0.058)	Loss -0.9044 (-0.9044)
Epoch: [70][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.9021 (-0.9021)
Epoch: [71][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8970 (-0.8970)
Epoch: [72][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8987 (-0.8987)
Epoch: [73][0/2]	Time  0.198 ( 0.198)	Data  0.060 ( 0.060)	Loss -0.8950 (-0.8950)
Epoch: [74][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.8993 (-0.8993)
Epoch: [75][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.8973 (-0.8973)
Epoch: [76][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.8888 (-0.8888)
Epoch: [77][0/2]	Time  0.210 ( 0.210)	Data  0.058 ( 0.058)	Loss -0.8943 (-0.8943)
Epoch: [78][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.8971 (-0.8971)
Epoch: [79][0/2]	Time  0.220 ( 0.220)	Data  0.058 ( 0.058)	Loss -0.8998 (-0.8998)
Epoch: [80][0/2]	Time  0.216 ( 0.216)	Data  0.059 ( 0.059)	Loss -0.9036 (-0.9036)
Epoch: [81][0/2]	Time  0.192 ( 0.192)	Data  0.059 ( 0.059)	Loss -0.8952 (-0.8952)
Epoch: [82][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.8957 (-0.8957)
Epoch: [83][0/2]	Time  0.194 ( 0.194)	Data  0.064 ( 0.064)	Loss -0.8994 (-0.8994)
Epoch: [84][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9064 (-0.9064)
Epoch: [85][0/2]	Time  0.197 ( 0.197)	Data  0.058 ( 0.058)	Loss -0.9017 (-0.9017)
Epoch: [86][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8944 (-0.8944)
Epoch: [87][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.8898 (-0.8898)
Epoch: [88][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8989 (-0.8989)
Epoch: [89][0/2]	Time  0.178 ( 0.178)	Data  0.058 ( 0.058)	Loss -0.9049 (-0.9049)
Epoch: [90][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9067 (-0.9067)
Epoch: [91][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9058 (-0.9058)
Epoch: [92][0/2]	Time  0.198 ( 0.198)	Data  0.063 ( 0.063)	Loss -0.9035 (-0.9035)
Epoch: [93][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9066 (-0.9066)
Epoch: [94][0/2]	Time  0.187 ( 0.187)	Data  0.059 ( 0.059)	Loss -0.9037 (-0.9037)
Epoch: [95][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8985 (-0.8985)
Epoch: [96][0/2]	Time  0.172 ( 0.172)	Data  0.058 ( 0.058)	Loss -0.9011 (-0.9011)
Epoch: [97][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9078 (-0.9078)
Epoch: [98][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.8948 (-0.8948)
Epoch: [99][0/2]	Time  0.179 ( 0.179)	Data  0.057 ( 0.057)	Loss -0.9018 (-0.9018)
Epoch: [100][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9065 (-0.9065)
Epoch: [101][0/2]	Time  0.206 ( 0.206)	Data  0.064 ( 0.064)	Loss -0.8992 (-0.8992)
Epoch: [102][0/2]	Time  0.192 ( 0.192)	Data  0.058 ( 0.058)	Loss -0.8880 (-0.8880)
Epoch: [103][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.8971 (-0.8971)
Epoch: [104][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.8892 (-0.8892)
Epoch: [105][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9017 (-0.9017)
Epoch: [106][0/2]	Time  0.179 ( 0.179)	Data  0.059 ( 0.059)	Loss -0.8884 (-0.8884)
Epoch: [107][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.8966 (-0.8966)
Epoch: [108][0/2]	Time  0.207 ( 0.207)	Data  0.058 ( 0.058)	Loss -0.8950 (-0.8950)
Epoch: [109][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.8944 (-0.8944)
Epoch: [110][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.9011 (-0.9011)
Epoch: [111][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.8914 (-0.8914)
Epoch: [112][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.9084 (-0.9084)
Epoch: [113][0/2]	Time  0.176 ( 0.176)	Data  0.058 ( 0.058)	Loss -0.8943 (-0.8943)
Epoch: [114][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9026 (-0.9026)
Epoch: [115][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.8946 (-0.8946)
Epoch: [116][0/2]	Time  0.194 ( 0.194)	Data  0.062 ( 0.062)	Loss -0.8918 (-0.8918)
Epoch: [117][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.8933 (-0.8933)
Epoch: [118][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.8831 (-0.8831)
Epoch: [119][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.8968 (-0.8968)
Epoch: [120][0/2]	Time  0.173 ( 0.173)	Data  0.059 ( 0.059)	Loss -0.9008 (-0.9008)
Epoch: [121][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.8960 (-0.8960)
Epoch: [122][0/2]	Time  0.201 ( 0.201)	Data  0.059 ( 0.059)	Loss -0.8920 (-0.8920)
Epoch: [123][0/2]	Time  0.184 ( 0.184)	Data  0.058 ( 0.058)	Loss -0.8995 (-0.8995)
Epoch: [124][0/2]	Time  0.199 ( 0.199)	Data  0.059 ( 0.059)	Loss -0.8966 (-0.8966)
Epoch: [125][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.8974 (-0.8974)
Epoch: [126][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9045 (-0.9045)
Epoch: [127][0/2]	Time  0.173 ( 0.173)	Data  0.057 ( 0.057)	Loss -0.9048 (-0.9048)
.\main_simsiam.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
Epoch: [128][0/2]	Time  0.193 ( 0.193)	Data  0.059 ( 0.059)	Loss -0.9033 (-0.9033)
Epoch: [129][0/2]	Time  0.205 ( 0.205)	Data  0.058 ( 0.058)	Loss -0.9033 (-0.9033)
Epoch: [130][0/2]	Time  0.183 ( 0.183)	Data  0.058 ( 0.058)	Loss -0.8986 (-0.8986)
Epoch: [131][0/2]	Time  0.203 ( 0.203)	Data  0.060 ( 0.060)	Loss -0.9010 (-0.9010)
Epoch: [132][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.8941 (-0.8941)
Epoch: [133][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9093 (-0.9093)
Epoch: [134][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.8996 (-0.8996)
Epoch: [135][0/2]	Time  0.190 ( 0.190)	Data  0.059 ( 0.059)	Loss -0.9044 (-0.9044)
Epoch: [136][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9055 (-0.9055)
Epoch: [137][0/2]	Time  0.187 ( 0.187)	Data  0.059 ( 0.059)	Loss -0.9012 (-0.9012)
Epoch: [138][0/2]	Time  0.207 ( 0.207)	Data  0.057 ( 0.057)	Loss -0.8971 (-0.8971)
Epoch: [139][0/2]	Time  0.177 ( 0.177)	Data  0.059 ( 0.059)	Loss -0.9039 (-0.9039)
Epoch: [140][0/2]	Time  0.193 ( 0.193)	Data  0.058 ( 0.058)	Loss -0.8936 (-0.8936)
Epoch: [141][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9059 (-0.9059)
Epoch: [142][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9056 (-0.9056)
Epoch: [143][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9111 (-0.9111)
Epoch: [144][0/2]	Time  0.180 ( 0.180)	Data  0.060 ( 0.060)	Loss -0.9078 (-0.9078)
Epoch: [145][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.9156 (-0.9156)
Epoch: [146][0/2]	Time  0.197 ( 0.197)	Data  0.059 ( 0.059)	Loss -0.9063 (-0.9063)
Epoch: [147][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9131 (-0.9131)
Epoch: [148][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9044 (-0.9044)
Epoch: [149][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.8983 (-0.8983)
Epoch: [150][0/2]	Time  0.207 ( 0.207)	Data  0.060 ( 0.060)	Loss -0.9092 (-0.9092)
Epoch: [151][0/2]	Time  0.181 ( 0.181)	Data  0.059 ( 0.059)	Loss -0.9067 (-0.9067)
Epoch: [152][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.8999 (-0.8999)
Epoch: [153][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9101 (-0.9101)
Epoch: [154][0/2]	Time  0.189 ( 0.189)	Data  0.058 ( 0.058)	Loss -0.9028 (-0.9028)
Epoch: [155][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9091 (-0.9091)
Epoch: [156][0/2]	Time  0.183 ( 0.183)	Data  0.059 ( 0.059)	Loss -0.9085 (-0.9085)
Epoch: [157][0/2]	Time  0.210 ( 0.210)	Data  0.065 ( 0.065)	Loss -0.9125 (-0.9125)
Epoch: [158][0/2]	Time  0.175 ( 0.175)	Data  0.059 ( 0.059)	Loss -0.9106 (-0.9106)
Epoch: [159][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9197 (-0.9197)
Epoch: [160][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9158 (-0.9158)
Epoch: [161][0/2]	Time  0.187 ( 0.187)	Data  0.059 ( 0.059)	Loss -0.9206 (-0.9206)
Epoch: [162][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9202 (-0.9202)
Epoch: [163][0/2]	Time  0.182 ( 0.182)	Data  0.057 ( 0.057)	Loss -0.9110 (-0.9110)
Epoch: [164][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9015 (-0.9015)
Epoch: [165][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.9139 (-0.9139)
Epoch: [166][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9101 (-0.9101)
Epoch: [167][0/2]	Time  0.205 ( 0.205)	Data  0.060 ( 0.060)	Loss -0.9183 (-0.9183)
Epoch: [168][0/2]	Time  0.190 ( 0.190)	Data  0.060 ( 0.060)	Loss -0.9211 (-0.9211)
Epoch: [169][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9065 (-0.9065)
Epoch: [170][0/2]	Time  0.179 ( 0.179)	Data  0.059 ( 0.059)	Loss -0.9191 (-0.9191)
Epoch: [171][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9184 (-0.9184)
Epoch: [172][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9134 (-0.9134)
Epoch: [173][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9194 (-0.9194)
Epoch: [174][0/2]	Time  0.204 ( 0.204)	Data  0.059 ( 0.059)	Loss -0.9127 (-0.9127)
Epoch: [175][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.9194 (-0.9194)
Epoch: [176][0/2]	Time  0.198 ( 0.198)	Data  0.057 ( 0.057)	Loss -0.9138 (-0.9138)
Epoch: [177][0/2]	Time  0.206 ( 0.206)	Data  0.058 ( 0.058)	Loss -0.9064 (-0.9064)
Epoch: [178][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9130 (-0.9130)
Epoch: [179][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9109 (-0.9109)
Epoch: [180][0/2]	Time  0.189 ( 0.189)	Data  0.060 ( 0.060)	Loss -0.9144 (-0.9144)
Epoch: [181][0/2]	Time  0.206 ( 0.206)	Data  0.062 ( 0.062)	Loss -0.9069 (-0.9069)
Epoch: [182][0/2]	Time  0.182 ( 0.182)	Data  0.058 ( 0.058)	Loss -0.9030 (-0.9030)
Epoch: [183][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9061 (-0.9061)
Epoch: [184][0/2]	Time  0.173 ( 0.173)	Data  0.059 ( 0.059)	Loss -0.9112 (-0.9112)
Epoch: [185][0/2]	Time  0.194 ( 0.194)	Data  0.058 ( 0.058)	Loss -0.9124 (-0.9124)
Epoch: [186][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9220 (-0.9220)
Epoch: [187][0/2]	Time  0.187 ( 0.187)	Data  0.058 ( 0.058)	Loss -0.9113 (-0.9113)
Epoch: [188][0/2]	Time  0.207 ( 0.207)	Data  0.060 ( 0.060)	Loss -0.9180 (-0.9180)
Epoch: [189][0/2]	Time  0.185 ( 0.185)	Data  0.059 ( 0.059)	Loss -0.9201 (-0.9201)
Epoch: [190][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9192 (-0.9192)
Epoch: [191][0/2]	Time  0.180 ( 0.180)	Data  0.059 ( 0.059)	Loss -0.9059 (-0.9059)
Epoch: [192][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9080 (-0.9080)
Epoch: [193][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9109 (-0.9109)
Epoch: [194][0/2]	Time  0.197 ( 0.197)	Data  0.059 ( 0.059)	Loss -0.9195 (-0.9195)
Epoch: [195][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9207 (-0.9207)
Epoch: [196][0/2]	Time  0.177 ( 0.177)	Data  0.058 ( 0.058)	Loss -0.9103 (-0.9103)
Epoch: [197][0/2]	Time  0.209 ( 0.209)	Data  0.059 ( 0.059)	Loss -0.9081 (-0.9081)
Epoch: [198][0/2]	Time  0.179 ( 0.179)	Data  0.064 ( 0.064)	Loss -0.9075 (-0.9075)
Epoch: [199][0/2]	Time  0.194 ( 0.194)	Data  0.059 ( 0.059)	Loss -0.9104 (-0.9104)
Epoch: [200][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9189 (-0.9189)
Epoch: [201][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9138 (-0.9138)
Epoch: [202][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.9134 (-0.9134)
Epoch: [203][0/2]	Time  0.185 ( 0.185)	Data  0.060 ( 0.060)	Loss -0.9068 (-0.9068)
Epoch: [204][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9172 (-0.9172)
Epoch: [205][0/2]	Time  0.176 ( 0.176)	Data  0.059 ( 0.059)	Loss -0.9160 (-0.9160)
Epoch: [206][0/2]	Time  0.195 ( 0.195)	Data  0.058 ( 0.058)	Loss -0.9095 (-0.9095)
Epoch: [207][0/2]	Time  0.201 ( 0.201)	Data  0.058 ( 0.058)	Loss -0.9183 (-0.9183)
Epoch: [208][0/2]	Time  0.185 ( 0.185)	Data  0.059 ( 0.059)	Loss -0.9162 (-0.9162)
Epoch: [209][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9141 (-0.9141)
Epoch: [210][0/2]	Time  0.182 ( 0.182)	Data  0.060 ( 0.060)	Loss -0.8944 (-0.8944)
Epoch: [211][0/2]	Time  0.198 ( 0.198)	Data  0.058 ( 0.058)	Loss -0.9111 (-0.9111)
Epoch: [212][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9121 (-0.9121)
Epoch: [213][0/2]	Time  0.188 ( 0.188)	Data  0.059 ( 0.059)	Loss -0.9139 (-0.9139)
Epoch: [214][0/2]	Time  0.203 ( 0.203)	Data  0.058 ( 0.058)	Loss -0.9017 (-0.9017)
Epoch: [215][0/2]	Time  0.175 ( 0.175)	Data  0.058 ( 0.058)	Loss -0.9108 (-0.9108)
Epoch: [216][0/2]	Time  0.199 ( 0.199)	Data  0.058 ( 0.058)	Loss -0.9146 (-0.9146)
Epoch: [217][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9144 (-0.9144)
Epoch: [218][0/2]	Time  0.195 ( 0.195)	Data  0.059 ( 0.059)	Loss -0.9122 (-0.9122)
Epoch: [219][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9119 (-0.9119)
Epoch: [220][0/2]	Time  0.185 ( 0.185)	Data  0.058 ( 0.058)	Loss -0.9005 (-0.9005)
Epoch: [221][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9099 (-0.9099)
Epoch: [222][0/2]	Time  0.180 ( 0.180)	Data  0.061 ( 0.061)	Loss -0.9151 (-0.9151)
Epoch: [223][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9151 (-0.9151)
Epoch: [224][0/2]	Time  0.204 ( 0.204)	Data  0.058 ( 0.058)	Loss -0.9210 (-0.9210)
Epoch: [225][0/2]	Time  0.192 ( 0.192)	Data  0.057 ( 0.057)	Loss -0.9114 (-0.9114)
Epoch: [226][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9185 (-0.9185)
Epoch: [227][0/2]	Time  0.189 ( 0.189)	Data  0.057 ( 0.057)	Loss -0.9160 (-0.9160)
Epoch: [228][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9217 (-0.9217)
Epoch: [229][0/2]	Time  0.180 ( 0.180)	Data  0.058 ( 0.058)	Loss -0.9215 (-0.9215)
Epoch: [230][0/2]	Time  0.196 ( 0.196)	Data  0.058 ( 0.058)	Loss -0.9135 (-0.9135)
Epoch: [231][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9130 (-0.9130)
Epoch: [232][0/2]	Time  0.188 ( 0.188)	Data  0.058 ( 0.058)	Loss -0.9242 (-0.9242)
Epoch: [233][0/2]	Time  0.200 ( 0.200)	Data  0.059 ( 0.059)	Loss -0.9171 (-0.9171)
Epoch: [234][0/2]	Time  0.184 ( 0.184)	Data  0.059 ( 0.059)	Loss -0.9088 (-0.9088)
Epoch: [235][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9241 (-0.9241)
Epoch: [236][0/2]	Time  0.179 ( 0.179)	Data  0.058 ( 0.058)	Loss -0.9156 (-0.9156)
Epoch: [237][0/2]	Time  0.198 ( 0.198)	Data  0.059 ( 0.059)	Loss -0.9157 (-0.9157)
Epoch: [238][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9131 (-0.9131)
Epoch: [239][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.9113 (-0.9113)
Epoch: [240][0/2]	Time  0.206 ( 0.206)	Data  0.059 ( 0.059)	Loss -0.9130 (-0.9130)
Epoch: [241][0/2]	Time  0.186 ( 0.186)	Data  0.059 ( 0.059)	Loss -0.9158 (-0.9158)
Epoch: [242][0/2]	Time  0.202 ( 0.202)	Data  0.058 ( 0.058)	Loss -0.9269 (-0.9269)
Epoch: [243][0/2]	Time  0.178 ( 0.178)	Data  0.059 ( 0.059)	Loss -0.9154 (-0.9154)
Epoch: [244][0/2]	Time  0.209 ( 0.209)	Data  0.058 ( 0.058)	Loss -0.9212 (-0.9212)
Epoch: [245][0/2]	Time  0.203 ( 0.203)	Data  0.059 ( 0.059)	Loss -0.9174 (-0.9174)
Epoch: [246][0/2]	Time  0.220 ( 0.220)	Data  0.063 ( 0.063)	Loss -0.9057 (-0.9057)
Epoch: [247][0/2]	Time  0.198 ( 0.198)	Data  0.060 ( 0.060)	Loss -0.9072 (-0.9072)
Epoch: [248][0/2]	Time  0.205 ( 0.205)	Data  0.059 ( 0.059)	Loss -0.9102 (-0.9102)
Epoch: [249][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.9085 (-0.9085)
Epoch: [250][0/2]	Time  0.206 ( 0.206)	Data  0.060 ( 0.060)	Loss -0.9236 (-0.9236)
Epoch: [251][0/2]	Time  0.189 ( 0.189)	Data  0.059 ( 0.059)	Loss -0.9200 (-0.9200)
Epoch: [252][0/2]	Time  0.202 ( 0.202)	Data  0.059 ( 0.059)	Loss -0.9147 (-0.9147)
Epoch: [253][0/2]	Time  0.182 ( 0.182)	Data  0.060 ( 0.060)	Loss -0.9137 (-0.9137)
Epoch: [254][0/2]	Time  0.163 ( 0.163)	Data  0.059 ( 0.059)	Loss -0.9179 (-0.9179)
Epoch: [255][0/2]	Time  0.160 ( 0.160)	Data  0.058 ( 0.058)	Loss -0.9191 (-0.9191)
Epoch: [256][0/2]	Time  0.163 ( 0.163)	Data  0.058 ( 0.058)	Loss -0.9174 (-0.9174)
Epoch: [257][0/2]	Time  0.155 ( 0.155)	Data  0.058 ( 0.058)	Loss -0.9209 (-0.9209)
Epoch: [258][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9204 (-0.9204)
Epoch: [259][0/2]	Time  0.163 ( 0.163)	Data  0.058 ( 0.058)	Loss -0.9185 (-0.9185)
Epoch: [260][0/2]	Time  0.167 ( 0.167)	Data  0.064 ( 0.064)	Loss -0.9217 (-0.9217)
Epoch: [261][0/2]	Time  0.161 ( 0.161)	Data  0.059 ( 0.059)	Loss -0.9225 (-0.9225)
Epoch: [262][0/2]	Time  0.160 ( 0.160)	Data  0.059 ( 0.059)	Loss -0.9138 (-0.9138)
Epoch: [263][0/2]	Time  0.163 ( 0.163)	Data  0.058 ( 0.058)	Loss -0.9155 (-0.9155)
Epoch: [264][0/2]	Time  0.160 ( 0.160)	Data  0.059 ( 0.059)	Loss -0.9050 (-0.9050)
Epoch: [265][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9075 (-0.9075)
Epoch: [266][0/2]	Time  0.164 ( 0.164)	Data  0.058 ( 0.058)	Loss -0.9096 (-0.9096)
Epoch: [267][0/2]	Time  0.155 ( 0.155)	Data  0.058 ( 0.058)	Loss -0.9172 (-0.9172)
Epoch: [268][0/2]	Time  0.162 ( 0.162)	Data  0.059 ( 0.059)	Loss -0.9149 (-0.9149)
Epoch: [269][0/2]	Time  0.165 ( 0.165)	Data  0.058 ( 0.058)	Loss -0.9071 (-0.9071)
Epoch: [270][0/2]	Time  0.158 ( 0.158)	Data  0.057 ( 0.057)	Loss -0.9132 (-0.9132)
Epoch: [271][0/2]	Time  0.167 ( 0.167)	Data  0.062 ( 0.062)	Loss -0.9163 (-0.9163)
Epoch: [272][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.9215 (-0.9215)
Epoch: [273][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9076 (-0.9076)
Epoch: [274][0/2]	Time  0.164 ( 0.164)	Data  0.058 ( 0.058)	Loss -0.9188 (-0.9188)
Epoch: [275][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.9153 (-0.9153)
Epoch: [276][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9164 (-0.9164)
Epoch: [277][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.9114 (-0.9114)
Epoch: [278][0/2]	Time  0.164 ( 0.164)	Data  0.058 ( 0.058)	Loss -0.9209 (-0.9209)
Epoch: [279][0/2]	Time  0.160 ( 0.160)	Data  0.060 ( 0.060)	Loss -0.9184 (-0.9184)
Epoch: [280][0/2]	Time  0.160 ( 0.160)	Data  0.057 ( 0.057)	Loss -0.9199 (-0.9199)
Epoch: [281][0/2]	Time  0.168 ( 0.168)	Data  0.062 ( 0.062)	Loss -0.9205 (-0.9205)
Epoch: [282][0/2]	Time  0.154 ( 0.154)	Data  0.057 ( 0.057)	Loss -0.9276 (-0.9276)
Epoch: [283][0/2]	Time  0.161 ( 0.161)	Data  0.057 ( 0.057)	Loss -0.9141 (-0.9141)
Epoch: [284][0/2]	Time  0.162 ( 0.162)	Data  0.058 ( 0.058)	Loss -0.9186 (-0.9186)
Epoch: [285][0/2]	Time  0.158 ( 0.158)	Data  0.057 ( 0.057)	Loss -0.9117 (-0.9117)
Epoch: [286][0/2]	Time  0.163 ( 0.163)	Data  0.059 ( 0.059)	Loss -0.9219 (-0.9219)
Epoch: [287][0/2]	Time  0.157 ( 0.157)	Data  0.058 ( 0.058)	Loss -0.9125 (-0.9125)
Epoch: [288][0/2]	Time  0.161 ( 0.161)	Data  0.059 ( 0.059)	Loss -0.9194 (-0.9194)
Epoch: [289][0/2]	Time  0.162 ( 0.162)	Data  0.060 ( 0.060)	Loss -0.9165 (-0.9165)
Epoch: [290][0/2]	Time  0.158 ( 0.158)	Data  0.058 ( 0.058)	Loss -0.9194 (-0.9194)
Epoch: [291][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9200 (-0.9200)
Epoch: [292][0/2]	Time  0.162 ( 0.162)	Data  0.058 ( 0.058)	Loss -0.9203 (-0.9203)
Epoch: [293][0/2]	Time  0.163 ( 0.163)	Data  0.059 ( 0.059)	Loss -0.9248 (-0.9248)
Epoch: [294][0/2]	Time  0.159 ( 0.159)	Data  0.058 ( 0.058)	Loss -0.9170 (-0.9170)
Epoch: [295][0/2]	Time  0.155 ( 0.155)	Data  0.057 ( 0.057)	Loss -0.9156 (-0.9156)
Epoch: [296][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9178 (-0.9178)
Epoch: [297][0/2]	Time  0.159 ( 0.159)	Data  0.057 ( 0.057)	Loss -0.9101 (-0.9101)
Epoch: [298][0/2]	Time  0.153 ( 0.153)	Data  0.058 ( 0.058)	Loss -0.9169 (-0.9169)
Epoch: [299][0/2]	Time  0.161 ( 0.161)	Data  0.058 ( 0.058)	Loss -0.9210 (-0.9210)
simsiam program ends here, while ssv:200, normal:200, excep_size:13
Use GPU: 0 for training
=> creating model 'resnet50'
=> loading checkpoint 'checkpoints/checkpoint_0099.pth.tar'
=> loaded pre-trained model 'checkpoints/checkpoint_0099.pth.tar'
Epoch: [0][0/1]	Time  0.455 ( 0.455)	Data  0.036 ( 0.036)	Loss 1.8075e+00 (1.8075e+00)	Acc@1   4.15 (  4.15)	Acc@5  93.96 ( 93.96)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.176 ( 0.176)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.051 ( 0.046)	Loss 1.6846e+01 (1.5358e+01)	Acc@1   0.00 ( 16.67)	Acc@5   0.00 ( 83.33)
 * Acc@1 16.667 Acc@5 83.333
=> loading 'checkpoints/checkpoint_0099.pth.tar' for sanity check
=> sanity check passed.
Epoch: [1][0/1]	Time  0.036 ( 0.036)	Data  0.016 ( 0.016)	Loss 4.4405e+00 (4.4405e+00)	Acc@1  75.47 ( 75.47)	Acc@5  95.09 ( 95.09)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.029 ( 0.029)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.029)	Loss 2.3540e+01 (1.6376e+01)	Acc@1   0.00 ( 20.37)	Acc@5   0.00 ( 83.33)
 * Acc@1 20.366 Acc@5 83.333
Epoch: [2][0/1]	Time  0.034 ( 0.034)	Data  0.020 ( 0.020)	Loss 5.0413e+00 (5.0413e+00)	Acc@1  76.23 ( 76.23)	Acc@5  95.09 ( 95.09)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.031 ( 0.031)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.028)	Loss 1.9089e+01 (1.2905e+01)	Acc@1   0.00 ( 27.10)	Acc@5   0.00 ( 83.33)
 * Acc@1 27.103 Acc@5 83.333
Epoch: [3][0/1]	Time  0.035 ( 0.035)	Data  0.021 ( 0.021)	Loss 4.1468e+00 (4.1468e+00)	Acc@1  78.49 ( 78.49)	Acc@5  95.09 ( 95.09)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.034 ( 0.034)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.029)	Loss 8.1957e+00 (8.0819e+00)	Acc@1   0.00 ( 30.88)	Acc@5  37.50 ( 91.98)
 * Acc@1 30.880 Acc@5 91.978
Epoch: [4][0/1]	Time  0.034 ( 0.034)	Data  0.020 ( 0.020)	Loss 2.6621e+00 (2.6621e+00)	Acc@1  79.25 ( 79.25)	Acc@5  96.98 ( 96.98)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.028 ( 0.028)	Loss 7.1990e-07 (7.1990e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.027)	Loss 3.6744e+00 (4.7088e+00)	Acc@1   0.00 ( 29.71)	Acc@5 100.00 ( 99.45)
 * Acc@1 29.712 Acc@5 99.455
Epoch: [5][0/1]	Time  0.032 ( 0.032)	Data  0.017 ( 0.017)	Loss 1.6571e+00 (1.6571e+00)	Acc@1  78.49 ( 78.49)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.033 ( 0.033)	Loss 3.1857e-02 (3.1857e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.027)	Loss 1.5696e+00 (2.5689e+00)	Acc@1   0.00 ( 39.88)	Acc@5 100.00 (100.00)
 * Acc@1 39.875 Acc@5 100.000
Epoch: [6][0/1]	Time  0.032 ( 0.032)	Data  0.018 ( 0.018)	Loss 7.2366e-01 (7.2366e-01)	Acc@1  80.38 ( 80.38)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.032 ( 0.032)	Loss 4.3343e+00 (4.3343e+00)	Acc@1   2.34 (  2.34)	Acc@5  98.05 ( 98.05)
Test: [10/11]	Time  0.004 ( 0.028)	Loss 7.7889e-02 (2.7891e+00)	Acc@1 100.00 ( 29.05)	Acc@5 100.00 ( 99.81)
 * Acc@1 29.050 Acc@5 99.805
Epoch: [7][0/1]	Time  0.029 ( 0.029)	Data  0.016 ( 0.016)	Loss 3.8584e+00 (3.8584e+00)	Acc@1  12.83 ( 12.83)	Acc@5  98.11 ( 98.11)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.030 ( 0.030)	Loss 6.0406e-04 (6.0406e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.028)	Loss 3.5901e+00 (4.8194e+00)	Acc@1   0.00 ( 33.41)	Acc@5 100.00 ( 98.52)
 * Acc@1 33.411 Acc@5 98.520
Epoch: [8][0/1]	Time  0.031 ( 0.031)	Data  0.017 ( 0.017)	Loss 1.3069e+00 (1.3069e+00)	Acc@1  82.26 ( 82.26)	Acc@5  98.87 ( 98.87)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.032 ( 0.032)	Loss 1.4435e-07 (1.4435e-07)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.028)	Loss 7.2345e+00 (6.2718e+00)	Acc@1   0.00 ( 33.64)	Acc@5 100.00 ( 95.64)
 * Acc@1 33.645 Acc@5 95.639
Epoch: [9][0/1]	Time  0.042 ( 0.042)	Data  0.017 ( 0.017)	Loss 2.1362e+00 (2.1362e+00)	Acc@1  82.64 ( 82.64)	Acc@5  98.49 ( 98.49)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.038)	Loss 1.1615e+01 (7.2680e+00)	Acc@1   0.00 ( 42.06)	Acc@5  75.00 ( 93.61)
 * Acc@1 42.056 Acc@5 93.614
Epoch: [10][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 2.7424e+00 (2.7424e+00)	Acc@1  81.89 ( 81.89)	Acc@5  98.49 ( 98.49)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 8.4034e+00 (8.4066e+00)	Acc@1   0.00 ( 33.57)	Acc@5  75.00 ( 93.38)
 * Acc@1 33.567 Acc@5 93.380
Epoch: [11][0/1]	Time  0.044 ( 0.044)	Data  0.019 ( 0.019)	Loss 2.9650e+00 (2.9650e+00)	Acc@1  79.25 ( 79.25)	Acc@5  98.11 ( 98.11)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 7.2314e+00 (8.5180e+00)	Acc@1   0.00 ( 31.66)	Acc@5  75.00 ( 94.39)
 * Acc@1 31.659 Acc@5 94.393
Epoch: [12][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 2.7056e+00 (2.7056e+00)	Acc@1  79.25 ( 79.25)	Acc@5  98.11 ( 98.11)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 6.2156e+00 (8.8886e+00)	Acc@1  25.00 ( 31.50)	Acc@5  62.50 ( 95.40)
 * Acc@1 31.503 Acc@5 95.405
Epoch: [13][0/1]	Time  0.044 ( 0.044)	Data  0.020 ( 0.020)	Loss 2.6782e+00 (2.6782e+00)	Acc@1  79.62 ( 79.62)	Acc@5  98.49 ( 98.49)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 0.0000e+00 (0.0000e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 4.3723e+00 (9.1621e+00)	Acc@1   0.00 ( 33.68)	Acc@5  87.50 ( 96.30)
 * Acc@1 33.684 Acc@5 96.301
Epoch: [14][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 2.3938e+00 (2.3938e+00)	Acc@1  82.26 ( 82.26)	Acc@5  98.87 ( 98.87)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 3.2596e-09 (3.2596e-09)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 5.4675e+00 (9.1987e+00)	Acc@1   0.00 ( 37.58)	Acc@5  75.00 ( 97.90)
 * Acc@1 37.578 Acc@5 97.897
Epoch: [15][0/1]	Time  0.041 ( 0.041)	Data  0.016 ( 0.016)	Loss 2.2420e+00 (2.2420e+00)	Acc@1  83.40 ( 83.40)	Acc@5  99.25 ( 99.25)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 5.8045e-06 (5.8045e-06)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 2.4401e+00 (8.1649e+00)	Acc@1  37.50 ( 35.51)	Acc@5 100.00 ( 99.14)
 * Acc@1 35.514 Acc@5 99.143
Epoch: [16][0/1]	Time  0.045 ( 0.045)	Data  0.019 ( 0.019)	Loss 1.7401e+00 (1.7401e+00)	Acc@1  80.38 ( 80.38)	Acc@5  99.25 ( 99.25)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 7.9243e-03 (7.9243e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 1.8783e+00 (6.3401e+00)	Acc@1  62.50 ( 43.22)	Acc@5 100.00 ( 99.49)
 * Acc@1 43.224 Acc@5 99.494
Epoch: [17][0/1]	Time  0.044 ( 0.044)	Data  0.019 ( 0.019)	Loss 1.1117e+00 (1.1117e+00)	Acc@1  81.89 ( 81.89)	Acc@5  99.25 ( 99.25)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 4.2543e-01 (4.2543e-01)	Acc@1  87.89 ( 87.89)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 2.4448e+00 (5.2455e+00)	Acc@1  50.00 ( 44.78)	Acc@5 100.00 ( 99.84)
 * Acc@1 44.782 Acc@5 99.844
Epoch: [18][0/1]	Time  0.044 ( 0.044)	Data  0.020 ( 0.020)	Loss 1.2100e+00 (1.2100e+00)	Acc@1  76.98 ( 76.98)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 7.8731e-01 (7.8731e-01)	Acc@1  78.52 ( 78.52)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 9.5605e-01 (4.3627e+00)	Acc@1  87.50 ( 45.29)	Acc@5 100.00 ( 99.88)
 * Acc@1 45.288 Acc@5 99.883
Epoch: [19][0/1]	Time  0.042 ( 0.042)	Data  0.018 ( 0.018)	Loss 1.4613e+00 (1.4613e+00)	Acc@1  66.04 ( 66.04)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.4873e-01 (1.4873e-01)	Acc@1  95.70 ( 95.70)	Acc@5  99.22 ( 99.22)
Test: [10/11]	Time  0.003 ( 0.038)	Loss 1.6524e-01 (3.2630e+00)	Acc@1  87.50 ( 51.01)	Acc@5 100.00 ( 99.77)
 * Acc@1 51.012 Acc@5 99.766
Epoch: [20][0/1]	Time  0.045 ( 0.045)	Data  0.019 ( 0.019)	Loss 8.7823e-01 (8.7823e-01)	Acc@1  84.53 ( 84.53)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 3.4932e-03 (3.4932e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 3.2212e-01 (2.6071e+00)	Acc@1  87.50 ( 55.18)	Acc@5 100.00 ( 99.92)
 * Acc@1 55.179 Acc@5 99.922
Epoch: [21][0/1]	Time  0.045 ( 0.045)	Data  0.019 ( 0.019)	Loss 9.3147e-01 (9.3147e-01)	Acc@1  84.53 ( 84.53)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 2.0235e-03 (2.0235e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 5.5972e-02 (2.2661e+00)	Acc@1 100.00 ( 59.19)	Acc@5 100.00 ( 99.10)
 * Acc@1 59.190 Acc@5 99.104
Epoch: [22][0/1]	Time  0.043 ( 0.043)	Data  0.018 ( 0.018)	Loss 1.1279e+00 (1.1279e+00)	Acc@1  86.04 ( 86.04)	Acc@5  98.87 ( 98.87)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 1.3485e-04 (1.3485e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 1.7423e-01 (2.8147e+00)	Acc@1  87.50 ( 56.23)	Acc@5 100.00 ( 96.38)
 * Acc@1 56.231 Acc@5 96.379
Epoch: [23][0/1]	Time  0.043 ( 0.043)	Data  0.018 ( 0.018)	Loss 1.0802e+00 (1.0802e+00)	Acc@1  84.91 ( 84.91)	Acc@5  99.25 ( 99.25)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 7.1075e-04 (7.1075e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 3.6182e-01 (3.3451e+00)	Acc@1  75.00 ( 54.56)	Acc@5 100.00 ( 96.50)
 * Acc@1 54.556 Acc@5 96.495
Epoch: [24][0/1]	Time  0.041 ( 0.041)	Data  0.017 ( 0.017)	Loss 1.1995e+00 (1.1995e+00)	Acc@1  83.40 ( 83.40)	Acc@5  99.25 ( 99.25)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.5992e-04 (1.5992e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 7.2473e-01 (2.7206e+00)	Acc@1  37.50 ( 49.73)	Acc@5 100.00 ( 98.83)
 * Acc@1 49.727 Acc@5 98.832
Epoch: [25][0/1]	Time  0.045 ( 0.045)	Data  0.019 ( 0.019)	Loss 1.0957e+00 (1.0957e+00)	Acc@1  84.15 ( 84.15)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 4.8654e-04 (4.8654e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 3.8611e+00 (2.4158e+00)	Acc@1  12.50 ( 52.65)	Acc@5 100.00 ( 99.84)
 * Acc@1 52.648 Acc@5 99.844
Epoch: [26][0/1]	Time  0.047 ( 0.047)	Data  0.021 ( 0.021)	Loss 1.1224e+00 (1.1224e+00)	Acc@1  84.53 ( 84.53)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 5.3646e-03 (5.3646e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 4.7323e+00 (2.5115e+00)	Acc@1  12.50 ( 55.92)	Acc@5 100.00 ( 99.88)
 * Acc@1 55.919 Acc@5 99.883
Epoch: [27][0/1]	Time  0.046 ( 0.046)	Data  0.020 ( 0.020)	Loss 9.5814e-01 (9.5814e-01)	Acc@1  83.77 ( 83.77)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 1.3488e-02 (1.3488e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 3.6732e+00 (2.2096e+00)	Acc@1  37.50 ( 55.88)	Acc@5 100.00 ( 99.92)
 * Acc@1 55.880 Acc@5 99.922
Epoch: [28][0/1]	Time  0.042 ( 0.042)	Data  0.018 ( 0.018)	Loss 8.6784e-01 (8.6784e-01)	Acc@1  84.91 ( 84.91)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 7.2039e-03 (7.2039e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 3.0290e+00 (2.1250e+00)	Acc@1  50.00 ( 55.37)	Acc@5 100.00 (100.00)
 * Acc@1 55.374 Acc@5 100.000
Epoch: [29][0/1]	Time  0.041 ( 0.041)	Data  0.017 ( 0.017)	Loss 7.3090e-01 (7.3090e-01)	Acc@1  85.66 ( 85.66)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 2.6669e-02 (2.6669e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 3.0929e+00 (2.3611e+00)	Acc@1  25.00 ( 53.50)	Acc@5 100.00 ( 99.92)
 * Acc@1 53.505 Acc@5 99.922
Epoch: [30][0/1]	Time  0.041 ( 0.041)	Data  0.015 ( 0.015)	Loss 7.8037e-01 (7.8037e-01)	Acc@1  84.53 ( 84.53)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 4.6284e-02 (4.6284e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.040)	Loss 9.8231e-01 (2.5287e+00)	Acc@1  62.50 ( 54.95)	Acc@5 100.00 (100.00)
 * Acc@1 54.945 Acc@5 100.000
Epoch: [31][0/1]	Time  0.041 ( 0.041)	Data  0.016 ( 0.016)	Loss 6.9934e-01 (6.9934e-01)	Acc@1  84.53 ( 84.53)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 1.0930e-01 (1.0930e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 8.8181e-02 (2.5573e+00)	Acc@1 100.00 ( 55.33)	Acc@5 100.00 (100.00)
 * Acc@1 55.335 Acc@5 100.000
Epoch: [32][0/1]	Time  0.042 ( 0.042)	Data  0.017 ( 0.017)	Loss 7.6703e-01 (7.6703e-01)	Acc@1  84.15 ( 84.15)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 1.6230e-01 (1.6230e-01)	Acc@1  94.14 ( 94.14)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 7.2383e-01 (2.3063e+00)	Acc@1  87.50 ( 55.69)	Acc@5 100.00 ( 99.92)
 * Acc@1 55.685 Acc@5 99.922
Epoch: [33][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 6.8323e-01 (6.8323e-01)	Acc@1  84.53 ( 84.53)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 2.9191e-01 (2.9191e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 1.8843e-01 (2.1926e+00)	Acc@1 100.00 ( 54.21)	Acc@5 100.00 ( 99.96)
 * Acc@1 54.206 Acc@5 99.961
Epoch: [34][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 7.5334e-01 (7.5334e-01)	Acc@1  81.51 ( 81.51)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 2.2262e-01 (2.2262e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 1.0945e+00 (2.2423e+00)	Acc@1  50.00 ( 51.48)	Acc@5 100.00 (100.00)
 * Acc@1 51.480 Acc@5 100.000
Epoch: [35][0/1]	Time  0.042 ( 0.042)	Data  0.017 ( 0.017)	Loss 6.1197e-01 (6.1197e-01)	Acc@1  84.53 ( 84.53)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 8.2137e-02 (8.2137e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 1.8089e+00 (2.1508e+00)	Acc@1  37.50 ( 53.12)	Acc@5 100.00 (100.00)
 * Acc@1 53.115 Acc@5 100.000
Epoch: [36][0/1]	Time  0.044 ( 0.044)	Data  0.019 ( 0.019)	Loss 5.0329e-01 (5.0329e-01)	Acc@1  87.55 ( 87.55)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 2.8415e-02 (2.8415e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 1.2637e+00 (2.0212e+00)	Acc@1  50.00 ( 57.09)	Acc@5 100.00 (100.00)
 * Acc@1 57.087 Acc@5 100.000
Epoch: [37][0/1]	Time  0.050 ( 0.050)	Data  0.024 ( 0.024)	Loss 4.1053e-01 (4.1053e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.047 ( 0.047)	Loss 4.1073e-03 (4.1073e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.040)	Loss 2.5968e+00 (2.0791e+00)	Acc@1  37.50 ( 55.33)	Acc@5 100.00 ( 99.96)
 * Acc@1 55.335 Acc@5 99.961
Epoch: [38][0/1]	Time  0.047 ( 0.047)	Data  0.021 ( 0.021)	Loss 4.8739e-01 (4.8739e-01)	Acc@1  85.66 ( 85.66)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 1.4264e-03 (1.4264e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.038)	Loss 2.6732e+00 (2.1217e+00)	Acc@1  37.50 ( 53.39)	Acc@5 100.00 (100.00)
 * Acc@1 53.388 Acc@5 100.000
Epoch: [39][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 4.8818e-01 (4.8818e-01)	Acc@1  86.79 ( 86.79)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 8.0063e-05 (8.0063e-05)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 2.0830e+00 (2.0256e+00)	Acc@1  50.00 ( 54.98)	Acc@5 100.00 ( 99.88)
 * Acc@1 54.984 Acc@5 99.883
Epoch: [40][0/1]	Time  0.039 ( 0.039)	Data  0.015 ( 0.015)	Loss 4.4705e-01 (4.4705e-01)	Acc@1  88.30 ( 88.30)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 9.7816e-04 (9.7816e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 1.4927e+00 (1.8893e+00)	Acc@1  62.50 ( 56.74)	Acc@5 100.00 ( 99.96)
 * Acc@1 56.737 Acc@5 99.961
Epoch: [41][0/1]	Time  0.040 ( 0.040)	Data  0.015 ( 0.015)	Loss 4.9428e-01 (4.9428e-01)	Acc@1  85.28 ( 85.28)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 2.6782e-04 (2.6782e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 1.7043e+00 (1.8067e+00)	Acc@1  62.50 ( 59.38)	Acc@5 100.00 (100.00)
 * Acc@1 59.385 Acc@5 100.000
Epoch: [42][0/1]	Time  0.042 ( 0.042)	Data  0.017 ( 0.017)	Loss 5.0710e-01 (5.0710e-01)	Acc@1  85.66 ( 85.66)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 7.2694e-03 (7.2694e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 1.1480e+00 (1.6896e+00)	Acc@1  62.50 ( 60.01)	Acc@5 100.00 ( 99.84)
 * Acc@1 60.008 Acc@5 99.844
Epoch: [43][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 4.3621e-01 (4.3621e-01)	Acc@1  87.55 ( 87.55)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 2.3337e-03 (2.3337e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 4.0310e-01 (1.7075e+00)	Acc@1  75.00 ( 60.12)	Acc@5 100.00 ( 99.84)
 * Acc@1 60.125 Acc@5 99.844
Epoch: [44][0/1]	Time  0.040 ( 0.040)	Data  0.016 ( 0.016)	Loss 5.3107e-01 (5.3107e-01)	Acc@1  86.42 ( 86.42)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 1.8448e-02 (1.8448e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 7.1650e-01 (1.6396e+00)	Acc@1  62.50 ( 59.70)	Acc@5 100.00 (100.00)
 * Acc@1 59.696 Acc@5 100.000
Epoch: [45][0/1]	Time  0.047 ( 0.047)	Data  0.021 ( 0.021)	Loss 4.1062e-01 (4.1062e-01)	Acc@1  87.55 ( 87.55)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 2.8486e-02 (2.8486e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.006 ( 0.039)	Loss 9.4249e-01 (1.6193e+00)	Acc@1  50.00 ( 60.94)	Acc@5 100.00 (100.00)
 * Acc@1 60.942 Acc@5 100.000
Epoch: [46][0/1]	Time  0.041 ( 0.041)	Data  0.016 ( 0.016)	Loss 3.8317e-01 (3.8317e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 5.2635e-02 (5.2635e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 9.4060e-01 (1.6301e+00)	Acc@1  50.00 ( 57.13)	Acc@5 100.00 (100.00)
 * Acc@1 57.126 Acc@5 100.000
Epoch: [47][0/1]	Time  0.046 ( 0.046)	Data  0.021 ( 0.021)	Loss 3.3213e-01 (3.3213e-01)	Acc@1  87.17 ( 87.17)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 3.0354e-02 (3.0354e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 7.1835e-01 (1.6216e+00)	Acc@1  62.50 ( 57.13)	Acc@5 100.00 (100.00)
 * Acc@1 57.126 Acc@5 100.000
Epoch: [48][0/1]	Time  0.047 ( 0.047)	Data  0.021 ( 0.021)	Loss 3.7770e-01 (3.7770e-01)	Acc@1  86.79 ( 86.79)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 1.4285e-02 (1.4285e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 8.8985e-01 (1.6281e+00)	Acc@1  62.50 ( 58.29)	Acc@5 100.00 (100.00)
 * Acc@1 58.294 Acc@5 100.000
Epoch: [49][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 3.5406e-01 (3.5406e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.037 ( 0.037)	Loss 1.5905e-02 (1.5905e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 7.5887e-01 (1.5193e+00)	Acc@1  62.50 ( 62.34)	Acc@5 100.00 (100.00)
 * Acc@1 62.344 Acc@5 100.000
Epoch: [50][0/1]	Time  0.039 ( 0.039)	Data  0.015 ( 0.015)	Loss 3.3647e-01 (3.3647e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 3.0632e-02 (3.0632e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 4.6951e-01 (1.4403e+00)	Acc@1  87.50 ( 63.59)	Acc@5 100.00 (100.00)
 * Acc@1 63.590 Acc@5 100.000
Epoch: [51][0/1]	Time  0.043 ( 0.043)	Data  0.017 ( 0.017)	Loss 3.8863e-01 (3.8863e-01)	Acc@1  87.92 ( 87.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 5.1048e-02 (5.1048e-02)	Acc@1  98.05 ( 98.05)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 6.0396e-01 (1.4325e+00)	Acc@1  75.00 ( 62.73)	Acc@5 100.00 (100.00)
 * Acc@1 62.734 Acc@5 100.000
Epoch: [52][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 3.6454e-01 (3.6454e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 3.2083e-02 (3.2083e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.037)	Loss 3.1236e-01 (1.3411e+00)	Acc@1  87.50 ( 64.25)	Acc@5 100.00 (100.00)
 * Acc@1 64.252 Acc@5 100.000
Epoch: [53][0/1]	Time  0.043 ( 0.043)	Data  0.018 ( 0.018)	Loss 3.5643e-01 (3.5643e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 4.9406e-02 (4.9406e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 3.6168e-01 (1.3228e+00)	Acc@1  87.50 ( 65.42)	Acc@5 100.00 (100.00)
 * Acc@1 65.421 Acc@5 100.000
Epoch: [54][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 3.9144e-01 (3.9144e-01)	Acc@1  86.04 ( 86.04)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 2.5052e-02 (2.5052e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.003 ( 0.038)	Loss 5.0536e-01 (1.3023e+00)	Acc@1  75.00 ( 65.19)	Acc@5 100.00 (100.00)
 * Acc@1 65.187 Acc@5 100.000
Epoch: [55][0/1]	Time  0.044 ( 0.044)	Data  0.018 ( 0.018)	Loss 3.5470e-01 (3.5470e-01)	Acc@1  87.55 ( 87.55)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.9367e-02 (1.9367e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 6.1193e-01 (1.2706e+00)	Acc@1  75.00 ( 66.94)	Acc@5 100.00 (100.00)
 * Acc@1 66.939 Acc@5 100.000
Epoch: [56][0/1]	Time  0.040 ( 0.040)	Data  0.015 ( 0.015)	Loss 3.3942e-01 (3.3942e-01)	Acc@1  90.19 ( 90.19)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 3.1777e-02 (3.1777e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 6.7271e-01 (1.2897e+00)	Acc@1  75.00 ( 63.32)	Acc@5 100.00 (100.00)
 * Acc@1 63.318 Acc@5 100.000
Epoch: [57][0/1]	Time  0.044 ( 0.044)	Data  0.019 ( 0.019)	Loss 2.9114e-01 (2.9114e-01)	Acc@1  89.81 ( 89.81)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 6.0491e-02 (6.0491e-02)	Acc@1  99.22 ( 99.22)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 6.0612e-01 (1.3601e+00)	Acc@1  75.00 ( 60.98)	Acc@5 100.00 ( 99.96)
 * Acc@1 60.981 Acc@5 99.961
Epoch: [58][0/1]	Time  0.044 ( 0.044)	Data  0.020 ( 0.020)	Loss 3.5821e-01 (3.5821e-01)	Acc@1  88.30 ( 88.30)	Acc@5  98.87 ( 98.87)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 3.7893e-02 (3.7893e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 6.5614e-01 (1.3989e+00)	Acc@1  75.00 ( 60.90)	Acc@5 100.00 (100.00)
 * Acc@1 60.903 Acc@5 100.000
Epoch: [59][0/1]	Time  0.040 ( 0.040)	Data  0.015 ( 0.015)	Loss 3.1304e-01 (3.1304e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 2.4208e-02 (2.4208e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 1.1819e+00 (1.3531e+00)	Acc@1  50.00 ( 60.98)	Acc@5 100.00 (100.00)
 * Acc@1 60.981 Acc@5 100.000
Epoch: [60][0/1]	Time  0.046 ( 0.046)	Data  0.021 ( 0.021)	Loss 3.4296e-01 (3.4296e-01)	Acc@1  88.30 ( 88.30)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.1794e-02 (1.1794e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 4.8759e-01 (1.3048e+00)	Acc@1  87.50 ( 63.20)	Acc@5 100.00 (100.00)
 * Acc@1 63.201 Acc@5 100.000
Epoch: [61][0/1]	Time  0.041 ( 0.041)	Data  0.017 ( 0.017)	Loss 3.0293e-01 (3.0293e-01)	Acc@1  90.19 ( 90.19)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 2.6763e-02 (2.6763e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 3.6409e-01 (1.3027e+00)	Acc@1 100.00 ( 65.03)	Acc@5 100.00 (100.00)
 * Acc@1 65.031 Acc@5 100.000
Epoch: [62][0/1]	Time  0.041 ( 0.041)	Data  0.016 ( 0.016)	Loss 3.3723e-01 (3.3723e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 4.9479e-03 (4.9479e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.038)	Loss 5.8676e-01 (1.2855e+00)	Acc@1  62.50 ( 64.76)	Acc@5 100.00 (100.00)
 * Acc@1 64.759 Acc@5 100.000
Epoch: [63][0/1]	Time  0.044 ( 0.044)	Data  0.019 ( 0.019)	Loss 3.4671e-01 (3.4671e-01)	Acc@1  90.57 ( 90.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 9.8920e-03 (9.8920e-03)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 5.9025e-01 (1.2762e+00)	Acc@1  87.50 ( 64.76)	Acc@5 100.00 ( 99.92)
 * Acc@1 64.759 Acc@5 99.922
Epoch: [64][0/1]	Time  0.039 ( 0.039)	Data  0.016 ( 0.016)	Loss 2.9899e-01 (2.9899e-01)	Acc@1  90.94 ( 90.94)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.037 ( 0.037)	Loss 6.8820e-03 (6.8820e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.036)	Loss 4.0734e-01 (1.3435e+00)	Acc@1  87.50 ( 64.76)	Acc@5 100.00 (100.00)
 * Acc@1 64.759 Acc@5 100.000
Epoch: [65][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 3.3688e-01 (3.3688e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 1.6037e-02 (1.6037e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 4.7545e-01 (1.3471e+00)	Acc@1 100.00 ( 62.73)	Acc@5 100.00 (100.00)
 * Acc@1 62.734 Acc@5 100.000
Epoch: [66][0/1]	Time  0.044 ( 0.044)	Data  0.018 ( 0.018)	Loss 3.2924e-01 (3.2924e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.2054e-03 (1.2054e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 4.2388e-01 (1.3169e+00)	Acc@1 100.00 ( 63.94)	Acc@5 100.00 (100.00)
 * Acc@1 63.941 Acc@5 100.000
Epoch: [67][0/1]	Time  0.044 ( 0.044)	Data  0.020 ( 0.020)	Loss 2.7582e-01 (2.7582e-01)	Acc@1  90.19 ( 90.19)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 1.4528e-02 (1.4528e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 5.7179e-01 (1.3996e+00)	Acc@1  87.50 ( 62.62)	Acc@5 100.00 (100.00)
 * Acc@1 62.617 Acc@5 100.000
Epoch: [68][0/1]	Time  0.042 ( 0.042)	Data  0.016 ( 0.016)	Loss 2.6891e-01 (2.6891e-01)	Acc@1  90.19 ( 90.19)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 3.2092e-02 (3.2092e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 7.8100e-01 (1.4434e+00)	Acc@1  75.00 ( 61.37)	Acc@5 100.00 (100.00)
 * Acc@1 61.371 Acc@5 100.000
Epoch: [69][0/1]	Time  0.043 ( 0.043)	Data  0.019 ( 0.019)	Loss 3.0601e-01 (3.0601e-01)	Acc@1  91.32 ( 91.32)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 1.1883e-02 (1.1883e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 8.2117e-01 (1.4174e+00)	Acc@1  75.00 ( 59.50)	Acc@5 100.00 (100.00)
 * Acc@1 59.502 Acc@5 100.000
Epoch: [70][0/1]	Time  0.039 ( 0.039)	Data  0.014 ( 0.014)	Loss 2.9161e-01 (2.9161e-01)	Acc@1  90.57 ( 90.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 5.4116e-03 (5.4116e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 1.0655e+00 (1.4653e+00)	Acc@1  62.50 ( 60.09)	Acc@5 100.00 (100.00)
 * Acc@1 60.086 Acc@5 100.000
Epoch: [71][0/1]	Time  0.047 ( 0.047)	Data  0.021 ( 0.021)	Loss 3.0295e-01 (3.0295e-01)	Acc@1  87.92 ( 87.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
.\main_lincls.py:113: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
.\main_lincls.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.pretrained, map_location="cpu")
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tr_np = np.concatenate(np.array(labels_tr)).astype('uint8')
C:\Users\bobobob\Desktop\1D-CNN-for-CWRU-master\data\data_preprocess.py:403: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels_tt_np = np.concatenate(np.array(labels_tt)).astype('uint8')
.\main_lincls.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(pretrained_weights, map_location="cpu")
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 1.0334e-02 (1.0334e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 1.1599e+00 (1.5101e+00)	Acc@1  37.50 ( 59.58)	Acc@5 100.00 (100.00)
 * Acc@1 59.579 Acc@5 100.000
Epoch: [72][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 2.8890e-01 (2.8890e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 1.8186e-02 (1.8186e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 9.4951e-01 (1.4562e+00)	Acc@1  50.00 ( 58.84)	Acc@5 100.00 (100.00)
 * Acc@1 58.840 Acc@5 100.000
Epoch: [73][0/1]	Time  0.045 ( 0.045)	Data  0.019 ( 0.019)	Loss 3.5154e-01 (3.5154e-01)	Acc@1  87.17 ( 87.17)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 3.1942e-02 (3.1942e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 1.1030e+00 (1.4846e+00)	Acc@1  50.00 ( 58.18)	Acc@5 100.00 (100.00)
 * Acc@1 58.178 Acc@5 100.000
Epoch: [74][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 3.2926e-01 (3.2926e-01)	Acc@1  87.92 ( 87.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 2.5386e-02 (2.5386e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.037)	Loss 8.5557e-01 (1.4369e+00)	Acc@1  50.00 ( 59.70)	Acc@5 100.00 (100.00)
 * Acc@1 59.696 Acc@5 100.000
Epoch: [75][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 2.8334e-01 (2.8334e-01)	Acc@1  87.17 ( 87.17)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 2.2614e-02 (2.2614e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 9.5344e-01 (1.3858e+00)	Acc@1  62.50 ( 59.97)	Acc@5 100.00 (100.00)
 * Acc@1 59.969 Acc@5 100.000
Epoch: [76][0/1]	Time  0.046 ( 0.046)	Data  0.020 ( 0.020)	Loss 3.3515e-01 (3.3515e-01)	Acc@1  87.17 ( 87.17)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 1.2457e-02 (1.2457e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 5.1727e-01 (1.4200e+00)	Acc@1  87.50 ( 60.94)	Acc@5 100.00 (100.00)
 * Acc@1 60.942 Acc@5 100.000
Epoch: [77][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 2.8747e-01 (2.8747e-01)	Acc@1  89.81 ( 89.81)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 2.3732e-02 (2.3732e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.040)	Loss 1.2466e+00 (1.4227e+00)	Acc@1  37.50 ( 60.05)	Acc@5 100.00 (100.00)
 * Acc@1 60.047 Acc@5 100.000
Epoch: [78][0/1]	Time  0.045 ( 0.045)	Data  0.020 ( 0.020)	Loss 3.0809e-01 (3.0809e-01)	Acc@1  87.92 ( 87.92)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 1.3979e-02 (1.3979e-02)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 5.5112e-01 (1.3412e+00)	Acc@1  87.50 ( 62.58)	Acc@5 100.00 (100.00)
 * Acc@1 62.578 Acc@5 100.000
Epoch: [79][0/1]	Time  0.045 ( 0.045)	Data  0.021 ( 0.021)	Loss 2.8840e-01 (2.8840e-01)	Acc@1  89.43 ( 89.43)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.037 ( 0.037)	Loss 5.7474e-03 (5.7474e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.036)	Loss 7.8836e-01 (1.3907e+00)	Acc@1  75.00 ( 61.57)	Acc@5 100.00 (100.00)
 * Acc@1 61.565 Acc@5 100.000
Epoch: [80][0/1]	Time  0.040 ( 0.040)	Data  0.015 ( 0.015)	Loss 3.2281e-01 (3.2281e-01)	Acc@1  89.43 ( 89.43)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.039 ( 0.039)	Loss 8.6763e-03 (8.6763e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.036)	Loss 6.6559e-01 (1.3482e+00)	Acc@1  75.00 ( 61.37)	Acc@5 100.00 (100.00)
 * Acc@1 61.371 Acc@5 100.000
Epoch: [81][0/1]	Time  0.042 ( 0.042)	Data  0.015 ( 0.015)	Loss 3.2157e-01 (3.2157e-01)	Acc@1  89.81 ( 89.81)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.044 ( 0.044)	Loss 3.9590e-02 (3.9590e-02)	Acc@1  99.22 ( 99.22)	Acc@5  99.61 ( 99.61)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 6.4151e-01 (1.3118e+00)	Acc@1 100.00 ( 63.16)	Acc@5 100.00 ( 99.96)
 * Acc@1 63.162 Acc@5 99.961
Epoch: [82][0/1]	Time  0.043 ( 0.043)	Data  0.017 ( 0.017)	Loss 3.3034e-01 (3.3034e-01)	Acc@1  87.55 ( 87.55)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.045 ( 0.045)	Loss 2.8083e-02 (2.8083e-02)	Acc@1  98.83 ( 98.83)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 1.1864e+00 (1.3403e+00)	Acc@1  50.00 ( 63.71)	Acc@5 100.00 (100.00)
 * Acc@1 63.707 Acc@5 100.000
Epoch: [83][0/1]	Time  0.041 ( 0.041)	Data  0.017 ( 0.017)	Loss 3.2194e-01 (3.2194e-01)	Acc@1  92.08 ( 92.08)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.040 ( 0.040)	Loss 2.7133e-03 (2.7133e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.039)	Loss 9.1806e-01 (1.3355e+00)	Acc@1  62.50 ( 62.46)	Acc@5 100.00 (100.00)
 * Acc@1 62.461 Acc@5 100.000
Epoch: [84][0/1]	Time  0.041 ( 0.041)	Data  0.015 ( 0.015)	Loss 2.8810e-01 (2.8810e-01)	Acc@1  88.68 ( 88.68)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 6.7622e-03 (6.7622e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.037)	Loss 8.7436e-01 (1.3530e+00)	Acc@1  62.50 ( 63.79)	Acc@5 100.00 (100.00)
 * Acc@1 63.785 Acc@5 100.000
Epoch: [85][0/1]	Time  0.040 ( 0.040)	Data  0.017 ( 0.017)	Loss 3.2771e-01 (3.2771e-01)	Acc@1  89.81 ( 89.81)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 8.0105e-03 (8.0105e-03)	Acc@1  99.61 ( 99.61)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 8.2582e-01 (1.3353e+00)	Acc@1  75.00 ( 63.20)	Acc@5 100.00 (100.00)
 * Acc@1 63.201 Acc@5 100.000
Epoch: [86][0/1]	Time  0.043 ( 0.043)	Data  0.018 ( 0.018)	Loss 2.7690e-01 (2.7690e-01)	Acc@1  90.94 ( 90.94)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.042 ( 0.042)	Loss 3.9176e-03 (3.9176e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 7.6822e-01 (1.3243e+00)	Acc@1  75.00 ( 63.05)	Acc@5 100.00 ( 99.92)
 * Acc@1 63.045 Acc@5 99.922
Epoch: [87][0/1]	Time  0.039 ( 0.039)	Data  0.014 ( 0.014)	Loss 2.6656e-01 (2.6656e-01)	Acc@1  89.81 ( 89.81)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.038 ( 0.038)	Loss 2.0372e-02 (2.0372e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 6.9753e-01 (1.3013e+00)	Acc@1  75.00 ( 62.93)	Acc@5 100.00 ( 99.96)
 * Acc@1 62.928 Acc@5 99.961
Epoch: [88][0/1]	Time  0.043 ( 0.043)	Data  0.017 ( 0.017)	Loss 2.8451e-01 (2.8451e-01)	Acc@1  90.57 ( 90.57)	Acc@5 100.00 (100.00)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.041 ( 0.041)	Loss 2.4867e-02 (2.4867e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.004 ( 0.038)	Loss 4.4854e-01 (1.3455e+00)	Acc@1  87.50 ( 63.32)	Acc@5 100.00 (100.00)
 * Acc@1 63.318 Acc@5 100.000
Epoch: [89][0/1]	Time  0.039 ( 0.039)	Data  0.016 ( 0.016)	Loss 3.2567e-01 (3.2567e-01)	Acc@1  90.57 ( 90.57)	Acc@5  99.62 ( 99.62)
lincls program ends here, while ssv:200, normal:200, excep_size:13
Test: [ 0/11]	Time  0.043 ( 0.043)	Loss 2.1554e-02 (2.1554e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Test: [10/11]	Time  0.005 ( 0.039)	Loss 9.4962e-01 (1.2630e+00)	Acc@1  62.50 ( 64.02)	Acc@5 100.00 (100.00)
 * Acc@1 64.019 Acc@5 100.000
